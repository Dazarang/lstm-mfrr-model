{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CASE 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import joblib\n",
                "import plotly.graph_objects as go\n",
                "\n",
                "# ---- PATHS ----\n",
                "MODEL_PATH = \"/new_model_test/best_model_SE2_Down_Volume.pth\"\n",
                "SCALER_X_PATH = \"/new_model_test/scaler_X_SE2_Down_Volume.joblib\"\n",
                "SCALER_Y_PATH = \"/new_model_test/scaler_y_SE2_Down_Volume.joblib\"\n",
                "FILE_PATH = \"/results/results_merged.csv\"\n",
                "\n",
                "REGION = \"SE2\"\n",
                "target_vars = [f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownActivatedVolume\"]\n",
                "\n",
                "# -- Model class --\n",
                "class MaskedLSTM(torch.nn.Module):\n",
                "    def __init__(self, input_size, hidden_size=32, num_layers=1, dense_size=32, output_size=1, horizon=1, dropout=0.08, dropout_lstm=0.27, bidirectional=True):\n",
                "        super().__init__()\n",
                "        self.horizon = horizon\n",
                "        self.output_size = output_size\n",
                "        actual_in = input_size * 2\n",
                "        self.lstm = torch.nn.LSTM(\n",
                "            input_size=actual_in,\n",
                "            hidden_size=hidden_size,\n",
                "            num_layers=num_layers,\n",
                "            dropout=dropout_lstm if num_layers > 1 else 0.0,\n",
                "            bidirectional=bidirectional,\n",
                "            batch_first=True,\n",
                "        )\n",
                "        hidden_out = hidden_size * (2 if bidirectional else 1)\n",
                "        self.fc1 = torch.nn.Linear(hidden_out, dense_size)\n",
                "        self.fc2 = torch.nn.Linear(dense_size, horizon * output_size)\n",
                "        self.relu = torch.nn.ReLU()\n",
                "        self.drop = torch.nn.Dropout(dropout)\n",
                "        self.drop_lstm = torch.nn.Dropout(dropout_lstm)\n",
                "    def forward(self, x, mask):\n",
                "        x = torch.cat((x, mask), dim=2)\n",
                "        out, _ = self.lstm(x)\n",
                "        out = self.drop_lstm(out)\n",
                "        out = self.relu(self.fc1(out[:, -1, :]))\n",
                "        out = self.drop(out)\n",
                "        out = self.fc2(out)\n",
                "        return out.view(out.size(0), self.horizon, self.output_size)\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "scaler_X = joblib.load(SCALER_X_PATH)\n",
                "scaler_y = joblib.load(SCALER_Y_PATH)\n",
                "\n",
                "best_params = {\n",
                "    \"hidden_size\": 32,\n",
                "    \"num_layers\": 1,\n",
                "    \"bidirectional\": True,\n",
                "    \"dense_size\": 32,\n",
                "    \"dropout\": 0.08137184695714653,\n",
                "    \"dropout_lstm\": 0.2675438586833995,\n",
                "    \"learning_rate\": 0.0013843186375837144,\n",
                "    \"batch_size\": 128,\n",
                "    \"seq_length\": 216,\n",
                "}\n",
                "seq_length = best_params[\"seq_length\"]\n",
                "\n",
                "# --- Load and preprocess data ---\n",
                "df_all = pd.read_csv(FILE_PATH)\n",
                "price_column_name_to_plot = f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownPrice\"\n",
                "\n",
                "# Store price data before dropping columns\n",
                "price_data_full = None\n",
                "if price_column_name_to_plot in df_all.columns:\n",
                "    price_data_full = df_all[['DateTime', price_column_name_to_plot]].copy()\n",
                "    price_data_full['DateTime'] = pd.to_datetime(price_data_full['DateTime'], utc=True)\n",
                "    price_data_full.set_index('DateTime', inplace=True)\n",
                "    print(f\"Stored '{price_column_name_to_plot}' separately for plotting.\")\n",
                "else:\n",
                "    print(f\"Warning: Price column '{price_column_name_to_plot}' not found in df_all. Will not be plotted.\")\n",
                "\n",
                "from filter_features import pick_region_filter\n",
                "BDZ_filter = pick_region_filter(region=REGION, remove_balancing=True)\n",
                "drop_list = [\n",
                "    f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedUpPrice\",\n",
                "    f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownPrice\",\n",
                "    f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedUpActivatedVolume\",\n",
                "    f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownActivatedVolume\",\n",
                "]\n",
                "BDZ_filter = [c for c in BDZ_filter if c not in drop_list or c in target_vars]\n",
                "df_all[\"DateTime\"] = pd.to_datetime(df_all[\"DateTime\"], utc=True)\n",
                "df_all.set_index(\"DateTime\", inplace=True)\n",
                "df_all = df_all.asfreq(\"h\")\n",
                "df = df_all[BDZ_filter].copy()\n",
                "df = df.asfreq(\"h\")\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "CREATE_INDICATOR_FEATURES = False\n",
                "CREATE_CYCLICAL_FEATURES = False\n",
                "CREATE_LAGGED_FEATURES = False\n",
                "\n",
                "new_indicator_features = []\n",
                "if CREATE_INDICATOR_FEATURES:\n",
                "    indicator_features_to_create = [col for col in BDZ_filter + target_vars if col in df.columns]\n",
                "    for col in indicator_features_to_create:\n",
                "        df[f'{col}_was_missing'] = df[col].isnull().astype(int)\n",
                "    new_indicator_features = [f'{col}_was_missing' for col in indicator_features_to_create]\n",
                "\n",
                "new_time_features = []\n",
                "if CREATE_CYCLICAL_FEATURES:\n",
                "    df['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24.0)\n",
                "    df['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24.0)\n",
                "    df['dayofweek_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7.0)\n",
                "    df['dayofweek_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7.0)\n",
                "    df['weekofyear_sin'] = np.sin(2 * np.pi * df.index.isocalendar().week / 52.0)\n",
                "    df['weekofyear_cos'] = np.cos(2 * np.pi * df.index.isocalendar().week / 52.0)\n",
                "    new_time_features = ['hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'weekofyear_sin', 'weekofyear_cos']\n",
                "\n",
                "new_lagged_features = []\n",
                "if CREATE_LAGGED_FEATURES:\n",
                "    for target_var in target_vars:\n",
                "        df[f'{target_var}_lag1'] = df[target_var].shift(1)\n",
                "        df[f'{target_var}_lag24'] = df[target_var].shift(24)\n",
                "        df[f'{target_var}_lag168'] = df[target_var].shift(168)\n",
                "    for target_var in target_vars:\n",
                "        new_lagged_features.extend([f'{target_var}_lag1', f'{target_var}_lag24', f'{target_var}_lag168'])\n",
                "\n",
                "df.dropna(axis=1, how=\"all\", inplace=True)\n",
                "\n",
                "exogenous_vars = [c for c in BDZ_filter if c not in target_vars]\n",
                "if CREATE_CYCLICAL_FEATURES:\n",
                "    exogenous_vars.extend(new_time_features)\n",
                "if CREATE_LAGGED_FEATURES:\n",
                "    exogenous_vars.extend(new_lagged_features)\n",
                "if CREATE_INDICATOR_FEATURES:\n",
                "    exogenous_vars.extend(new_indicator_features)\n",
                "exogenous_vars = sorted(list(set(var for var in exogenous_vars if var in df.columns)))\n",
                "\n",
                "# --- Instantiate the model ---\n",
                "model = MaskedLSTM(\n",
                "    input_size=len(exogenous_vars),\n",
                "    hidden_size=best_params[\"hidden_size\"],\n",
                "    num_layers=best_params[\"num_layers\"],\n",
                "    dense_size=best_params[\"dense_size\"],\n",
                "    output_size=len(target_vars),\n",
                "    dropout=best_params[\"dropout\"],\n",
                "    dropout_lstm=best_params[\"dropout_lstm\"],\n",
                "    bidirectional=best_params[\"bidirectional\"],\n",
                ").to(DEVICE)\n",
                "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
                "model.eval()\n",
                "\n",
                "# --- Robust window selection: will always work for any seq_length and date range ---\n",
                "\n",
                "desired_start = pd.Timestamp(\"2023-09-05\", tz=\"UTC\")\n",
                "desired_end = pd.Timestamp(\"2023-09-08\", tz=\"UTC\")\n",
                "\n",
                "all_dates = df.index\n",
                "try:\n",
                "    first_pred_idx = all_dates.get_loc(desired_start)\n",
                "except KeyError:\n",
                "    raise ValueError(f\"Desired start {desired_start} not in DataFrame index!\")\n",
                "\n",
                "if first_pred_idx < seq_length:\n",
                "    raise ValueError(f\"Not enough data before {desired_start} to build input sequence with seq_length={seq_length}.\")\n",
                "\n",
                "window_start = all_dates[first_pred_idx - seq_length]\n",
                "window_end = desired_end\n",
                "\n",
                "df_pred_window = df.loc[window_start:window_end].copy()\n",
                "\n",
                "# -- Scale features --\n",
                "X_scaled = scaler_X.transform(df_pred_window[exogenous_vars])\n",
                "y_true = df_pred_window[target_vars].values\n",
                "\n",
                "# -- Build sequences --\n",
                "def create_sequences(X, lookback, horizon=1):\n",
                "    num_samples = X.shape[0] - lookback - horizon + 1\n",
                "    X_seq = np.zeros((num_samples, lookback, X.shape[1]), dtype=np.float32)\n",
                "    mask_seq = np.ones((num_samples, lookback, X.shape[1]), dtype=np.float32)\n",
                "    for i in range(num_samples):\n",
                "        X_seq[i] = X[i : i + lookback]\n",
                "    return X_seq, mask_seq\n",
                "\n",
                "X_seq, mask_seq = create_sequences(X_scaled, seq_length, 1)\n",
                "nan_pos = np.isnan(X_seq)\n",
                "mask_seq[nan_pos] = 0.0\n",
                "X_seq[nan_pos] = 0.0\n",
                "\n",
                "dates_seq = df_pred_window.index[seq_length : seq_length + len(X_seq)]\n",
                "y_true_aligned = y_true[seq_length : seq_length + len(X_seq)]\n",
                "\n",
                "# -- Filter for predictions within the requested window only --\n",
                "mask = (dates_seq >= desired_start) & (dates_seq <= desired_end)\n",
                "dates_seq = dates_seq[mask]\n",
                "y_true_aligned = y_true_aligned[mask]\n",
                "\n",
                "with torch.no_grad():\n",
                "    X_tensor = torch.tensor(X_seq, dtype=torch.float32).to(DEVICE)\n",
                "    mask_tensor = torch.tensor(mask_seq, dtype=torch.float32).to(DEVICE)\n",
                "    y_pred_scaled = model(X_tensor, mask_tensor).cpu().numpy()[:, 0, :]  # (n, target_dim)\n",
                "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
                "y_pred = y_pred[mask]\n",
                "\n",
                "# --- Add price data ---\n",
                "price_data_to_plot = None\n",
                "if price_data_full is not None and price_column_name_to_plot in price_data_full.columns:\n",
                "    try:\n",
                "        # Align price data with dates_seq\n",
                "        price_data_to_plot = price_data_full.loc[dates_seq, price_column_name_to_plot].values.astype(float)\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not align price data with dates_seq: {e}\")\n",
                "        price_data_to_plot = None\n",
                "\n",
                "# -- Plot\n",
                "fig = go.Figure()\n",
                "fig.add_trace(go.Scatter(\n",
                "    x=dates_seq,\n",
                "    y=y_true_aligned[:, 0],\n",
                "    mode='lines+markers',\n",
                "    name='True',\n",
                "    marker=dict(symbol='circle', size=6)\n",
                "))\n",
                "fig.add_trace(go.Scatter(\n",
                "    x=dates_seq,\n",
                "    y=y_pred[:, 0],\n",
                "    mode='lines+markers',\n",
                "    name='Predicted',\n",
                "    marker=dict(symbol='x', size=6)\n",
                "))\n",
                "if price_data_to_plot is not None:\n",
                "    fig.add_trace(go.Scatter(\n",
                "        x=dates_seq,\n",
                "        y=price_data_to_plot,\n",
                "        mode='lines',\n",
                "        name='mFRR Down Price',\n",
                "        line=dict(color='orange', width=2),\n",
                "        yaxis='y2'\n",
                "    ))\n",
                "fig.update_layout(\n",
                "    title=f\"True vs Predicted for {target_vars[0]} ({desired_start.date()} to {desired_end.date()})\",\n",
                "    xaxis_title=\"Date\",\n",
                "    yaxis=dict(\n",
                "        title=\"Volume (MW)\"\n",
                "    ),\n",
                "    yaxis2=dict(\n",
                "        title=\"mFRR Down Price (€/MWh)\",\n",
                "        overlaying='y',\n",
                "        side='right',\n",
                "        showgrid=False\n",
                "    ),\n",
                "    template=\"plotly_white\",\n",
                "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
                ")\n",
                "fig.update_xaxes(tickformat=\"%d %b %Y %H:%M\")\n",
                "fig.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Case 1 Plot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": [
                "desired_start = pd.Timestamp(\"2023-09-05\", tz=\"UTC\")\n",
                "desired_end = pd.Timestamp(\"2023-09-07\", tz=\"UTC\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import joblib\n",
                "import plotly.graph_objects as go\n",
                "\n",
                "# ---- PATHS ----\n",
                "MODEL_PATH = \"/new_model_test/best_model_SE2_Down_Volume.pth\"\n",
                "SCALER_X_PATH = \"/new_model_test/scaler_X_SE2_Down_Volume.joblib\"\n",
                "SCALER_Y_PATH = \"/new_model_test/scaler_y_SE2_Down_Volume.joblib\"\n",
                "FILE_PATH = \"/results/results_merged.csv\"\n",
                "\n",
                "REGION = \"SE2\"\n",
                "target_vars = [f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownActivatedVolume\"]\n",
                "\n",
                "# -- Model class --\n",
                "class MaskedLSTM(torch.nn.Module):\n",
                "    def __init__(self, input_size, hidden_size=32, num_layers=1, dense_size=32, output_size=1, horizon=1, dropout=0.08, dropout_lstm=0.27, bidirectional=True):\n",
                "        super().__init__()\n",
                "        self.horizon = horizon\n",
                "        self.output_size = output_size\n",
                "        actual_in = input_size * 2\n",
                "        self.lstm = torch.nn.LSTM(\n",
                "            input_size=actual_in,\n",
                "            hidden_size=hidden_size,\n",
                "            num_layers=num_layers,\n",
                "            dropout=dropout_lstm if num_layers > 1 else 0.0,\n",
                "            bidirectional=bidirectional,\n",
                "            batch_first=True,\n",
                "        )\n",
                "        hidden_out = hidden_size * (2 if bidirectional else 1)\n",
                "        self.fc1 = torch.nn.Linear(hidden_out, dense_size)\n",
                "        self.fc2 = torch.nn.Linear(dense_size, horizon * output_size)\n",
                "        self.relu = torch.nn.ReLU()\n",
                "        self.drop = torch.nn.Dropout(dropout)\n",
                "        self.drop_lstm = torch.nn.Dropout(dropout_lstm)\n",
                "    def forward(self, x, mask):\n",
                "        x = torch.cat((x, mask), dim=2)\n",
                "        out, _ = self.lstm(x)\n",
                "        out = self.drop_lstm(out)\n",
                "        out = self.relu(self.fc1(out[:, -1, :]))\n",
                "        out = self.drop(out)\n",
                "        out = self.fc2(out)\n",
                "        return out.view(out.size(0), self.horizon, self.output_size)\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "scaler_X = joblib.load(SCALER_X_PATH)\n",
                "scaler_y = joblib.load(SCALER_Y_PATH)\n",
                "\n",
                "best_params = {\n",
                "    \"hidden_size\": 32,\n",
                "    \"num_layers\": 1,\n",
                "    \"bidirectional\": True,\n",
                "    \"dense_size\": 32,\n",
                "    \"dropout\": 0.08137184695714653,\n",
                "    \"dropout_lstm\": 0.2675438586833995,\n",
                "    \"learning_rate\": 0.0013843186375837144,\n",
                "    \"batch_size\": 128,\n",
                "    \"seq_length\": 216,\n",
                "}\n",
                "seq_length = best_params[\"seq_length\"]\n",
                "\n",
                "# --- Load and preprocess data ---\n",
                "df_all = pd.read_csv(FILE_PATH)\n",
                "price_column_name_to_plot = f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownPrice\"\n",
                "\n",
                "# Store price data before dropping columns\n",
                "price_data_full = None\n",
                "if price_column_name_to_plot in df_all.columns:\n",
                "    price_data_full = df_all[['DateTime', price_column_name_to_plot]].copy()\n",
                "    price_data_full['DateTime'] = pd.to_datetime(price_data_full['DateTime'], utc=True)\n",
                "    price_data_full.set_index('DateTime', inplace=True)\n",
                "    print(f\"Stored '{price_column_name_to_plot}' separately for plotting.\")\n",
                "else:\n",
                "    print(f\"Warning: Price column '{price_column_name_to_plot}' not found in df_all. Will not be plotted.\")\n",
                "\n",
                "from filter_features import pick_region_filter\n",
                "BDZ_filter = pick_region_filter(region=REGION, remove_balancing=True)\n",
                "drop_list = [\n",
                "    f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedUpPrice\",\n",
                "    f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownPrice\",\n",
                "    f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedUpActivatedVolume\",\n",
                "    f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownActivatedVolume\",\n",
                "]\n",
                "BDZ_filter = [c for c in BDZ_filter if c not in drop_list or c in target_vars]\n",
                "df_all[\"DateTime\"] = pd.to_datetime(df_all[\"DateTime\"], utc=True)\n",
                "df_all.set_index(\"DateTime\", inplace=True)\n",
                "df_all = df_all.asfreq(\"h\")\n",
                "df = df_all[BDZ_filter].copy()\n",
                "df = df.asfreq(\"h\")\n",
                "\n",
                "def fill_ahead_features(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Forward-fills 0/NaN gaps in *-ahead* columns within their natural period\n",
                "    (day-ahead → daily, week-ahead → Monday-anchored weeks, etc.).\n",
                "\n",
                "    A value is propagated only up to the end of the period, and only while the\n",
                "    entries being filled are 0 or NaN.\n",
                "    \"\"\"\n",
                "    df = df.copy()\n",
                "\n",
                "    period_freq = {\n",
                "        \"dayahead\":  \"D\",        # daily groups\n",
                "        \"weekahead\": \"W-MON\",    # ISO weeks starting on Monday\n",
                "        \"monthahead\":\"MS\",       # month start\n",
                "        \"yearahead\": \"AS\"        # year start\n",
                "    }\n",
                "\n",
                "    for period, freq in period_freq.items():\n",
                "        ahead_cols = [c for c in df.columns if period in c.lower()]\n",
                "        if not ahead_cols:\n",
                "            continue\n",
                "\n",
                "        for col in ahead_cols:\n",
                "            s = df[col]\n",
                "\n",
                "            # treat *strictly* 0 as missing, but preserve genuine zeros by\n",
                "            # forward-filling only into 0/NaN slots\n",
                "            s_filled = (\n",
                "                s.replace(0, np.nan)                       # step 1: 0 → NaN\n",
                "                 .groupby(pd.Grouper(freq=freq))           # step 2: group\n",
                "                 .ffill()                                  # step 3: ffill inside group\n",
                "                 .fillna(0)                                # step 4: keep leading zeros\n",
                "            )\n",
                "\n",
                "            df[col] = np.where(\n",
                "                (s == 0) | s.isna(),        # fill only where original was 0/NaN\n",
                "                s_filled,                   #   …with the forward-filled value\n",
                "                s                           # keep genuine entries untouched\n",
                "            )\n",
                "\n",
                "    return df\n",
                "\n",
                "# \n",
                "\n",
                "\n",
                "CREATE_INDICATOR_FEATURES = False\n",
                "CREATE_CYCLICAL_FEATURES = False\n",
                "CREATE_LAGGED_FEATURES = False\n",
                "\n",
                "new_indicator_features = []\n",
                "if CREATE_INDICATOR_FEATURES:\n",
                "    indicator_features_to_create = [col for col in BDZ_filter + target_vars if col in df.columns]\n",
                "    for col in indicator_features_to_create:\n",
                "        df[f'{col}_was_missing'] = df[col].isnull().astype(int)\n",
                "    new_indicator_features = [f'{col}_was_missing' for col in indicator_features_to_create]\n",
                "\n",
                "new_time_features = []\n",
                "if CREATE_CYCLICAL_FEATURES:\n",
                "    df['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24.0)\n",
                "    df['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24.0)\n",
                "    df['dayofweek_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7.0)\n",
                "    df['dayofweek_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7.0)\n",
                "    df['weekofyear_sin'] = np.sin(2 * np.pi * df.index.isocalendar().week / 52.0)\n",
                "    df['weekofyear_cos'] = np.cos(2 * np.pi * df.index.isocalendar().week / 52.0)\n",
                "    new_time_features = ['hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'weekofyear_sin', 'weekofyear_cos']\n",
                "\n",
                "new_lagged_features = []\n",
                "if CREATE_LAGGED_FEATURES:\n",
                "    for target_var in target_vars:\n",
                "        df[f'{target_var}_lag1'] = df[target_var].shift(1)\n",
                "        df[f'{target_var}_lag24'] = df[target_var].shift(24)\n",
                "        df[f'{target_var}_lag168'] = df[target_var].shift(168)\n",
                "    for target_var in target_vars:\n",
                "        new_lagged_features.extend([f'{target_var}_lag1', f'{target_var}_lag24', f'{target_var}_lag168'])\n",
                "\n",
                "df.dropna(axis=1, how=\"all\", inplace=True)\n",
                "\n",
                "exogenous_vars = [c for c in BDZ_filter if c not in target_vars]\n",
                "if CREATE_CYCLICAL_FEATURES:\n",
                "    exogenous_vars.extend(new_time_features)\n",
                "if CREATE_LAGGED_FEATURES:\n",
                "    exogenous_vars.extend(new_lagged_features)\n",
                "if CREATE_INDICATOR_FEATURES:\n",
                "    exogenous_vars.extend(new_indicator_features)\n",
                "exogenous_vars = sorted(list(set(var for var in exogenous_vars if var in df.columns)))\n",
                "\n",
                "# --- Instantiate the model ---\n",
                "model = MaskedLSTM(\n",
                "    input_size=len(exogenous_vars),\n",
                "    hidden_size=best_params[\"hidden_size\"],\n",
                "    num_layers=best_params[\"num_layers\"],\n",
                "    dense_size=best_params[\"dense_size\"],\n",
                "    output_size=len(target_vars),\n",
                "    dropout=best_params[\"dropout\"],\n",
                "    dropout_lstm=best_params[\"dropout_lstm\"],\n",
                "    bidirectional=best_params[\"bidirectional\"],\n",
                ").to(DEVICE)\n",
                "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
                "model.eval()\n",
                "\n",
                "# --- Robust window selection: will always work for any seq_length and date range ---\n",
                "\n",
                "# desired_start = pd.Timestamp(\"2023-09-05\", tz=\"UTC\")\n",
                "# desired_end = pd.Timestamp(\"2023-09-08\", tz=\"UTC\")\n",
                "\n",
                "all_dates = df.index\n",
                "try:\n",
                "    first_pred_idx = all_dates.get_loc(desired_start)\n",
                "except KeyError:\n",
                "    raise ValueError(f\"Desired start {desired_start} not in DataFrame index!\")\n",
                "\n",
                "if first_pred_idx < seq_length:\n",
                "    raise ValueError(f\"Not enough data before {desired_start} to build input sequence with seq_length={seq_length}.\")\n",
                "\n",
                "window_start = all_dates[first_pred_idx - seq_length]\n",
                "window_end = desired_end\n",
                "\n",
                "df_pred_window = df.loc[window_start:window_end].copy()\n",
                "\n",
                "# -- Scale features --\n",
                "X_scaled = scaler_X.transform(df_pred_window[exogenous_vars])\n",
                "y_true = df_pred_window[target_vars].values\n",
                "\n",
                "# -- Build sequences --\n",
                "def create_sequences(X, lookback, horizon=1):\n",
                "    num_samples = X.shape[0] - lookback - horizon + 1\n",
                "    X_seq = np.zeros((num_samples, lookback, X.shape[1]), dtype=np.float32)\n",
                "    mask_seq = np.ones((num_samples, lookback, X.shape[1]), dtype=np.float32)\n",
                "    for i in range(num_samples):\n",
                "        X_seq[i] = X[i : i + lookback]\n",
                "    return X_seq, mask_seq\n",
                "\n",
                "X_seq, mask_seq = create_sequences(X_scaled, seq_length, 1)\n",
                "nan_pos = np.isnan(X_seq)\n",
                "mask_seq[nan_pos] = 0.0\n",
                "X_seq[nan_pos] = 0.0\n",
                "\n",
                "dates_seq = df_pred_window.index[seq_length : seq_length + len(X_seq)]\n",
                "y_true_aligned = y_true[seq_length : seq_length + len(X_seq)]\n",
                "\n",
                "# -- Filter for predictions within the requested window only --\n",
                "mask = (dates_seq >= desired_start) & (dates_seq <= desired_end)\n",
                "dates_seq = dates_seq[mask]\n",
                "y_true_aligned = y_true_aligned[mask]\n",
                "\n",
                "with torch.no_grad():\n",
                "    X_tensor = torch.tensor(X_seq, dtype=torch.float32).to(DEVICE)\n",
                "    mask_tensor = torch.tensor(mask_seq, dtype=torch.float32).to(DEVICE)\n",
                "    y_pred_scaled = model(X_tensor, mask_tensor).cpu().numpy()[:, 0, :]  # (n, target_dim)\n",
                "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
                "y_pred = y_pred[mask]\n",
                "\n",
                "# --- Add price data ---\n",
                "price_data_to_plot = None\n",
                "if price_data_full is not None and price_column_name_to_plot in price_data_full.columns:\n",
                "    try:\n",
                "        # Align price data with dates_seq\n",
                "        price_data_to_plot = price_data_full.loc[dates_seq, price_column_name_to_plot].values.astype(float)\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not align price data with dates_seq: {e}\")\n",
                "        price_data_to_plot = None\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.dates as mdates\n",
                "\n",
                "# --- Colors and markers ---\n",
                "true_color = \"#636efa\"    # Plotly default blue\n",
                "pred_color = \"#ef553b\"    # Plotly default red\n",
                "price_color = \"orange\"\n",
                "\n",
                "# --- Matplotlib plot ---\n",
                "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
                "\n",
                "# True values\n",
                "ax1.plot(dates_seq, y_true_aligned[:, 0], label='True', marker='o', markersize=5, linewidth=2)\n",
                "\n",
                "# Predicted values\n",
                "ax1.plot(dates_seq, y_pred[:, 0], label='Predicted', color=pred_color, marker='x', markersize=5, linewidth=2)\n",
                "\n",
                "ax1.set_xlabel(\"Date\", fontsize=17)\n",
                "ax1.set_ylabel(\"Volume (MW)\", fontsize=17)\n",
                "ax1.tick_params(axis='y', labelsize=16)\n",
                "ax1.tick_params(axis='x', labelsize=16)\n",
                "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%d %b %Y\\n%H:%M'))\n",
                "plt.xticks(rotation=45)\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Add price on secondary y-axis\n",
                "if price_data_to_plot is not None:\n",
                "    ax2 = ax1.twinx()\n",
                "    ax2.plot(dates_seq, price_data_to_plot, label='mFRR Down Price', color=price_color, linewidth=2)\n",
                "    ax2.set_ylabel(\"mFRR Down Price (€/MWh)\", fontsize=17, color=price_color)\n",
                "    ax2.tick_params(axis='y', labelcolor=price_color, labelsize=16)\n",
                "    ax2.grid(False)\n",
                "else:\n",
                "    ax2 = None\n",
                "\n",
                "# Combined legend\n",
                "lines_labels = [ax.get_legend_handles_labels() for ax in [ax1, ax2] if ax is not None]\n",
                "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
                "plt.legend(lines, labels, loc='upper right', fontsize=16, frameon=True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": [
                "ATTR_START = \"2023-09-05 19:00\"\n",
                "ATTR_END   = \"2023-09-06 08:00\"\n",
                "\n",
                "from datetime import datetime\n",
                "ATTR_START_dt = datetime.strptime(ATTR_START, \"%Y-%m-%d %H:%M\")\n",
                "ATTR_END_dt   = datetime.strptime(ATTR_END, \"%Y-%m-%d %H:%M\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## IG"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "attr_summaries = {}       # master container\n",
                "# \n",
                "\n",
                "\n",
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "from captum.attr import IntegratedGradients\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd                 # already imported above\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 0.  CONFIG & OUTPUT DIR\n",
                "# ------------------------------------------------------------\n",
                "plots_dir = \"attribution_plots\"\n",
                "os.makedirs(plots_dir, exist_ok=True)\n",
                "\n",
                "# ATTR_START = \"2023-09-05 19:00\"\n",
                "# ATTR_END   = \"2023-09-06 08:00\"\n",
                "attribution_times = pd.date_range(\n",
                "    ATTR_START, ATTR_END, freq=\"h\", tz=\"UTC\"\n",
                ")\n",
                "print(\"Attribution times:\", attribution_times)\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 1.  BUILD INPUT / MASK TENSORS *** with the SAME preprocessing as inference ***\n",
                "# ------------------------------------------------------------\n",
                "input_seqs, mask_seqs = [], []\n",
                "\n",
                "for t in attribution_times:\n",
                "    if t not in df.index:\n",
                "        print(f\"⚠️  {t} missing in df.index – skipped\");   continue\n",
                "    idx = df.index.get_loc(t)\n",
                "    if idx < seq_length:\n",
                "        print(f\"⚠️  Not enough history before {t} – skipped\");   continue\n",
                "\n",
                "    # raw window (seq_len × features) --------------------------\n",
                "    window_raw = df.iloc[idx - seq_length: idx][exogenous_vars].values\n",
                "\n",
                "    # ---> scale exactly like in inference\n",
                "    window_scaled = scaler_X.transform(window_raw)\n",
                "\n",
                "    # ---> build mask (1 = present, 0 = missing)\n",
                "    mask = (~np.isnan(window_scaled)).astype(np.float32)\n",
                "\n",
                "    # ---> impute NaNs with 0  (model expects this)\n",
                "    window_scaled[np.isnan(window_scaled)] = 0.0\n",
                "\n",
                "    input_seqs.append(window_scaled.astype(np.float32))\n",
                "    mask_seqs.append(mask)\n",
                "\n",
                "if not input_seqs:\n",
                "    raise RuntimeError(\"No valid windows after filtering.\")\n",
                "\n",
                "X_attr      = torch.tensor(np.stack(input_seqs)).to(DEVICE)       # (N, L, F)\n",
                "X_mask_attr = torch.tensor(np.stack(mask_seqs)).to(DEVICE)        # (N, L, F)\n",
                "\n",
                "# quick sanity-check\n",
                "print(\"Any NaNs after preprocessing?  X:\", torch.isnan(X_attr).any().item(),\n",
                "      \" mask:\", torch.isnan(X_mask_attr).any().item())\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 2.  BASELINES  (in scaled space, no NaNs)\n",
                "# ------------------------------------------------------------\n",
                "zeros_X  = torch.zeros_like(X_attr[:1])\n",
                "median_X = torch.median(X_attr, dim=0, keepdim=True).values\n",
                "\n",
                "# --- Compute mean baseline (classic mean) --------------------\n",
                "mean_X = torch.mean(X_attr, dim=0, keepdim=True)\n",
                "mean_M = X_mask_attr[:1]                    # reuse mask for consistency\n",
                "\n",
                "# Sanity check\n",
                "assert not torch.isnan(mean_X).any(), \"mean baseline still has NaNs!\"\n",
                "assert mean_X.shape  == (1, seq_length, len(exogenous_vars))\n",
                "assert mean_M.shape  == (1, seq_length, len(exogenous_vars))\n",
                "\n",
                "baselines = {\n",
                "    \"zeros\": (zeros_X,  X_mask_attr[:1]),\n",
                "    \"median\": (median_X, X_mask_attr[:1]),\n",
                "    \"mean\":   (mean_X,   mean_M),\n",
                "}\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 3.  INTEGRATED GRADIENTS  – compatible with Captum < 0.7\n",
                "# ------------------------------------------------------------\n",
                "def model_forward(x, m):\n",
                "    return model(x, m)            # (batch, 1, 1)\n",
                "\n",
                "ig = IntegratedGradients(model_forward)\n",
                "\n",
                "attr_results = {}\n",
                "for name, (base_x, base_m) in baselines.items():\n",
                "    # >>>> the 2-tuple style works in every Captum version\n",
                "    (attr_x, attr_mask), delta = ig.attribute(\n",
                "        inputs=(X_attr, X_mask_attr),\n",
                "        baselines=(base_x.expand_as(X_attr),\n",
                "                   base_m.expand_as(X_mask_attr)),\n",
                "        target=0,\n",
                "        n_steps=64,\n",
                "        method=\"riemann_trapezoid\",\n",
                "        internal_batch_size=32,\n",
                "        return_convergence_delta=True\n",
                "    )\n",
                "\n",
                "    print(f\"{name:9s} | attr range [{attr_x.min():.2e}, {attr_x.max():.2e}] \"\n",
                "          f\"δ-mean={delta.abs().mean():.2e}\")\n",
                "\n",
                "    attr_results[name] = attr_x.cpu().numpy()   # (N, L, F) – keep only X\n",
                "\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 4.  AGGREGATE IMPORTANCE  (unchanged)\n",
                "#     • |attr|  →  mean over (sample, time)  →  normalise to %\n",
                "# ------------------------------------------------------------\n",
                "attr_summary = {}\n",
                "for name, a in attr_results.items():\n",
                "    importance = np.mean(np.abs(a), axis=(0, 1))  # (F,)\n",
                "    importance /= importance.sum() + 1e-12        # convert to %\n",
                "    attr_summary[name] = importance\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 5.  PLOT\n",
                "# ------------------------------------------------------------\n",
                "top_k   = 10\n",
                "colors  = [\"#3498db\", \"#2ecc71\", \"#e74c3c\"]   # enough for 3 baselines\n",
                "fig, axes = plt.subplots(2, 3, figsize=(20, 8))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, (name, imp) in enumerate(attr_summary.items()):\n",
                "    idx_sorted = np.argsort(imp)[-top_k:][::-1]\n",
                "    feature_indices = idx_sorted      # indices into exogenous_vars\n",
                "    ids = [f\"#{idx+1}\" for idx in feature_indices]   # consistent ID = index+1\n",
                "\n",
                "    # --- bars ---\n",
                "    ax = axes[i]\n",
                "    ax.bar(range(top_k), imp[feature_indices] * 100, color=colors[i])       # %\n",
                "    ax.set_xticks(range(top_k))\n",
                "    ax.set_xticklabels(ids)\n",
                "    ax.set_xlabel(\"Feature ID\")\n",
                "    ax.set_ylabel(\"Contribution (%)\")\n",
                "    ax.set_title(f\"Integrated Gradients – {name}\")\n",
                "\n",
                "    # --- table ---\n",
                "    ax_t = axes[i + 3]\n",
                "    table_data = [[ids[j], exogenous_vars[feature_indices[j]]]\n",
                "                  for j in range(top_k)]\n",
                "    ax_t.axis(\"off\")\n",
                "    ax_t.set_title(f\"{name}  mapping\", pad=10)\n",
                "    tbl = ax_t.table(\n",
                "        cellText=table_data, colLabels=[\"ID\", \"Feature\"],\n",
                "        cellLoc=\"left\", loc=\"center\",\n",
                "        colWidths=[0.07, 0.93],\n",
                "    )\n",
                "    tbl.auto_set_font_size(False)\n",
                "    tbl.set_fontsize(9)\n",
                "    tbl.scale(1.2, 1.2)\n",
                "\n",
                "\n",
                "plt.suptitle(\"Feature Attribution (Integrated Gradients)\\n\"\n",
                "             f\"{ATTR_START} – {ATTR_END}\", fontsize=16)\n",
                "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
                "\n",
                "out_path = os.path.join(plots_dir, \"integrated_gradients_selected_window.png\")\n",
                "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
                "plt.show()\n",
                "print(\"Saved ➜\", out_path)\n",
                "\n",
                "attr_summaries[\"IG\"] = {k: v.copy() for k, v in attr_summary.items()}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## DL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, numpy as np, torch, matplotlib.pyplot as plt, pandas as pd\n",
                "from captum.attr import DeepLiftShap\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 0.  CONFIG & OUTPUT DIR\n",
                "# ------------------------------------------------------------\n",
                "plots_dir = \"attribution_plots\";  os.makedirs(plots_dir, exist_ok=True)\n",
                "# ATTR_START = \"2023-09-05 19:00\"\n",
                "# ATTR_END = \"2023-09-06 08:00\"\n",
                "attribution_times = pd.date_range(ATTR_START, ATTR_END, freq=\"h\", tz=\"UTC\")\n",
                "print(\"Attribution times:\", attribution_times)\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 1.  BUILD INPUT / MASK TENSORS (unchanged)\n",
                "# ------------------------------------------------------------\n",
                "input_seqs, mask_seqs = [], []\n",
                "for t in attribution_times:\n",
                "    if t not in df.index:\n",
                "        print(f\"⚠️  {t} missing in df.index – skipped\");   continue\n",
                "    idx = df.index.get_loc(t)\n",
                "    if idx < seq_length:\n",
                "        print(f\"⚠️  Not enough history before {t} – skipped\");   continue\n",
                "    window_raw = df.iloc[idx - seq_length: idx][exogenous_vars].values\n",
                "    window_scaled = scaler_X.transform(window_raw)\n",
                "    mask = (~np.isnan(window_scaled)).astype(np.float32)\n",
                "    window_scaled[np.isnan(window_scaled)] = 0.0\n",
                "    input_seqs.append(window_scaled.astype(np.float32));  mask_seqs.append(mask)\n",
                "\n",
                "if not input_seqs:  raise RuntimeError(\"No valid windows after filtering.\")\n",
                "\n",
                "X_attr      = torch.tensor(np.stack(input_seqs)).to(DEVICE)      # (N, L, F)\n",
                "X_mask_attr = torch.tensor(np.stack(mask_seqs)).to(DEVICE)       # (N, L, F)\n",
                "print(\"Any NaNs? X:\", torch.isnan(X_attr).any().item(),\n",
                "      \"mask:\", torch.isnan(X_mask_attr).any().item())\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 2.  BASELINES (mean instead of pre-spike)\n",
                "# ------------------------------------------------------------\n",
                "zeros_X  = torch.zeros_like(X_attr[:1])\n",
                "median_X = torch.median(X_attr, dim=0, keepdim=True).values\n",
                "\n",
                "# Compute mean baseline over all valid attribution windows\n",
                "mean_raw = []\n",
                "for t in attribution_times:\n",
                "    idx = df.index.get_loc(t)\n",
                "    if idx < seq_length:\n",
                "        continue\n",
                "    window_raw = df.iloc[idx - seq_length: idx][exogenous_vars]\n",
                "    mean_raw.append(window_raw.values)\n",
                "if not mean_raw:\n",
                "    raise ValueError(\"No valid windows to compute mean baseline.\")\n",
                "mean_raw = np.mean(np.stack(mean_raw), axis=0)  # shape: (L, F)\n",
                "mean_scaled = scaler_X.transform(mean_raw)\n",
                "mask_mean = (~np.isnan(mean_scaled)).astype(np.float32)\n",
                "mean_scaled[np.isnan(mean_scaled)] = 0.0\n",
                "baseline_mean_X = torch.tensor(mean_scaled, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
                "baseline_mean_M = torch.tensor(mask_mean,        dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
                "\n",
                "baselines = {\n",
                "    \"zeros\":     (zeros_X,        X_mask_attr[:1]),\n",
                "    \"median\":    (median_X,       X_mask_attr[:1]),\n",
                "    \"mean\":      (baseline_mean_X, baseline_mean_M),\n",
                "}\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 3.  DEEPLIFT SHAP (wrapped model)\n",
                "# ------------------------------------------------------------\n",
                "class Wrapper(torch.nn.Module):\n",
                "    def __init__(self, core):  super().__init__();  self.core = core\n",
                "    def forward(self, x, m):   return self.core(x, m)\n",
                "\n",
                "dlshap = DeepLiftShap(Wrapper(model))\n",
                "\n",
                "attr_results = {}\n",
                "for name, (base_x, base_m) in baselines.items():\n",
                "    (attr_x, _attr_m), delta = dlshap.attribute(\n",
                "        inputs=(X_attr, X_mask_attr),\n",
                "        baselines=(base_x.expand_as(X_attr),\n",
                "                   base_m.expand_as(X_mask_attr)),\n",
                "        target=0,\n",
                "        return_convergence_delta=True\n",
                "    )\n",
                "    print(f\"{name:9s} | attr range \"\n",
                "          f\"[{attr_x.min():.2e}, {attr_x.max():.2e}] \"\n",
                "          f\"δ-mean={delta.abs().mean():.2e}\")\n",
                "    attr_results[name] = attr_x.detach().cpu().numpy()   # (N, L, F)\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 4.  AGGREGATE IMPORTANCE (unchanged)\n",
                "# ------------------------------------------------------------\n",
                "attr_summary = {}\n",
                "for name, a in attr_results.items():\n",
                "    importance = np.mean(np.abs(a), axis=(0, 1))\n",
                "    importance /= importance.sum() + 1e-12\n",
                "    attr_summary[name] = importance\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 5.  PLOT (only titles & file name changed)\n",
                "# ------------------------------------------------------------\n",
                "top_k, colors = 10, [\"#3498db\", \"#2ecc71\", \"#e74c3c\"]\n",
                "fig, axes = plt.subplots(2, 3, figsize=(20, 8));  axes = axes.flatten()\n",
                "\n",
                "for i, (name, imp) in enumerate(attr_summary.items()):\n",
                "    idx_sorted = np.argsort(imp)[-top_k:][::-1];  ids = [f\"#{idx+1}\" for idx in idx_sorted]\n",
                "    ax = axes[i]\n",
                "    ax.bar(range(top_k), imp[idx_sorted] * 100, color=colors[i])\n",
                "    ax.set_xticks(range(top_k));  ax.set_xticklabels(ids)\n",
                "    ax.set_xlabel(\"Feature ID\");   ax.set_ylabel(\"Contribution (%)\")\n",
                "    ax.set_title(f\"DeepLift SHAP – {name}\")\n",
                "    ax_t = axes[i + 3];  ax_t.axis(\"off\")\n",
                "    ax_t.set_title(f\"{name}  mapping\", pad=10)\n",
                "    tbl = ax_t.table(cellText=[[ids[j], exogenous_vars[idx_sorted[j]]] for j in range(top_k)],\n",
                "                     colLabels=[\"ID\", \"Feature\"], cellLoc=\"left\", loc=\"center\",\n",
                "                     colWidths=[0.07, 0.93])\n",
                "    tbl.auto_set_font_size(False); tbl.set_fontsize(9); tbl.scale(1.2, 1.2)\n",
                "\n",
                "plt.suptitle(\"Feature Attribution (DeepLift SHAP)\\n\"\n",
                "             f\"{ATTR_START} – {ATTR_END}\", fontsize=16)\n",
                "plt.tight_layout(rect=[0,0,1,0.96])\n",
                "out_path = os.path.join(plots_dir, \"deeplift_shap_selected_window.png\")\n",
                "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
                "plt.show();  print(\"Saved ➜\", out_path)\n",
                "\n",
                "attr_summaries[\"DL_SHAP\"] = {k: v.copy() for k, v in attr_summary.items()}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## FA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "from captum.attr import FeatureAblation\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 0.  CONFIG & OUTPUT DIR\n",
                "# ------------------------------------------------------------\n",
                "plots_dir = \"attribution_plots\"\n",
                "os.makedirs(plots_dir, exist_ok=True)\n",
                "\n",
                "# ATTR_START = \"2023-09-05 19:00\"\n",
                "# ATTR_END   = \"2023-09-06 08:00\"\n",
                "attribution_times = pd.date_range(\n",
                "    ATTR_START, ATTR_END, freq=\"h\", tz=\"UTC\"\n",
                ")\n",
                "print(\"Attribution times:\", attribution_times)\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 1.  BUILD INPUT / MASK TENSORS  *** SAME AS INFERENCE ***\n",
                "# ------------------------------------------------------------\n",
                "input_seqs, mask_seqs = [], []\n",
                "\n",
                "for t in attribution_times:\n",
                "    if t not in df.index:\n",
                "        print(f\"⚠️  {t} missing in df.index – skipped\");   continue\n",
                "    idx = df.index.get_loc(t)\n",
                "    if idx < seq_length:\n",
                "        print(f\"⚠️  Not enough history before {t} – skipped\");   continue\n",
                "\n",
                "    # raw window (seq_len × features) --------------------------\n",
                "    window_raw = df.iloc[idx - seq_length: idx][exogenous_vars].values\n",
                "\n",
                "    # ---> scale exactly like in inference\n",
                "    window_scaled = scaler_X.transform(window_raw)\n",
                "\n",
                "    # ---> build mask (1 = present, 0 = missing)\n",
                "    mask = (~np.isnan(window_scaled)).astype(np.float32)\n",
                "\n",
                "    # ---> impute NaNs with 0  (model expects this)\n",
                "    window_scaled[np.isnan(window_scaled)] = 0.0\n",
                "\n",
                "    input_seqs.append(window_scaled.astype(np.float32))\n",
                "    mask_seqs.append(mask)\n",
                "\n",
                "if not input_seqs:\n",
                "    raise RuntimeError(\"No valid windows after filtering.\")\n",
                "\n",
                "X_attr      = torch.tensor(np.stack(input_seqs)).to(DEVICE)       # (N, L, F)\n",
                "X_mask_attr = torch.tensor(np.stack(mask_seqs)).to(DEVICE)        # (N, L, F)\n",
                "\n",
                "# quick sanity-check\n",
                "print(\"Any NaNs after preprocessing?  X:\", torch.isnan(X_attr).any().item(),\n",
                "      \" mask:\", torch.isnan(X_mask_attr).any().item())\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 2.  BASELINES  (in scaled space, no NaNs) – USE MEAN INSTEAD OF PRE-SPIKE\n",
                "# ------------------------------------------------------------\n",
                "zeros_X  = torch.zeros_like(X_attr[:1])\n",
                "median_X = torch.median(X_attr, dim=0, keepdim=True).values\n",
                "\n",
                "# --- Compute mean baseline over all valid attribution windows ---\n",
                "mean_raw = []\n",
                "for t in attribution_times:\n",
                "    idx = df.index.get_loc(t)\n",
                "    if idx < seq_length:\n",
                "        continue\n",
                "    window_raw = df.iloc[idx - seq_length: idx][exogenous_vars]\n",
                "    mean_raw.append(window_raw.values)\n",
                "if not mean_raw:\n",
                "    raise ValueError(\"No valid windows to compute mean baseline.\")\n",
                "mean_raw = np.mean(np.stack(mean_raw), axis=0)  # shape: (L, F)\n",
                "mean_scaled = scaler_X.transform(mean_raw)\n",
                "mask_mean = (~np.isnan(mean_scaled)).astype(np.float32)\n",
                "mean_scaled[np.isnan(mean_scaled)] = 0.0\n",
                "baseline_mean_X = torch.tensor(mean_scaled, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
                "baseline_mean_M = torch.tensor(mask_mean,        dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
                "\n",
                "baselines = {\n",
                "    \"zeros\":     (zeros_X,   X_mask_attr[:1]),\n",
                "    \"median\":    (median_X,  X_mask_attr[:1]),\n",
                "    # \"mean\":      (baseline_mean_X, baseline_mean_M),\n",
                "}\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 3.  FEATURE ABLATION  – Captum\n",
                "# ------------------------------------------------------------\n",
                "def model_forward(x, m):\n",
                "    return model(x, m)            # (batch, 1, 1)\n",
                "\n",
                "fa = FeatureAblation(model_forward)\n",
                "\n",
                "attr_results = {}\n",
                "for name, (base_x, base_m) in baselines.items():\n",
                "    # >>>> Feature Ablation call mirrors previous IG signature\n",
                "    attr_tuple = fa.attribute(\n",
                "        inputs=(X_attr, X_mask_attr),\n",
                "        baselines=(base_x.expand_as(X_attr),\n",
                "                   base_m.expand_as(X_mask_attr)),\n",
                "        target=0,\n",
                "        perturbations_per_eval=32,    # controls internal batching like internal_batch_size\n",
                "        feature_mask=None             # default: each scalar is its own feature\n",
                "    )\n",
                "\n",
                "    # attr_tuple is (attr_X, attr_mask)\n",
                "    attr_x, _ = attr_tuple\n",
                "    print(f\"{name:9s} | attr range [{attr_x.min():.2e}, {attr_x.max():.2e}]\")\n",
                "\n",
                "    attr_results[name] = attr_x.cpu().numpy()   # (N, L, F) – keep only X\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 4.  AGGREGATE IMPORTANCE  (unchanged)\n",
                "#     • |attr|  →  mean over (sample, time)  →  normalise to %\n",
                "# ------------------------------------------------------------\n",
                "attr_summary = {}\n",
                "for name, a in attr_results.items():\n",
                "    importance = np.mean(np.abs(a), axis=(0, 1))  # (F,)\n",
                "    importance /= importance.sum() + 1e-12        # convert to %\n",
                "    attr_summary[name] = importance\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 5.  PLOT\n",
                "# ------------------------------------------------------------\n",
                "top_k   = 10\n",
                "colors  = [\"#3498db\", \"#2ecc71\", \"#e74c3c\"]\n",
                "fig, axes = plt.subplots(2, 3, figsize=(20, 8))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, (name, imp) in enumerate(attr_summary.items()):\n",
                "    idx_sorted = np.argsort(imp)[-top_k:][::-1]\n",
                "    feature_indices = idx_sorted      # indices into exogenous_vars\n",
                "    ids = [f\"#{idx+1}\" for idx in feature_indices]   # consistent ID = index+1\n",
                "\n",
                "    # --- bars ---\n",
                "    ax = axes[i]\n",
                "    ax.bar(range(top_k), imp[feature_indices] * 100, color=colors[i])       # %\n",
                "    ax.set_xticks(range(top_k))\n",
                "    ax.set_xticklabels(ids)\n",
                "    ax.set_xlabel(\"Feature ID\")\n",
                "    ax.set_ylabel(\"Contribution (%)\")\n",
                "    ax.set_title(f\"Feature Ablation – {name}\")\n",
                "\n",
                "    # --- table ---\n",
                "    ax_t = axes[i + 3]\n",
                "    table_data = [[ids[j], exogenous_vars[feature_indices[j]]]\n",
                "                  for j in range(top_k)]\n",
                "    ax_t.axis(\"off\")\n",
                "    ax_t.set_title(f\"{name}  mapping\", pad=10)\n",
                "    tbl = ax_t.table(\n",
                "        cellText=table_data, colLabels=[\"ID\", \"Feature\"],\n",
                "        cellLoc=\"left\", loc=\"center\",\n",
                "        colWidths=[0.07, 0.93],\n",
                "    )\n",
                "    tbl.auto_set_font_size(False)\n",
                "    tbl.set_fontsize(9)\n",
                "    tbl.scale(1.2, 1.2)\n",
                "\n",
                "plt.suptitle(\"Feature Attribution (Feature Ablation)\\n\"\n",
                "             f\"{ATTR_START} – {ATTR_END}\", fontsize=16)\n",
                "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
                "\n",
                "out_path = os.path.join(plots_dir, \"feature_ablation_selected_window.png\")\n",
                "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
                "plt.show()\n",
                "print(\"Saved ➜\", out_path)\n",
                "\n",
                "attr_summaries[\"FA\"] = {k: v.copy() for k, v in attr_summary.items()}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Top params"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ------------------------------------------------------------------\n",
                "# ❶  PARAMETERS & SANITY-CHECK\n",
                "# ------------------------------------------------------------------\n",
                "top_k = 10\n",
                "methods    = list(attr_summaries.keys())                     # ['IG', 'DL_SHAP', 'FA']\n",
                "baselines  = list(attr_summaries[methods[0]].keys())         # ['zeros', 'median', 'pre_spike']\n",
                "\n",
                "print(\"Methods   :\", methods)\n",
                "print(\"Baselines :\", baselines)\n",
                "\n",
                "# ------------------------------------------------------------------\n",
                "# ❷  COLLECT UNIQUE INDICES PER BASELINE\n",
                "# ------------------------------------------------------------------\n",
                "baseline_to_idxs = {b: set() for b in baselines}             # { 'zeros': set(), ... }\n",
                "\n",
                "for method in methods:\n",
                "    for baseline, importance in attr_summaries[method].items():\n",
                "        top_idx = np.argsort(importance)[-top_k:]            # this method+baseline’s top-k\n",
                "        baseline_to_idxs[baseline].update(top_idx)           # accumulate in the set\n",
                "\n",
                "# ------------------------------------------------------------------\n",
                "# ❸  REPORT\n",
                "# ------------------------------------------------------------------\n",
                "for baseline in baselines:\n",
                "    idxs  = sorted(baseline_to_idxs[baseline])\n",
                "    names = [exogenous_vars[i] for i in idxs]\n",
                "\n",
                "    print(f\"\\nBaseline: {baseline}\")\n",
                "    print(f\"Unique top {top_k} features across {len(methods)} methods: {len(names)}\")\n",
                "    print(\"-\"*60)\n",
                "    for n, feat in enumerate(names, 1):\n",
                "        print(f\"{n:2d}. {feat}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Top param plots"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib as mpl\n",
                "\n",
                "# --------------------------------------------------\n",
                "# ❶ CONFIG - Enhanced for professional presentation\n",
                "# --------------------------------------------------\n",
                "top_k = 10\n",
                "methods = [\"IG\", \"DL_SHAP\", \"FA\"]\n",
                "# Scientific color palette (colorblind-friendly)\n",
                "colors = [\"#4477AA\", \"#66CCEE\", \"#EE6677\"]  \n",
                "bar_width = 0.25\n",
                "font_family = 'serif'  # Academic standard\n",
                "\n",
                "# Set overall matplotlib style for thesis-quality figures\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "mpl.rcParams['font.family'] = font_family\n",
                "mpl.rcParams['font.size'] = 16\n",
                "mpl.rcParams['axes.titlesize'] = 18\n",
                "mpl.rcParams['axes.labelsize'] = 17\n",
                "mpl.rcParams['xtick.labelsize'] = 16\n",
                "mpl.rcParams['ytick.labelsize'] = 16\n",
                "mpl.rcParams['legend.fontsize'] = 16\n",
                "mpl.rcParams['figure.titlesize'] = 20\n",
                "\n",
                "# --------------------------------------------------\n",
                "# ❷ COLLECT UNIQUE TOP-k FEATURES PER BASELINE (exclude 'mean')\n",
                "# --------------------------------------------------\n",
                "all_baselines = list(attr_summaries[methods[0]].keys())\n",
                "baselines = [b for b in all_baselines if b != \"mean\"]\n",
                "baseline_to_idxs = {b: set() for b in baselines}\n",
                "\n",
                "for m in methods:\n",
                "    for b, imp in attr_summaries[m].items():\n",
                "        if b == \"mean\":\n",
                "            continue\n",
                "        top_idx = np.argsort(imp)[-top_k:]\n",
                "        baseline_to_idxs[b].update(top_idx)\n",
                "\n",
                "# --------------------------------------------------\n",
                "# ❸ BUILD THE FIGURE - Enhanced for thesis presentation\n",
                "# --------------------------------------------------\n",
                "fig, axes = plt.subplots(len(baselines), 1,\n",
                "                         figsize=(10, 3.5*len(baselines)),  # More compact, thesis-friendly ratio\n",
                "                         constrained_layout=True)  # Better spacing management\n",
                "\n",
                "# Title and subtitle removed per user request\n",
                "\n",
                "if len(baselines) == 1:\n",
                "    axes = [axes]\n",
                "\n",
                "# Corrected method names for legend\n",
                "method_names = {\n",
                "    \"IG\": \"Integrated Gradient\",\n",
                "    \"DL_SHAP\": \"DeepLIFT SHAP\",\n",
                "    \"FA\": \"Feature Ablation\"\n",
                "}\n",
                "\n",
                "for row, baseline in enumerate(baselines):\n",
                "    ax = axes[row]\n",
                "    idxs = sorted(baseline_to_idxs[baseline])\n",
                "    n = len(idxs)\n",
                "    \n",
                "    # x-locations for the centre of each \"feature group\"\n",
                "    x_centres = np.arange(n)\n",
                "    \n",
                "    # Draw bars with enhanced styling\n",
                "    for j, (method, color) in enumerate(zip(methods, colors)):\n",
                "        imp = attr_summaries[method][baseline] * 100\n",
                "        heights = imp[idxs]\n",
                "        \n",
                "        # Add bars with hatching for better distinction in grayscale printing\n",
                "        hatch_patterns = ['', '///', '...']\n",
                "        ax.bar(x_centres + (j-1)*bar_width, heights,\n",
                "               width=bar_width, color=color, alpha=0.85,\n",
                "               hatch=hatch_patterns[j], \n",
                "               label=method_names[method] if row==0 else \"\",\n",
                "               edgecolor='black', linewidth=0.5)\n",
                "    \n",
                "    # Enhanced subplot styling\n",
                "    ax.set_title(f\"Baseline: {baseline.capitalize()}\", loc='left', fontweight='normal')\n",
                "    ax.set_ylabel(\"Contribution (%)\", fontweight='normal')\n",
                "    ax.set_xlabel(\"Feature Index\", fontweight='normal')\n",
                "    ax.set_xticks(x_centres)\n",
                "    \n",
                "    # Improved tick labels with smaller rotation for readability\n",
                "    feature_labels = [f\"#{i+1}\" for i in idxs]\n",
                "    ax.set_xticklabels(feature_labels, rotation=45, ha='right')\n",
                "    \n",
                "    # Refined grid\n",
                "    ax.grid(axis=\"y\", alpha=0.3, linestyle='--')\n",
                "    \n",
                "    # Add spines for more professional look\n",
                "    for spine in ax.spines.values():\n",
                "        spine.set_visible(True)\n",
                "        spine.set_linewidth(0.75)\n",
                "    \n",
                "    # Add a horizontal line at y=0\n",
                "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8, alpha=0.5)\n",
                "\n",
                "# Create legend with enhanced styling - top right with frame\n",
                "handles, labels = axes[0].get_legend_handles_labels()\n",
                "legend = fig.legend(handles, labels, loc='upper center', \n",
                "                   bbox_to_anchor=(0.5, 0.01),  # Below all subplots\n",
                "                   ncol=len(methods), frameon=True, \n",
                "                   fancybox=True, shadow=True)\n",
                "\n",
                "# Figure caption removed as requested\n",
                "\n",
                "plt.savefig('feature_importance_comparison.pdf', bbox_inches='tight', dpi=300)\n",
                "plt.savefig('feature_importance_comparison.png', bbox_inches='tight', dpi=300)\n",
                "plt.show()\n",
                "\n",
                "# --------------------------------------------------\n",
                "# ❹ PRINT THE \"# – Feature name\" LISTS (with enhanced formatting)\n",
                "# --------------------------------------------------\n",
                "print(\"\\nTable X: Feature Index Mapping\")\n",
                "print(\"=\" * 50)\n",
                "for baseline in baselines:\n",
                "    idxs = sorted(baseline_to_idxs[baseline])\n",
                "    print(f\"\\nBaseline: {baseline.capitalize()}\")\n",
                "    print(\"-\" * 40)\n",
                "    print(f\"{'Index':<8} {'Feature Name':<30}\")\n",
                "    print(\"-\" * 40)\n",
                "    for i in idxs:\n",
                "        print(f\"#{i+1:<7} {exogenous_vars[i]:<30}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature plots"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib as mpl\n",
                "from math import ceil\n",
                "import matplotlib.dates as mdates\n",
                "from matplotlib.ticker import MaxNLocator\n",
                "import textwrap\n",
                "\n",
                "# --------------------------------------------------\n",
                "# ❶ USER CONFIG - Enhanced for thesis presentation\n",
                "# --------------------------------------------------\n",
                "top_n = 6                                     # how many features to show\n",
                "ATTR_START = pd.Timestamp(\"2023-09-01 19:00\", tz=\"UTC\")\n",
                "ATTR_END = pd.Timestamp(\"2023-09-10 08:00\", tz=\"UTC\")\n",
                "PEAK_START = pd.Timestamp(\"2023-09-05 19:00\", tz=\"UTC\")\n",
                "PEAK_END = pd.Timestamp(\"2023-09-06 08:00\", tz=\"UTC\")\n",
                "\n",
                "\n",
                "\n",
                "plot_style = \"subplots\"\n",
                "use_zscore = False\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "font_family = 'serif'\n",
                "mpl.rcParams['font.family'] = font_family\n",
                "mpl.rcParams['font.size'] = 12\n",
                "mpl.rcParams['axes.titlesize'] = 14\n",
                "mpl.rcParams['axes.labelsize'] = 13\n",
                "mpl.rcParams['xtick.labelsize'] = 11\n",
                "mpl.rcParams['ytick.labelsize'] = 11\n",
                "mpl.rcParams['legend.fontsize'] = 12\n",
                "mpl.rcParams['figure.titlesize'] = 16\n",
                "\n",
                "# ---------- DETERMINISTIC COLOR ASSIGNMENT FOR GIVEN INDEXES ----------\n",
                "special_indexes = [7, 15, 36, 35, 33, 30, 17, 34, 18]\n",
                "special_indexes_sorted = sorted(special_indexes)  # [7, 15, 17, 18, 30, 33, 34, 35, 36]\n",
                "special_color_map = {}\n",
                "\n",
                "# Choose a color palette (can be larger than 9 if you like)\n",
                "color_palette = plt.get_cmap(\"tab10\", len(special_indexes_sorted)).colors\n",
                "for i, idx in enumerate(special_indexes_sorted):\n",
                "    special_color_map[idx] = color_palette[i]\n",
                "\n",
                "def get_feature_color(idx, default_palette=plt.cm.tab10.colors):\n",
                "    if idx in special_color_map:\n",
                "        return special_color_map[idx]\n",
                "    # fallback to default cycle for non-specified indexes\n",
                "    return default_palette[idx % len(default_palette)]\n",
                "\n",
                "# ------------ EXAMPLE: REST OF YOUR CODE BELOW ----------------------\n",
                "\n",
                "\n",
                "methods = [\"IG\", \"DL_SHAP\", \"FA\"]\n",
                "method_names = {\n",
                "    \"IG\": \"Integrated Gradient\",\n",
                "    \"DL_SHAP\": \"DeepLIFT SHAP\",\n",
                "    \"FA\": \"Feature Ablation\"\n",
                "}\n",
                "all_baselines = list(attr_summaries[methods[0]].keys())\n",
                "baselines = [b for b in all_baselines if b != \"mean\"]\n",
                "\n",
                "attr_pct = {}\n",
                "for m in methods:\n",
                "    attr_pct[m] = {}\n",
                "    for b in baselines:\n",
                "        v = attr_summaries[m][b].astype(float)\n",
                "        v = v / (v.sum() + 1e-12)\n",
                "        attr_pct[m][b] = v\n",
                "\n",
                "all_vectors = [attr_pct[m][b] for m in methods for b in baselines]\n",
                "mean_imp = np.mean(all_vectors, axis=0)\n",
                "top_idx = np.argsort(mean_imp)[-top_n:][::-1]\n",
                "top_names = [exogenous_vars[i] for i in top_idx]\n",
                "\n",
                "slice_df = df.loc[ATTR_START:ATTR_END, top_names]\n",
                "if use_zscore and plot_style == \"overlay\":\n",
                "    slice_df = (slice_df - slice_df.mean()) / slice_df.std(ddof=0)\n",
                "\n",
                "# --------------------------------------------------\n",
                "# ❺ PLOT - Enhanced for thesis quality: 3x2 subplots, wrapped titles\n",
                "# --------------------------------------------------\n",
                "def get_time_locator(start, end):\n",
                "    duration_hours = (end - start).total_seconds() / 3600\n",
                "    if duration_hours <= 48:\n",
                "        return mdates.HourLocator(interval=2), mdates.DateFormatter('%H:%M\\n%d-%b')\n",
                "    elif duration_hours <= 96:\n",
                "        return mdates.HourLocator(interval=6), mdates.DateFormatter('%d-%b\\n%H:%M')\n",
                "    elif duration_hours <= 168:\n",
                "        return mdates.HourLocator(interval=12), mdates.DateFormatter('%d-%b\\n%H:%M')\n",
                "    else:\n",
                "        return mdates.DayLocator(interval=1), mdates.DateFormatter('%d-%b')\n",
                "\n",
                "if plot_style == \"overlay\":\n",
                "    fig, ax = plt.subplots(figsize=(10, 6))\n",
                "    ax.axvspan(PEAK_START, PEAK_END, alpha=0.15, color='gray', label='Peak Period')\n",
                "    line_styles = ['-', '--', ':', '-.', '-', '--']\n",
                "    for i, (name, style) in enumerate(zip(top_names, line_styles)):\n",
                "        color = get_feature_color(top_idx[i])\n",
                "        ax.plot(slice_df.index, slice_df[name],\n",
                "                linewidth=2.5,\n",
                "                color=color,\n",
                "                linestyle=style,\n",
                "                label=f\"#{top_idx[i]+1} {name}\")\n",
                "    locator, formatter = get_time_locator(ATTR_START, ATTR_END)\n",
                "    ax.xaxis.set_major_locator(locator)\n",
                "    ax.xaxis.set_major_formatter(formatter)\n",
                "    ax.set_title(\"Temporal Evolution of Top Feature Contributions\", fontweight='normal', pad=15)\n",
                "    ax.set_ylabel(\"Z-Score\" if use_zscore else \"Feature Value\", fontweight='normal')\n",
                "    ax.set_xlabel(\"Date\", fontweight='normal')\n",
                "    ax.grid(alpha=0.3, linestyle='--')\n",
                "    for spine in ax.spines.values():\n",
                "        spine.set_visible(True)\n",
                "        spine.set_linewidth(0.75)\n",
                "    handles, labels = ax.get_legend_handles_labels()\n",
                "    if 'Peak Period' not in labels:\n",
                "        from matplotlib.patches import Patch\n",
                "        handles.append(Patch(facecolor='gray', alpha=0.15))\n",
                "        labels.append('Peak Period (23:00-05:00)')\n",
                "    ax.legend(handles, labels, bbox_to_anchor=(1.01, 1), loc=\"bottom left\", frameon=True, fancybox=True, shadow=True)\n",
                "    fig.tight_layout()\n",
                "\n",
                "elif plot_style == \"subplots\":\n",
                "    nrows, ncols = 3, 2\n",
                "    fig, axes = plt.subplots(nrows, ncols, figsize=(13, 9), sharex=True, constrained_layout=True)\n",
                "    axes = axes.flatten()\n",
                "    fig.suptitle(\"Temporal Evolution of Key Feature Contributions (Case 1)\", fontweight='normal', y=1.06)\n",
                "    wrap_width = 65\n",
                "\n",
                "    locator, formatter = get_time_locator(ATTR_START, ATTR_END)\n",
                "\n",
                "    for i, (ax, idx, name) in enumerate(zip(axes, top_idx, top_names)):\n",
                "        color = get_feature_color(idx)\n",
                "        ax.axvspan(PEAK_START, PEAK_END, alpha=0.3, color='gray')\n",
                "        ax.plot(slice_df.index, slice_df[name], linewidth=2.5, color=color)\n",
                "        feature_title = f\"#{idx+1}: {name} ({mean_imp[idx]*100:.2f}%)\"\n",
                "        feature_title_wrapped = \"\\n\".join(textwrap.wrap(feature_title, wrap_width))\n",
                "        ax.set_title(feature_title_wrapped, loc='left', fontweight='normal', fontsize=11)\n",
                "        ax.set_ylabel(\"Value\", fontweight='normal')\n",
                "        ax.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
                "        ax.grid(alpha=0.3, linestyle='--')\n",
                "        for spine in ax.spines.values():\n",
                "            spine.set_visible(True)\n",
                "            spine.set_linewidth(0.75)\n",
                "        val_min = slice_df[name].min()\n",
                "        val_max = slice_df[name].max()\n",
                "        if val_min == val_max:\n",
                "            delta = val_min * 0.01 if val_min != 0 else 0.01\n",
                "            ax.axhspan(val_min - delta, val_max + delta, alpha=0.1, color=color)\n",
                "        else:\n",
                "            ax.axhspan(val_min, val_max, alpha=0.1, color=color)\n",
                "        if i == 0:\n",
                "            ax.text(0.98, 0.95, 'Peak Period', transform=ax.transAxes,\n",
                "                    bbox=dict(facecolor='gray', alpha=0.3, edgecolor='none', pad=3),\n",
                "                    ha='right', va='top', fontsize=10)\n",
                "    for j in range(top_n, nrows * ncols):\n",
                "        fig.delaxes(axes[j])\n",
                "    for ax in axes[-ncols:]:\n",
                "        ax.xaxis.set_major_locator(locator)\n",
                "        ax.xaxis.set_major_formatter(formatter)\n",
                "        ax.set_xlabel(\"Date\", fontweight='normal')\n",
                "    period_text = (f\"Period: {ATTR_START.strftime('%Y-%m-%d %H:%M')} to {ATTR_END.strftime('%Y-%m-%d %H:%M')} UTC | \"\n",
                "                  f\"Peak Activity: {PEAK_START.strftime('%Y-%m-%d %H:%M')} to {PEAK_END.strftime('%Y-%m-%d %H:%M')} UTC\")\n",
                "    fig.text(0.5, 1.02, period_text, ha='center', fontstyle='italic', fontsize=11)\n",
                "\n",
                "else:\n",
                "    raise ValueError(\"plot_style must be 'subplots' or 'overlay'\")\n",
                "\n",
                "plt.savefig('feature_timeseries_analysis.pdf', bbox_inches='tight', dpi=300)\n",
                "plt.savefig('feature_timeseries_analysis.png', bbox_inches='tight', dpi=300)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib as mpl\n",
                "from math import ceil\n",
                "import matplotlib.dates as mdates\n",
                "from matplotlib.ticker import MaxNLocator\n",
                "import textwrap\n",
                "\n",
                "# --------------------------------------------------\n",
                "# ❶ USER CONFIG - Enhanced for thesis presentation\n",
                "# --------------------------------------------------\n",
                "top_n = 6                                     # how many features to show\n",
                "ATTR_START = pd.Timestamp(\"2023-09-01 19:00\", tz=\"UTC\")\n",
                "ATTR_END = pd.Timestamp(\"2023-09-10 08:00\", tz=\"UTC\")\n",
                "PEAK_START = pd.Timestamp(\"2023-09-05 19:00\", tz=\"UTC\")\n",
                "PEAK_END = pd.Timestamp(\"2023-09-06 08:00\", tz=\"UTC\")\n",
                "PEAK_PRICE = pd.Timestamp(\"2023-09-06 03:00\", tz=\"UTC\")  # New peak price line\n",
                "\n",
                "plot_style = \"subplots\"\n",
                "use_zscore = False\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "font_family = 'serif'\n",
                "mpl.rcParams['font.family'] = font_family\n",
                "mpl.rcParams['font.size'] = 16\n",
                "mpl.rcParams['axes.titlesize'] = 18\n",
                "mpl.rcParams['axes.labelsize'] = 15\n",
                "mpl.rcParams['xtick.labelsize'] = 14\n",
                "mpl.rcParams['ytick.labelsize'] = 14\n",
                "mpl.rcParams['legend.fontsize'] = 16\n",
                "mpl.rcParams['figure.titlesize'] = 20\n",
                "\n",
                "# --- EXPLICIT HIGH-CONTRAST COLORS FOR YOUR INDEXES ---\n",
                "SPECIAL_INDEX_COLORS = { # index nb - 1\n",
                "    6:   \"#1f77b4\",  # blue\n",
                "    14:  \"#ff7f0e\",  # orange\n",
                "    16:  \"#e377c2\",  # pink\n",
                "    17:  \"#bcbd22\",  # olive/lime\n",
                "    29:  \"#8c564b\",  # brown\n",
                "    32:  \"#9467bd\",  # purple\n",
                "    33:  \"#7f7f7f\",  # gray\n",
                "    34:  \"#d62728\",  # red\n",
                "    35:  \"#2ca02c\",  # green\n",
                "}\n",
                "\n",
                "def get_feature_color(idx, default_palette=plt.cm.tab10.colors):\n",
                "    if idx in SPECIAL_INDEX_COLORS:\n",
                "        return SPECIAL_INDEX_COLORS[idx]\n",
                "    return default_palette[idx % len(default_palette)]\n",
                "\n",
                "# --- YOUR DATA/PREPROCESSING ---\n",
                "\n",
                "methods = [\"IG\", \"DL_SHAP\", \"FA\"]\n",
                "all_baselines = list(attr_summaries[methods[0]].keys())\n",
                "baselines = [b for b in all_baselines if b != \"mean\"]\n",
                "\n",
                "attr_pct = {}\n",
                "for m in methods:\n",
                "    attr_pct[m] = {}\n",
                "    for b in baselines:\n",
                "        v = attr_summaries[m][b].astype(float)\n",
                "        v = v / (v.sum() + 1e-12)\n",
                "        attr_pct[m][b] = v\n",
                "\n",
                "all_vectors = [attr_pct[m][b] for m in methods for b in baselines]\n",
                "mean_imp = np.mean(all_vectors, axis=0)\n",
                "top_idx = np.argsort(mean_imp)[-top_n:][::-1]\n",
                "top_names = [exogenous_vars[i] for i in top_idx]\n",
                "\n",
                "slice_df = df.loc[ATTR_START:ATTR_END, top_names]\n",
                "if use_zscore and plot_style == \"overlay\":\n",
                "    slice_df = (slice_df - slice_df.mean()) / slice_df.std(ddof=0)\n",
                "\n",
                "# --- PLOTTING ---\n",
                "def get_time_locator(start, end):\n",
                "    duration_hours = (end - start).total_seconds() / 3600\n",
                "    if duration_hours <= 48:\n",
                "        return mdates.HourLocator(interval=2), mdates.DateFormatter('%H:%M\\n%d-%b')\n",
                "    elif duration_hours <= 96:\n",
                "        return mdates.HourLocator(interval=6), mdates.DateFormatter('%d-%b\\n%H:%M')\n",
                "    elif duration_hours <= 168:\n",
                "        return mdates.HourLocator(interval=12), mdates.DateFormatter('%d-%b\\n%H:%M')\n",
                "    else:\n",
                "        return mdates.DayLocator(interval=1), mdates.DateFormatter('%d-%b')\n",
                "\n",
                "if plot_style == \"overlay\":\n",
                "    fig, ax = plt.subplots(figsize=(10, 6))\n",
                "    ax.axvspan(PEAK_START, PEAK_END, alpha=0.15, color='gray', label='Peak Period')\n",
                "    ax.axvline(PEAK_PRICE, color='red', linestyle='--', linewidth=2, label='Peak Price')\n",
                "    line_styles = ['-', '--', ':', '-.', '-', '--']\n",
                "    for i, (name, style) in enumerate(zip(top_names, line_styles)):\n",
                "        color = get_feature_color(top_idx[i])\n",
                "        ax.plot(slice_df.index, slice_df[name],\n",
                "                linewidth=2.5,\n",
                "                color=color,\n",
                "                linestyle=style,\n",
                "                label=f\"#{top_idx[i]+1} {name}\")\n",
                "    locator, formatter = get_time_locator(ATTR_START, ATTR_END)\n",
                "    ax.xaxis.set_major_locator(locator)\n",
                "    ax.xaxis.set_major_formatter(formatter)\n",
                "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
                "    ax.set_ylabel(\"Z-Score\" if use_zscore else \"Feature Value\", fontweight='normal')\n",
                "    ax.set_xlabel(\"Date\", fontweight='normal')\n",
                "    ax.grid(alpha=0.3, linestyle='--')\n",
                "    for spine in ax.spines.values():\n",
                "        spine.set_visible(True)\n",
                "        spine.set_linewidth(0.75)\n",
                "    handles, labels = ax.get_legend_handles_labels()\n",
                "    if 'Peak Period' not in labels:\n",
                "        from matplotlib.patches import Patch\n",
                "        handles.append(Patch(facecolor='gray', alpha=0.15))\n",
                "        labels.append('Peak Period (23:00-05:00)')\n",
                "    ax.legend(handles, labels, bbox_to_anchor=(1.01, 1), loc=\"bottom left\", frameon=True, fancybox=True, shadow=True)\n",
                "    fig.tight_layout()\n",
                "\n",
                "elif plot_style == \"subplots\":\n",
                "    nrows, ncols = 3, 2\n",
                "    fig, axes = plt.subplots(nrows, ncols, figsize=(13, 9), sharex=True, constrained_layout=True)\n",
                "    axes = axes.flatten()\n",
                "    wrap_width = 41\n",
                "\n",
                "    locator, formatter = get_time_locator(ATTR_START, ATTR_END)\n",
                "\n",
                "    for i, (ax, idx, name) in enumerate(zip(axes, top_idx, top_names)):\n",
                "        color = get_feature_color(idx)\n",
                "        ax.axvspan(PEAK_START, PEAK_END, alpha=0.3, color='gray')\n",
                "        ax.axvline(PEAK_PRICE, color='red', linestyle='--', linewidth=2)\n",
                "        ax.plot(slice_df.index, slice_df[name], linewidth=2.5, color=color)\n",
                "        feature_title = f\"#{idx+1}: {name} ({mean_imp[idx]*100:.2f}%)\"\n",
                "        feature_title_wrapped = \"\\n\".join(textwrap.wrap(feature_title, wrap_width))\n",
                "        ax.set_title(feature_title_wrapped, loc='left', fontweight='normal', fontsize=16)\n",
                "        ax.set_ylabel(\"Value\", fontweight='normal')\n",
                "        ax.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
                "        ax.grid(alpha=0.3, linestyle='--')\n",
                "        for spine in ax.spines.values():\n",
                "            spine.set_visible(True)\n",
                "            spine.set_linewidth(0.75)\n",
                "        val_min = slice_df[name].min()\n",
                "        val_max = slice_df[name].max()\n",
                "        if val_min == val_max:\n",
                "            delta = val_min * 0.01 if val_min != 0 else 0.01\n",
                "            ax.axhspan(val_min - delta, val_max + delta, alpha=0.1, color=color)\n",
                "        else:\n",
                "            ax.axhspan(val_min, val_max, alpha=0.1, color=color)\n",
                "        if i == 0:\n",
                "            ax.text(0.98, 0.95, 'Peak Period', transform=ax.transAxes,\n",
                "                    bbox=dict(facecolor='gray', alpha=0.3, edgecolor='none', pad=3),\n",
                "                    ha='right', va='top', fontsize=14)\n",
                "            ax.text(0.98, 0.80, 'Peak Price', transform=ax.transAxes,\n",
                "                    bbox=dict(facecolor='red', alpha=0.3, edgecolor='none', pad=3),\n",
                "                    ha='right', va='top', fontsize=14)\n",
                "    for j in range(top_n, nrows * ncols):\n",
                "        fig.delaxes(axes[j])\n",
                "    for ax in axes[-ncols:]:\n",
                "        ax.xaxis.set_major_locator(locator)\n",
                "        ax.xaxis.set_major_formatter(formatter)\n",
                "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
                "        ax.set_xlabel(\"Date\", fontweight='normal')\n",
                "\n",
                "\n",
                "else:\n",
                "    raise ValueError(\"plot_style must be 'subplots' or 'overlay'\")\n",
                "\n",
                "plt.savefig('feature_timeseries_analysis.pdf', bbox_inches='tight', dpi=300)\n",
                "plt.savefig('feature_timeseries_analysis.png', bbox_inches='tight', dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CASE 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import joblib\n",
                "import plotly.graph_objects as go\n",
                "\n",
                "# ---- PATHS ----\n",
                "MODEL_PATH = \"/new_model_test/best_model_SE2_Down_Volume.pth\"\n",
                "SCALER_X_PATH = \"/new_model_test/scaler_X_SE2_Down_Volume.joblib\"\n",
                "SCALER_Y_PATH = \"/new_model_test/scaler_y_SE2_Down_Volume.joblib\"\n",
                "FILE_PATH = \"/results/results_merged.csv\"\n",
                "\n",
                "REGION = \"SE2\"\n",
                "target_vars = [f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownActivatedVolume\"]\n",
                "\n",
                "# -- Model class --\n",
                "class MaskedLSTM(torch.nn.Module):\n",
                "    def __init__(self, input_size, hidden_size=32, num_layers=1, dense_size=32, output_size=1, horizon=1, dropout=0.08, dropout_lstm=0.27, bidirectional=True):\n",
                "        super().__init__()\n",
                "        self.horizon = horizon\n",
                "        self.output_size = output_size\n",
                "        actual_in = input_size * 2\n",
                "        self.lstm = torch.nn.LSTM(\n",
                "            input_size=actual_in,\n",
                "            hidden_size=hidden_size,\n",
                "            num_layers=num_layers,\n",
                "            dropout=dropout_lstm if num_layers > 1 else 0.0,\n",
                "            bidirectional=bidirectional,\n",
                "            batch_first=True,\n",
                "        )\n",
                "        hidden_out = hidden_size * (2 if bidirectional else 1)\n",
                "        self.fc1 = torch.nn.Linear(hidden_out, dense_size)\n",
                "        self.fc2 = torch.nn.Linear(dense_size, horizon * output_size)\n",
                "        self.relu = torch.nn.ReLU()\n",
                "        self.drop = torch.nn.Dropout(dropout)\n",
                "        self.drop_lstm = torch.nn.Dropout(dropout_lstm)\n",
                "    def forward(self, x, mask):\n",
                "        x = torch.cat((x, mask), dim=2)\n",
                "        out, _ = self.lstm(x)\n",
                "        out = self.drop_lstm(out)\n",
                "        out = self.relu(self.fc1(out[:, -1, :]))\n",
                "        out = self.drop(out)\n",
                "        out = self.fc2(out)\n",
                "        return out.view(out.size(0), self.horizon, self.output_size)\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "scaler_X = joblib.load(SCALER_X_PATH)\n",
                "scaler_y = joblib.load(SCALER_Y_PATH)\n",
                "\n",
                "best_params = {\n",
                "    \"hidden_size\": 32,\n",
                "    \"num_layers\": 1,\n",
                "    \"bidirectional\": True,\n",
                "    \"dense_size\": 32,\n",
                "    \"dropout\": 0.08137184695714653,\n",
                "    \"dropout_lstm\": 0.2675438586833995,\n",
                "    \"learning_rate\": 0.0013843186375837144,\n",
                "    \"batch_size\": 128,\n",
                "    \"seq_length\": 216,\n",
                "}\n",
                "seq_length = best_params[\"seq_length\"]\n",
                "\n",
                "# --- Load and preprocess data ---\n",
                "df_all = pd.read_csv(FILE_PATH)\n",
                "price_column_name_to_plot = f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownPrice\"\n",
                "\n",
                "# Store price data before dropping columns\n",
                "price_data_full = None\n",
                "if price_column_name_to_plot in df_all.columns:\n",
                "    price_data_full = df_all[['DateTime', price_column_name_to_plot]].copy()\n",
                "    price_data_full['DateTime'] = pd.to_datetime(price_data_full['DateTime'], utc=True)\n",
                "    price_data_full.set_index('DateTime', inplace=True)\n",
                "    print(f\"Stored '{price_column_name_to_plot}' separately for plotting.\")\n",
                "else:\n",
                "    print(f\"Warning: Price column '{price_column_name_to_plot}' not found in df_all. Will not be plotted.\")\n",
                "\n",
                "from filter_features import pick_region_filter\n",
                "BDZ_filter = pick_region_filter(region=REGION, remove_balancing=True)\n",
                "drop_list = [\n",
                "    f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedUpPrice\",\n",
                "    f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownPrice\",\n",
                "    f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedUpActivatedVolume\",\n",
                "    f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownActivatedVolume\",\n",
                "]\n",
                "BDZ_filter = [c for c in BDZ_filter if c not in drop_list or c in target_vars]\n",
                "df_all[\"DateTime\"] = pd.to_datetime(df_all[\"DateTime\"], utc=True)\n",
                "df_all.set_index(\"DateTime\", inplace=True)\n",
                "df_all = df_all.asfreq(\"h\")\n",
                "df = df_all[BDZ_filter].copy()\n",
                "df = df.asfreq(\"h\")\n",
                "\n",
                "\n",
                "# \n",
                "\n",
                "\n",
                "CREATE_INDICATOR_FEATURES = False\n",
                "CREATE_CYCLICAL_FEATURES = False\n",
                "CREATE_LAGGED_FEATURES = False\n",
                "\n",
                "new_indicator_features = []\n",
                "if CREATE_INDICATOR_FEATURES:\n",
                "    indicator_features_to_create = [col for col in BDZ_filter + target_vars if col in df.columns]\n",
                "    for col in indicator_features_to_create:\n",
                "        df[f'{col}_was_missing'] = df[col].isnull().astype(int)\n",
                "    new_indicator_features = [f'{col}_was_missing' for col in indicator_features_to_create]\n",
                "\n",
                "new_time_features = []\n",
                "if CREATE_CYCLICAL_FEATURES:\n",
                "    df['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24.0)\n",
                "    df['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24.0)\n",
                "    df['dayofweek_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7.0)\n",
                "    df['dayofweek_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7.0)\n",
                "    df['weekofyear_sin'] = np.sin(2 * np.pi * df.index.isocalendar().week / 52.0)\n",
                "    df['weekofyear_cos'] = np.cos(2 * np.pi * df.index.isocalendar().week / 52.0)\n",
                "    new_time_features = ['hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'weekofyear_sin', 'weekofyear_cos']\n",
                "\n",
                "new_lagged_features = []\n",
                "if CREATE_LAGGED_FEATURES:\n",
                "    for target_var in target_vars:\n",
                "        df[f'{target_var}_lag1'] = df[target_var].shift(1)\n",
                "        df[f'{target_var}_lag24'] = df[target_var].shift(24)\n",
                "        df[f'{target_var}_lag168'] = df[target_var].shift(168)\n",
                "    for target_var in target_vars:\n",
                "        new_lagged_features.extend([f'{target_var}_lag1', f'{target_var}_lag24', f'{target_var}_lag168'])\n",
                "\n",
                "df.dropna(axis=1, how=\"all\", inplace=True)\n",
                "\n",
                "exogenous_vars = [c for c in BDZ_filter if c not in target_vars]\n",
                "if CREATE_CYCLICAL_FEATURES:\n",
                "    exogenous_vars.extend(new_time_features)\n",
                "if CREATE_LAGGED_FEATURES:\n",
                "    exogenous_vars.extend(new_lagged_features)\n",
                "if CREATE_INDICATOR_FEATURES:\n",
                "    exogenous_vars.extend(new_indicator_features)\n",
                "exogenous_vars = sorted(list(set(var for var in exogenous_vars if var in df.columns)))\n",
                "\n",
                "# --- Instantiate the model ---\n",
                "model = MaskedLSTM(\n",
                "    input_size=len(exogenous_vars),\n",
                "    hidden_size=best_params[\"hidden_size\"],\n",
                "    num_layers=best_params[\"num_layers\"],\n",
                "    dense_size=best_params[\"dense_size\"],\n",
                "    output_size=len(target_vars),\n",
                "    dropout=best_params[\"dropout\"],\n",
                "    dropout_lstm=best_params[\"dropout_lstm\"],\n",
                "    bidirectional=best_params[\"bidirectional\"],\n",
                ").to(DEVICE)\n",
                "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
                "model.eval()\n",
                "\n",
                "# --- Robust window selection: will always work for any seq_length and date range ---\n",
                "\n",
                "desired_start = pd.Timestamp(\"2024-04-07 06:00:00\", tz=\"UTC\")\n",
                "desired_end = pd.Timestamp(\"2024-04-07 22:00:00\", tz=\"UTC\")\n",
                "\n",
                "all_dates = df.index\n",
                "try:\n",
                "    first_pred_idx = all_dates.get_loc(desired_start)\n",
                "except KeyError:\n",
                "    raise ValueError(f\"Desired start {desired_start} not in DataFrame index!\")\n",
                "\n",
                "if first_pred_idx < seq_length:\n",
                "    raise ValueError(f\"Not enough data before {desired_start} to build input sequence with seq_length={seq_length}.\")\n",
                "\n",
                "window_start = all_dates[first_pred_idx - seq_length]\n",
                "window_end = desired_end\n",
                "\n",
                "df_pred_window = df.loc[window_start:window_end].copy()\n",
                "\n",
                "# -- Scale features --\n",
                "X_scaled = scaler_X.transform(df_pred_window[exogenous_vars])\n",
                "y_true = df_pred_window[target_vars].values\n",
                "\n",
                "# -- Build sequences --\n",
                "def create_sequences(X, lookback, horizon=1):\n",
                "    num_samples = X.shape[0] - lookback - horizon + 1\n",
                "    X_seq = np.zeros((num_samples, lookback, X.shape[1]), dtype=np.float32)\n",
                "    mask_seq = np.ones((num_samples, lookback, X.shape[1]), dtype=np.float32)\n",
                "    for i in range(num_samples):\n",
                "        X_seq[i] = X[i : i + lookback]\n",
                "    return X_seq, mask_seq\n",
                "\n",
                "X_seq, mask_seq = create_sequences(X_scaled, seq_length, 1)\n",
                "nan_pos = np.isnan(X_seq)\n",
                "mask_seq[nan_pos] = 0.0\n",
                "X_seq[nan_pos] = 0.0\n",
                "\n",
                "dates_seq = df_pred_window.index[seq_length : seq_length + len(X_seq)]\n",
                "y_true_aligned = y_true[seq_length : seq_length + len(X_seq)]\n",
                "\n",
                "# -- Filter for predictions within the requested window only --\n",
                "mask = (dates_seq >= desired_start) & (dates_seq <= desired_end)\n",
                "dates_seq = dates_seq[mask]\n",
                "y_true_aligned = y_true_aligned[mask]\n",
                "\n",
                "with torch.no_grad():\n",
                "    X_tensor = torch.tensor(X_seq, dtype=torch.float32).to(DEVICE)\n",
                "    mask_tensor = torch.tensor(mask_seq, dtype=torch.float32).to(DEVICE)\n",
                "    y_pred_scaled = model(X_tensor, mask_tensor).cpu().numpy()[:, 0, :]  # (n, target_dim)\n",
                "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
                "y_pred = y_pred[mask]\n",
                "\n",
                "# --- Add price data ---\n",
                "price_data_to_plot = None\n",
                "if price_data_full is not None and price_column_name_to_plot in price_data_full.columns:\n",
                "    try:\n",
                "        # Align price data with dates_seq\n",
                "        price_data_to_plot = price_data_full.loc[dates_seq, price_column_name_to_plot].values.astype(float)\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not align price data with dates_seq: {e}\")\n",
                "        price_data_to_plot = None\n",
                "\n",
                "# -- Plot\n",
                "fig = go.Figure()\n",
                "fig.add_trace(go.Scatter(\n",
                "    x=dates_seq,\n",
                "    y=y_true_aligned[:, 0],\n",
                "    mode='lines+markers',\n",
                "    name='True',\n",
                "    marker=dict(symbol='circle', size=6)\n",
                "))\n",
                "fig.add_trace(go.Scatter(\n",
                "    x=dates_seq,\n",
                "    y=y_pred[:, 0],\n",
                "    mode='lines+markers',\n",
                "    name='Predicted',\n",
                "    marker=dict(symbol='x', size=6)\n",
                "))\n",
                "if price_data_to_plot is not None:\n",
                "    fig.add_trace(go.Scatter(\n",
                "        x=dates_seq,\n",
                "        y=price_data_to_plot,\n",
                "        mode='lines',\n",
                "        name='mFRR Down Price',\n",
                "        line=dict(color='orange', width=2),\n",
                "        yaxis='y2'\n",
                "    ))\n",
                "fig.update_layout(\n",
                "    title=f\"True vs Predicted for {target_vars[0]} ({desired_start.date()} to {desired_end.date()})\",\n",
                "    xaxis_title=\"Date\",\n",
                "    yaxis=dict(\n",
                "        title=\"Volume (MW)\"\n",
                "    ),\n",
                "    yaxis2=dict(\n",
                "        title=\"mFRR Down Price (€/MWh)\",\n",
                "        overlaying='y',\n",
                "        side='right',\n",
                "        showgrid=False\n",
                "    ),\n",
                "    template=\"plotly_white\",\n",
                "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
                ")\n",
                "fig.update_xaxes(tickformat=\"%d %b %Y %H:%M\")\n",
                "fig.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Case 2 plot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [],
            "source": [
                "desired_start = pd.Timestamp(\"2024-04-07 06:00:00\", tz=\"UTC\")\n",
                "desired_end = pd.Timestamp(\"2024-04-07 22:00:00\", tz=\"UTC\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import joblib\n",
                "import plotly.graph_objects as go\n",
                "\n",
                "# ---- PATHS ----\n",
                "MODEL_PATH = \"/new_model_test/best_model_SE2_Down_Volume.pth\"\n",
                "SCALER_X_PATH = \"/new_model_test/scaler_X_SE2_Down_Volume.joblib\"\n",
                "SCALER_Y_PATH = \"/new_model_test/scaler_y_SE2_Down_Volume.joblib\"\n",
                "FILE_PATH = \"/results/results_merged.csv\"\n",
                "\n",
                "REGION = \"SE2\"\n",
                "target_vars = [f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownActivatedVolume\"]\n",
                "\n",
                "# -- Model class --\n",
                "class MaskedLSTM(torch.nn.Module):\n",
                "    def __init__(self, input_size, hidden_size=32, num_layers=1, dense_size=32, output_size=1, horizon=1, dropout=0.08, dropout_lstm=0.27, bidirectional=True):\n",
                "        super().__init__()\n",
                "        self.horizon = horizon\n",
                "        self.output_size = output_size\n",
                "        actual_in = input_size * 2\n",
                "        self.lstm = torch.nn.LSTM(\n",
                "            input_size=actual_in,\n",
                "            hidden_size=hidden_size,\n",
                "            num_layers=num_layers,\n",
                "            dropout=dropout_lstm if num_layers > 1 else 0.0,\n",
                "            bidirectional=bidirectional,\n",
                "            batch_first=True,\n",
                "        )\n",
                "        hidden_out = hidden_size * (2 if bidirectional else 1)\n",
                "        self.fc1 = torch.nn.Linear(hidden_out, dense_size)\n",
                "        self.fc2 = torch.nn.Linear(dense_size, horizon * output_size)\n",
                "        self.relu = torch.nn.ReLU()\n",
                "        self.drop = torch.nn.Dropout(dropout)\n",
                "        self.drop_lstm = torch.nn.Dropout(dropout_lstm)\n",
                "    def forward(self, x, mask):\n",
                "        x = torch.cat((x, mask), dim=2)\n",
                "        out, _ = self.lstm(x)\n",
                "        out = self.drop_lstm(out)\n",
                "        out = self.relu(self.fc1(out[:, -1, :]))\n",
                "        out = self.drop(out)\n",
                "        out = self.fc2(out)\n",
                "        return out.view(out.size(0), self.horizon, self.output_size)\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "scaler_X = joblib.load(SCALER_X_PATH)\n",
                "scaler_y = joblib.load(SCALER_Y_PATH)\n",
                "\n",
                "best_params = {\n",
                "    \"hidden_size\": 32,\n",
                "    \"num_layers\": 1,\n",
                "    \"bidirectional\": True,\n",
                "    \"dense_size\": 32,\n",
                "    \"dropout\": 0.08137184695714653,\n",
                "    \"dropout_lstm\": 0.2675438586833995,\n",
                "    \"learning_rate\": 0.0013843186375837144,\n",
                "    \"batch_size\": 128,\n",
                "    \"seq_length\": 216,\n",
                "}\n",
                "seq_length = best_params[\"seq_length\"]\n",
                "\n",
                "# --- Load and preprocess data ---\n",
                "df_all = pd.read_csv(FILE_PATH)\n",
                "price_column_name_to_plot = f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownPrice\"\n",
                "\n",
                "# Store price data before dropping columns\n",
                "price_data_full = None\n",
                "if price_column_name_to_plot in df_all.columns:\n",
                "    price_data_full = df_all[['DateTime', price_column_name_to_plot]].copy()\n",
                "    price_data_full['DateTime'] = pd.to_datetime(price_data_full['DateTime'], utc=True)\n",
                "    price_data_full.set_index('DateTime', inplace=True)\n",
                "    print(f\"Stored '{price_column_name_to_plot}' separately for plotting.\")\n",
                "else:\n",
                "    print(f\"Warning: Price column '{price_column_name_to_plot}' not found in df_all. Will not be plotted.\")\n",
                "\n",
                "from filter_features import pick_region_filter\n",
                "BDZ_filter = pick_region_filter(region=REGION, remove_balancing=True)\n",
                "drop_list = [\n",
                "    f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedUpPrice\",\n",
                "    f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownPrice\",\n",
                "    f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedUpActivatedVolume\",\n",
                "    f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownActivatedVolume\",\n",
                "]\n",
                "BDZ_filter = [c for c in BDZ_filter if c not in drop_list or c in target_vars]\n",
                "df_all[\"DateTime\"] = pd.to_datetime(df_all[\"DateTime\"], utc=True)\n",
                "df_all.set_index(\"DateTime\", inplace=True)\n",
                "df_all = df_all.asfreq(\"h\")\n",
                "df = df_all[BDZ_filter].copy()\n",
                "df = df.asfreq(\"h\")\n",
                "\n",
                "\n",
                "# \n",
                "\n",
                "\n",
                "CREATE_INDICATOR_FEATURES = False\n",
                "CREATE_CYCLICAL_FEATURES = False\n",
                "CREATE_LAGGED_FEATURES = False\n",
                "\n",
                "new_indicator_features = []\n",
                "if CREATE_INDICATOR_FEATURES:\n",
                "    indicator_features_to_create = [col for col in BDZ_filter + target_vars if col in df.columns]\n",
                "    for col in indicator_features_to_create:\n",
                "        df[f'{col}_was_missing'] = df[col].isnull().astype(int)\n",
                "    new_indicator_features = [f'{col}_was_missing' for col in indicator_features_to_create]\n",
                "\n",
                "new_time_features = []\n",
                "if CREATE_CYCLICAL_FEATURES:\n",
                "    df['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24.0)\n",
                "    df['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24.0)\n",
                "    df['dayofweek_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7.0)\n",
                "    df['dayofweek_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7.0)\n",
                "    df['weekofyear_sin'] = np.sin(2 * np.pi * df.index.isocalendar().week / 52.0)\n",
                "    df['weekofyear_cos'] = np.cos(2 * np.pi * df.index.isocalendar().week / 52.0)\n",
                "    new_time_features = ['hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'weekofyear_sin', 'weekofyear_cos']\n",
                "\n",
                "new_lagged_features = []\n",
                "if CREATE_LAGGED_FEATURES:\n",
                "    for target_var in target_vars:\n",
                "        df[f'{target_var}_lag1'] = df[target_var].shift(1)\n",
                "        df[f'{target_var}_lag24'] = df[target_var].shift(24)\n",
                "        df[f'{target_var}_lag168'] = df[target_var].shift(168)\n",
                "    for target_var in target_vars:\n",
                "        new_lagged_features.extend([f'{target_var}_lag1', f'{target_var}_lag24', f'{target_var}_lag168'])\n",
                "\n",
                "df.dropna(axis=1, how=\"all\", inplace=True)\n",
                "\n",
                "exogenous_vars = [c for c in BDZ_filter if c not in target_vars]\n",
                "if CREATE_CYCLICAL_FEATURES:\n",
                "    exogenous_vars.extend(new_time_features)\n",
                "if CREATE_LAGGED_FEATURES:\n",
                "    exogenous_vars.extend(new_lagged_features)\n",
                "if CREATE_INDICATOR_FEATURES:\n",
                "    exogenous_vars.extend(new_indicator_features)\n",
                "exogenous_vars = sorted(list(set(var for var in exogenous_vars if var in df.columns)))\n",
                "\n",
                "# --- Instantiate the model ---\n",
                "model = MaskedLSTM(\n",
                "    input_size=len(exogenous_vars),\n",
                "    hidden_size=best_params[\"hidden_size\"],\n",
                "    num_layers=best_params[\"num_layers\"],\n",
                "    dense_size=best_params[\"dense_size\"],\n",
                "    output_size=len(target_vars),\n",
                "    dropout=best_params[\"dropout\"],\n",
                "    dropout_lstm=best_params[\"dropout_lstm\"],\n",
                "    bidirectional=best_params[\"bidirectional\"],\n",
                ").to(DEVICE)\n",
                "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
                "model.eval()\n",
                "\n",
                "# --- Robust window selection: will always work for any seq_length and date range ---\n",
                "\n",
                "# desired_start = pd.Timestamp(\"2024-04-07 06:00:00\", tz=\"UTC\")\n",
                "# desired_end = pd.Timestamp(\"2024-04-07 22:00:00\", tz=\"UTC\")\n",
                "\n",
                "all_dates = df.index\n",
                "try:\n",
                "    first_pred_idx = all_dates.get_loc(desired_start)\n",
                "except KeyError:\n",
                "    raise ValueError(f\"Desired start {desired_start} not in DataFrame index!\")\n",
                "\n",
                "if first_pred_idx < seq_length:\n",
                "    raise ValueError(f\"Not enough data before {desired_start} to build input sequence with seq_length={seq_length}.\")\n",
                "\n",
                "window_start = all_dates[first_pred_idx - seq_length]\n",
                "window_end = desired_end\n",
                "\n",
                "df_pred_window = df.loc[window_start:window_end].copy()\n",
                "\n",
                "# -- Scale features --\n",
                "X_scaled = scaler_X.transform(df_pred_window[exogenous_vars])\n",
                "y_true = df_pred_window[target_vars].values\n",
                "\n",
                "# -- Build sequences --\n",
                "def create_sequences(X, lookback, horizon=1):\n",
                "    num_samples = X.shape[0] - lookback - horizon + 1\n",
                "    X_seq = np.zeros((num_samples, lookback, X.shape[1]), dtype=np.float32)\n",
                "    mask_seq = np.ones((num_samples, lookback, X.shape[1]), dtype=np.float32)\n",
                "    for i in range(num_samples):\n",
                "        X_seq[i] = X[i : i + lookback]\n",
                "    return X_seq, mask_seq\n",
                "\n",
                "X_seq, mask_seq = create_sequences(X_scaled, seq_length, 1)\n",
                "nan_pos = np.isnan(X_seq)\n",
                "mask_seq[nan_pos] = 0.0\n",
                "X_seq[nan_pos] = 0.0\n",
                "\n",
                "dates_seq = df_pred_window.index[seq_length : seq_length + len(X_seq)]\n",
                "y_true_aligned = y_true[seq_length : seq_length + len(X_seq)]\n",
                "\n",
                "# -- Filter for predictions within the requested window only --\n",
                "mask = (dates_seq >= desired_start) & (dates_seq <= desired_end)\n",
                "dates_seq = dates_seq[mask]\n",
                "y_true_aligned = y_true_aligned[mask]\n",
                "\n",
                "with torch.no_grad():\n",
                "    X_tensor = torch.tensor(X_seq, dtype=torch.float32).to(DEVICE)\n",
                "    mask_tensor = torch.tensor(mask_seq, dtype=torch.float32).to(DEVICE)\n",
                "    y_pred_scaled = model(X_tensor, mask_tensor).cpu().numpy()[:, 0, :]  # (n, target_dim)\n",
                "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
                "y_pred = y_pred[mask]\n",
                "\n",
                "# --- Add price data ---\n",
                "price_data_to_plot = None\n",
                "if price_data_full is not None and price_column_name_to_plot in price_data_full.columns:\n",
                "    try:\n",
                "        # Align price data with dates_seq\n",
                "        price_data_to_plot = price_data_full.loc[dates_seq, price_column_name_to_plot].values.astype(float)\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not align price data with dates_seq: {e}\")\n",
                "        price_data_to_plot = None\n",
                "\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.dates as mdates\n",
                "\n",
                "# --- Colors and markers ---\n",
                "true_color = \"#636efa\"    # Plotly default blue\n",
                "pred_color = \"#ef553b\"    # Plotly default red\n",
                "price_color = \"orange\"\n",
                "\n",
                "# --- Matplotlib plot ---\n",
                "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
                "\n",
                "# True values\n",
                "ax1.plot(dates_seq, y_true_aligned[:, 0], label='True', marker='o', markersize=5, linewidth=2)\n",
                "\n",
                "# Predicted values\n",
                "ax1.plot(dates_seq, y_pred[:, 0], label='Predicted', color=pred_color, marker='x', markersize=5, linewidth=2)\n",
                "\n",
                "ax1.set_xlabel(\"Date\", fontsize=17)\n",
                "ax1.set_ylabel(\"Volume (MW)\", fontsize=17)\n",
                "ax1.tick_params(axis='y', labelsize=16)\n",
                "ax1.tick_params(axis='x', labelsize=16)\n",
                "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%d %b %Y\\n%H:%M'))\n",
                "plt.xticks(rotation=45)\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Add price on secondary y-axis\n",
                "if price_data_to_plot is not None:\n",
                "    ax2 = ax1.twinx()\n",
                "    ax2.plot(dates_seq, price_data_to_plot, label='mFRR Down Price', color=price_color, linewidth=2)\n",
                "    ax2.set_ylabel(\"mFRR Down Price (€/MWh)\", fontsize=17, color=price_color)\n",
                "    ax2.tick_params(axis='y', labelcolor=price_color, labelsize=16)\n",
                "    ax2.grid(False)\n",
                "\n",
                "else:\n",
                "    ax2 = None\n",
                "\n",
                "# Combined legend\n",
                "lines_labels = [ax.get_legend_handles_labels() for ax in [ax1, ax2] if ax is not None]\n",
                "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
                "plt.legend(lines, labels, loc='upper right', fontsize=16, frameon=True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature importance"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## IG"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [],
            "source": [
                "ATTR_START = \"2024-04-07 09:00\"\n",
                "ATTR_END   = \"2024-04-07 17:00\"\n",
                "\n",
                "from datetime import datetime\n",
                "ATTR_START_dt = datetime.strptime(ATTR_START, \"%Y-%m-%d %H:%M\")\n",
                "ATTR_END_dt   = datetime.strptime(ATTR_END, \"%Y-%m-%d %H:%M\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "attr_summaries = {}       # master container\n",
                "# \n",
                "\n",
                "\n",
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "from captum.attr import IntegratedGradients\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd                 # already imported above\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 0.  CONFIG & OUTPUT DIR\n",
                "# ------------------------------------------------------------\n",
                "plots_dir = \"attribution_plots\"\n",
                "os.makedirs(plots_dir, exist_ok=True)\n",
                "\n",
                "# ATTR_START = \"2024-04-07 09:00\"\n",
                "# ATTR_END   = \"2024-04-07 17:00\"\n",
                "attribution_times = pd.date_range(\n",
                "    ATTR_START, ATTR_END, freq=\"h\", tz=\"UTC\"\n",
                ")\n",
                "print(\"Attribution times:\", attribution_times)\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 1.  BUILD INPUT / MASK TENSORS *** with the SAME preprocessing as inference ***\n",
                "# ------------------------------------------------------------\n",
                "input_seqs, mask_seqs = [], []\n",
                "\n",
                "for t in attribution_times:\n",
                "    if t not in df.index:\n",
                "        print(f\"⚠️  {t} missing in df.index – skipped\");   continue\n",
                "    idx = df.index.get_loc(t)\n",
                "    if idx < seq_length:\n",
                "        print(f\"⚠️  Not enough history before {t} – skipped\");   continue\n",
                "\n",
                "    # raw window (seq_len × features) --------------------------\n",
                "    window_raw = df.iloc[idx - seq_length: idx][exogenous_vars].values\n",
                "\n",
                "    # ---> scale exactly like in inference\n",
                "    window_scaled = scaler_X.transform(window_raw)\n",
                "\n",
                "    # ---> build mask (1 = present, 0 = missing)\n",
                "    mask = (~np.isnan(window_scaled)).astype(np.float32)\n",
                "\n",
                "    # ---> impute NaNs with 0  (model expects this)\n",
                "    window_scaled[np.isnan(window_scaled)] = 0.0\n",
                "\n",
                "    input_seqs.append(window_scaled.astype(np.float32))\n",
                "    mask_seqs.append(mask)\n",
                "\n",
                "if not input_seqs:\n",
                "    raise RuntimeError(\"No valid windows after filtering.\")\n",
                "\n",
                "X_attr      = torch.tensor(np.stack(input_seqs)).to(DEVICE)       # (N, L, F)\n",
                "X_mask_attr = torch.tensor(np.stack(mask_seqs)).to(DEVICE)        # (N, L, F)\n",
                "\n",
                "# quick sanity-check\n",
                "print(\"Any NaNs after preprocessing?  X:\", torch.isnan(X_attr).any().item(),\n",
                "      \" mask:\", torch.isnan(X_mask_attr).any().item())\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 2.  BASELINES  (in scaled space, no NaNs)\n",
                "# ------------------------------------------------------------\n",
                "zeros_X  = torch.zeros_like(X_attr[:1])\n",
                "median_X = torch.median(X_attr, dim=0, keepdim=True).values\n",
                "\n",
                "# --- Compute mean baseline (classic mean) --------------------\n",
                "mean_X = torch.mean(X_attr, dim=0, keepdim=True)\n",
                "mean_M = X_mask_attr[:1]                    # reuse mask for consistency\n",
                "\n",
                "# Sanity check\n",
                "assert not torch.isnan(mean_X).any(), \"mean baseline still has NaNs!\"\n",
                "assert mean_X.shape  == (1, seq_length, len(exogenous_vars))\n",
                "assert mean_M.shape  == (1, seq_length, len(exogenous_vars))\n",
                "\n",
                "baselines = {\n",
                "    \"zeros\": (zeros_X,  X_mask_attr[:1]),\n",
                "    \"median\": (median_X, X_mask_attr[:1]),\n",
                "    \"mean\":   (mean_X,   mean_M),\n",
                "}\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 3.  INTEGRATED GRADIENTS  – compatible with Captum < 0.7\n",
                "# ------------------------------------------------------------\n",
                "def model_forward(x, m):\n",
                "    return model(x, m)            # (batch, 1, 1)\n",
                "\n",
                "ig = IntegratedGradients(model_forward)\n",
                "\n",
                "attr_results = {}\n",
                "for name, (base_x, base_m) in baselines.items():\n",
                "    # >>>> the 2-tuple style works in every Captum version\n",
                "    (attr_x, attr_mask), delta = ig.attribute(\n",
                "        inputs=(X_attr, X_mask_attr),\n",
                "        baselines=(base_x.expand_as(X_attr),\n",
                "                   base_m.expand_as(X_mask_attr)),\n",
                "        target=0,\n",
                "        n_steps=64,\n",
                "        method=\"riemann_trapezoid\",\n",
                "        internal_batch_size=32,\n",
                "        return_convergence_delta=True\n",
                "    )\n",
                "\n",
                "    print(f\"{name:9s} | attr range [{attr_x.min():.2e}, {attr_x.max():.2e}] \"\n",
                "          f\"δ-mean={delta.abs().mean():.2e}\")\n",
                "\n",
                "    attr_results[name] = attr_x.cpu().numpy()   # (N, L, F) – keep only X\n",
                "\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 4.  AGGREGATE IMPORTANCE  (unchanged)\n",
                "#     • |attr|  →  mean over (sample, time)  →  normalise to %\n",
                "# ------------------------------------------------------------\n",
                "attr_summary = {}\n",
                "for name, a in attr_results.items():\n",
                "    importance = np.mean(np.abs(a), axis=(0, 1))  # (F,)\n",
                "    importance /= importance.sum() + 1e-12        # convert to %\n",
                "    attr_summary[name] = importance\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 5.  PLOT\n",
                "# ------------------------------------------------------------\n",
                "top_k   = 10\n",
                "colors  = [\"#3498db\", \"#2ecc71\", \"#e74c3c\"]   # enough for 3 baselines\n",
                "fig, axes = plt.subplots(2, 3, figsize=(20, 8))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, (name, imp) in enumerate(attr_summary.items()):\n",
                "    idx_sorted = np.argsort(imp)[-top_k:][::-1]\n",
                "    feature_indices = idx_sorted      # indices into exogenous_vars\n",
                "    ids = [f\"#{idx+1}\" for idx in feature_indices]   # consistent ID = index+1\n",
                "\n",
                "    # --- bars ---\n",
                "    ax = axes[i]\n",
                "    ax.bar(range(top_k), imp[feature_indices] * 100, color=colors[i])       # %\n",
                "    ax.set_xticks(range(top_k))\n",
                "    ax.set_xticklabels(ids)\n",
                "    ax.set_xlabel(\"Feature ID\")\n",
                "    ax.set_ylabel(\"Contribution (%)\")\n",
                "    ax.set_title(f\"Integrated Gradients – {name}\")\n",
                "\n",
                "    # --- table ---\n",
                "    ax_t = axes[i + 3]\n",
                "    table_data = [[ids[j], exogenous_vars[feature_indices[j]]]\n",
                "                  for j in range(top_k)]\n",
                "    ax_t.axis(\"off\")\n",
                "    ax_t.set_title(f\"{name}  mapping\", pad=10)\n",
                "    tbl = ax_t.table(\n",
                "        cellText=table_data, colLabels=[\"ID\", \"Feature\"],\n",
                "        cellLoc=\"left\", loc=\"center\",\n",
                "        colWidths=[0.07, 0.93],\n",
                "    )\n",
                "    tbl.auto_set_font_size(False)\n",
                "    tbl.set_fontsize(9)\n",
                "    tbl.scale(1.2, 1.2)\n",
                "\n",
                "\n",
                "plt.suptitle(\"Feature Attribution (Integrated Gradients)\\n\"\n",
                "             f\"{ATTR_START} – {ATTR_END}\", fontsize=16)\n",
                "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
                "\n",
                "out_path = os.path.join(plots_dir, \"integrated_gradients_selected_window.png\")\n",
                "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
                "plt.show()\n",
                "print(\"Saved ➜\", out_path)\n",
                "\n",
                "attr_summaries[\"IG\"] = {k: v.copy() for k, v in attr_summary.items()}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## DL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, numpy as np, torch, matplotlib.pyplot as plt, pandas as pd\n",
                "from captum.attr import DeepLiftShap\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 0.  CONFIG & OUTPUT DIR\n",
                "# ------------------------------------------------------------\n",
                "plots_dir = \"attribution_plots\";  os.makedirs(plots_dir, exist_ok=True)\n",
                "# ATTR_START = \"2024-04-07 09:00\"\n",
                "# ATTR_END   = \"2024-04-07 17:00\"\n",
                "attribution_times = pd.date_range(ATTR_START, ATTR_END, freq=\"h\", tz=\"UTC\")\n",
                "print(\"Attribution times:\", attribution_times)\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 1.  BUILD INPUT / MASK TENSORS (unchanged)\n",
                "# ------------------------------------------------------------\n",
                "input_seqs, mask_seqs = [], []\n",
                "for t in attribution_times:\n",
                "    if t not in df.index:\n",
                "        print(f\"⚠️  {t} missing in df.index – skipped\");   continue\n",
                "    idx = df.index.get_loc(t)\n",
                "    if idx < seq_length:\n",
                "        print(f\"⚠️  Not enough history before {t} – skipped\");   continue\n",
                "    window_raw = df.iloc[idx - seq_length: idx][exogenous_vars].values\n",
                "    window_scaled = scaler_X.transform(window_raw)\n",
                "    mask = (~np.isnan(window_scaled)).astype(np.float32)\n",
                "    window_scaled[np.isnan(window_scaled)] = 0.0\n",
                "    input_seqs.append(window_scaled.astype(np.float32));  mask_seqs.append(mask)\n",
                "\n",
                "if not input_seqs:  raise RuntimeError(\"No valid windows after filtering.\")\n",
                "\n",
                "X_attr      = torch.tensor(np.stack(input_seqs)).to(DEVICE)      # (N, L, F)\n",
                "X_mask_attr = torch.tensor(np.stack(mask_seqs)).to(DEVICE)       # (N, L, F)\n",
                "print(\"Any NaNs? X:\", torch.isnan(X_attr).any().item(),\n",
                "      \"mask:\", torch.isnan(X_mask_attr).any().item())\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 2.  BASELINES (mean instead of pre-spike)\n",
                "# ------------------------------------------------------------\n",
                "zeros_X  = torch.zeros_like(X_attr[:1])\n",
                "median_X = torch.median(X_attr, dim=0, keepdim=True).values\n",
                "\n",
                "# Compute mean baseline over all valid attribution windows\n",
                "mean_raw = []\n",
                "for t in attribution_times:\n",
                "    idx = df.index.get_loc(t)\n",
                "    if idx < seq_length:\n",
                "        continue\n",
                "    window_raw = df.iloc[idx - seq_length: idx][exogenous_vars]\n",
                "    mean_raw.append(window_raw.values)\n",
                "if not mean_raw:\n",
                "    raise ValueError(\"No valid windows to compute mean baseline.\")\n",
                "mean_raw = np.mean(np.stack(mean_raw), axis=0)  # shape: (L, F)\n",
                "mean_scaled = scaler_X.transform(mean_raw)\n",
                "mask_mean = (~np.isnan(mean_scaled)).astype(np.float32)\n",
                "mean_scaled[np.isnan(mean_scaled)] = 0.0\n",
                "baseline_mean_X = torch.tensor(mean_scaled, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
                "baseline_mean_M = torch.tensor(mask_mean,        dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
                "\n",
                "baselines = {\n",
                "    \"zeros\":     (zeros_X,        X_mask_attr[:1]),\n",
                "    \"median\":    (median_X,       X_mask_attr[:1]),\n",
                "    \"mean\":      (baseline_mean_X, baseline_mean_M),\n",
                "}\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 3.  DEEPLIFT SHAP (wrapped model)\n",
                "# ------------------------------------------------------------\n",
                "class Wrapper(torch.nn.Module):\n",
                "    def __init__(self, core):  super().__init__();  self.core = core\n",
                "    def forward(self, x, m):   return self.core(x, m)\n",
                "\n",
                "dlshap = DeepLiftShap(Wrapper(model))\n",
                "\n",
                "attr_results = {}\n",
                "for name, (base_x, base_m) in baselines.items():\n",
                "    (attr_x, _attr_m), delta = dlshap.attribute(\n",
                "        inputs=(X_attr, X_mask_attr),\n",
                "        baselines=(base_x.expand_as(X_attr),\n",
                "                   base_m.expand_as(X_mask_attr)),\n",
                "        target=0,\n",
                "        return_convergence_delta=True\n",
                "    )\n",
                "    print(f\"{name:9s} | attr range \"\n",
                "          f\"[{attr_x.min():.2e}, {attr_x.max():.2e}] \"\n",
                "          f\"δ-mean={delta.abs().mean():.2e}\")\n",
                "    attr_results[name] = attr_x.detach().cpu().numpy()   # (N, L, F)\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 4.  AGGREGATE IMPORTANCE (unchanged)\n",
                "# ------------------------------------------------------------\n",
                "attr_summary = {}\n",
                "for name, a in attr_results.items():\n",
                "    importance = np.mean(np.abs(a), axis=(0, 1))\n",
                "    importance /= importance.sum() + 1e-12\n",
                "    attr_summary[name] = importance\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 5.  PLOT (only titles & file name changed)\n",
                "# ------------------------------------------------------------\n",
                "top_k, colors = 10, [\"#3498db\", \"#2ecc71\", \"#e74c3c\"]\n",
                "fig, axes = plt.subplots(2, 3, figsize=(20, 8));  axes = axes.flatten()\n",
                "\n",
                "for i, (name, imp) in enumerate(attr_summary.items()):\n",
                "    idx_sorted = np.argsort(imp)[-top_k:][::-1];  ids = [f\"#{idx+1}\" for idx in idx_sorted]\n",
                "    ax = axes[i]\n",
                "    ax.bar(range(top_k), imp[idx_sorted] * 100, color=colors[i])\n",
                "    ax.set_xticks(range(top_k));  ax.set_xticklabels(ids)\n",
                "    ax.set_xlabel(\"Feature ID\");   ax.set_ylabel(\"Contribution (%)\")\n",
                "    ax.set_title(f\"DeepLift SHAP – {name}\")\n",
                "    ax_t = axes[i + 3];  ax_t.axis(\"off\")\n",
                "    ax_t.set_title(f\"{name}  mapping\", pad=10)\n",
                "    tbl = ax_t.table(cellText=[[ids[j], exogenous_vars[idx_sorted[j]]] for j in range(top_k)],\n",
                "                     colLabels=[\"ID\", \"Feature\"], cellLoc=\"left\", loc=\"center\",\n",
                "                     colWidths=[0.07, 0.93])\n",
                "    tbl.auto_set_font_size(False); tbl.set_fontsize(9); tbl.scale(1.2, 1.2)\n",
                "\n",
                "plt.suptitle(\"Feature Attribution (DeepLift SHAP)\\n\"\n",
                "             f\"{ATTR_START} – {ATTR_END}\", fontsize=16)\n",
                "plt.tight_layout(rect=[0,0,1,0.96])\n",
                "out_path = os.path.join(plots_dir, \"deeplift_shap_selected_window.png\")\n",
                "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
                "plt.show();  print(\"Saved ➜\", out_path)\n",
                "\n",
                "attr_summaries[\"DL_SHAP\"] = {k: v.copy() for k, v in attr_summary.items()}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## FA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "from captum.attr import FeatureAblation\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 0.  CONFIG & OUTPUT DIR\n",
                "# ------------------------------------------------------------\n",
                "plots_dir = \"attribution_plots\"\n",
                "os.makedirs(plots_dir, exist_ok=True)\n",
                "\n",
                "# ATTR_START = \"2024-04-07 09:00\"\n",
                "# ATTR_END   = \"2024-04-07 17:00\"\n",
                "attribution_times = pd.date_range(\n",
                "    ATTR_START, ATTR_END, freq=\"h\", tz=\"UTC\"\n",
                ")\n",
                "print(\"Attribution times:\", attribution_times)\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 1.  BUILD INPUT / MASK TENSORS  *** SAME AS INFERENCE ***\n",
                "# ------------------------------------------------------------\n",
                "input_seqs, mask_seqs = [], []\n",
                "\n",
                "for t in attribution_times:\n",
                "    if t not in df.index:\n",
                "        print(f\"⚠️  {t} missing in df.index – skipped\");   continue\n",
                "    idx = df.index.get_loc(t)\n",
                "    if idx < seq_length:\n",
                "        print(f\"⚠️  Not enough history before {t} – skipped\");   continue\n",
                "\n",
                "    # raw window (seq_len × features) --------------------------\n",
                "    window_raw = df.iloc[idx - seq_length: idx][exogenous_vars].values\n",
                "\n",
                "    # ---> scale exactly like in inference\n",
                "    window_scaled = scaler_X.transform(window_raw)\n",
                "\n",
                "    # ---> build mask (1 = present, 0 = missing)\n",
                "    mask = (~np.isnan(window_scaled)).astype(np.float32)\n",
                "\n",
                "    # ---> impute NaNs with 0  (model expects this)\n",
                "    window_scaled[np.isnan(window_scaled)] = 0.0\n",
                "\n",
                "    input_seqs.append(window_scaled.astype(np.float32))\n",
                "    mask_seqs.append(mask)\n",
                "\n",
                "if not input_seqs:\n",
                "    raise RuntimeError(\"No valid windows after filtering.\")\n",
                "\n",
                "X_attr      = torch.tensor(np.stack(input_seqs)).to(DEVICE)       # (N, L, F)\n",
                "X_mask_attr = torch.tensor(np.stack(mask_seqs)).to(DEVICE)        # (N, L, F)\n",
                "\n",
                "# quick sanity-check\n",
                "print(\"Any NaNs after preprocessing?  X:\", torch.isnan(X_attr).any().item(),\n",
                "      \" mask:\", torch.isnan(X_mask_attr).any().item())\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 2.  BASELINES  (in scaled space, no NaNs) – USE MEAN INSTEAD OF PRE-SPIKE\n",
                "# ------------------------------------------------------------\n",
                "zeros_X  = torch.zeros_like(X_attr[:1])\n",
                "median_X = torch.median(X_attr, dim=0, keepdim=True).values\n",
                "\n",
                "# --- Compute mean baseline over all valid attribution windows ---\n",
                "mean_raw = []\n",
                "for t in attribution_times:\n",
                "    idx = df.index.get_loc(t)\n",
                "    if idx < seq_length:\n",
                "        continue\n",
                "    window_raw = df.iloc[idx - seq_length: idx][exogenous_vars]\n",
                "    mean_raw.append(window_raw.values)\n",
                "if not mean_raw:\n",
                "    raise ValueError(\"No valid windows to compute mean baseline.\")\n",
                "mean_raw = np.mean(np.stack(mean_raw), axis=0)  # shape: (L, F)\n",
                "mean_scaled = scaler_X.transform(mean_raw)\n",
                "mask_mean = (~np.isnan(mean_scaled)).astype(np.float32)\n",
                "mean_scaled[np.isnan(mean_scaled)] = 0.0\n",
                "baseline_mean_X = torch.tensor(mean_scaled, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
                "baseline_mean_M = torch.tensor(mask_mean,        dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
                "\n",
                "baselines = {\n",
                "    \"zeros\":     (zeros_X,   X_mask_attr[:1]),\n",
                "    \"median\":    (median_X,  X_mask_attr[:1]),\n",
                "    # \"mean\":      (baseline_mean_X, baseline_mean_M),\n",
                "}\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 3.  FEATURE ABLATION  – Captum\n",
                "# ------------------------------------------------------------\n",
                "def model_forward(x, m):\n",
                "    return model(x, m)            # (batch, 1, 1)\n",
                "\n",
                "fa = FeatureAblation(model_forward)\n",
                "\n",
                "attr_results = {}\n",
                "for name, (base_x, base_m) in baselines.items():\n",
                "    # >>>> Feature Ablation call mirrors previous IG signature\n",
                "    attr_tuple = fa.attribute(\n",
                "        inputs=(X_attr, X_mask_attr),\n",
                "        baselines=(base_x.expand_as(X_attr),\n",
                "                   base_m.expand_as(X_mask_attr)),\n",
                "        target=0,\n",
                "        perturbations_per_eval=32,    # controls internal batching like internal_batch_size\n",
                "        feature_mask=None             # default: each scalar is its own feature\n",
                "    )\n",
                "\n",
                "    # attr_tuple is (attr_X, attr_mask)\n",
                "    attr_x, _ = attr_tuple\n",
                "    print(f\"{name:9s} | attr range [{attr_x.min():.2e}, {attr_x.max():.2e}]\")\n",
                "\n",
                "    attr_results[name] = attr_x.cpu().numpy()   # (N, L, F) – keep only X\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 4.  AGGREGATE IMPORTANCE  (unchanged)\n",
                "#     • |attr|  →  mean over (sample, time)  →  normalise to %\n",
                "# ------------------------------------------------------------\n",
                "attr_summary = {}\n",
                "for name, a in attr_results.items():\n",
                "    importance = np.mean(np.abs(a), axis=(0, 1))  # (F,)\n",
                "    importance /= importance.sum() + 1e-12        # convert to %\n",
                "    attr_summary[name] = importance\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 5.  PLOT\n",
                "# ------------------------------------------------------------\n",
                "top_k   = 10\n",
                "colors  = [\"#3498db\", \"#2ecc71\", \"#e74c3c\"]\n",
                "fig, axes = plt.subplots(2, 3, figsize=(20, 8))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, (name, imp) in enumerate(attr_summary.items()):\n",
                "    idx_sorted = np.argsort(imp)[-top_k:][::-1]\n",
                "    feature_indices = idx_sorted      # indices into exogenous_vars\n",
                "    ids = [f\"#{idx+1}\" for idx in feature_indices]   # consistent ID = index+1\n",
                "\n",
                "    # --- bars ---\n",
                "    ax = axes[i]\n",
                "    ax.bar(range(top_k), imp[feature_indices] * 100, color=colors[i])       # %\n",
                "    ax.set_xticks(range(top_k))\n",
                "    ax.set_xticklabels(ids)\n",
                "    ax.set_xlabel(\"Feature ID\")\n",
                "    ax.set_ylabel(\"Contribution (%)\")\n",
                "    ax.set_title(f\"Feature Ablation – {name}\")\n",
                "\n",
                "    # --- table ---\n",
                "    ax_t = axes[i + 3]\n",
                "    table_data = [[ids[j], exogenous_vars[feature_indices[j]]]\n",
                "                  for j in range(top_k)]\n",
                "    ax_t.axis(\"off\")\n",
                "    ax_t.set_title(f\"{name}  mapping\", pad=10)\n",
                "    tbl = ax_t.table(\n",
                "        cellText=table_data, colLabels=[\"ID\", \"Feature\"],\n",
                "        cellLoc=\"left\", loc=\"center\",\n",
                "        colWidths=[0.07, 0.93],\n",
                "    )\n",
                "    tbl.auto_set_font_size(False)\n",
                "    tbl.set_fontsize(9)\n",
                "    tbl.scale(1.2, 1.2)\n",
                "\n",
                "plt.suptitle(\"Feature Attribution (Feature Ablation)\\n\"\n",
                "             f\"{ATTR_START} – {ATTR_END}\", fontsize=16)\n",
                "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
                "\n",
                "out_path = os.path.join(plots_dir, \"feature_ablation_selected_window.png\")\n",
                "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
                "plt.show()\n",
                "print(\"Saved ➜\", out_path)\n",
                "\n",
                "attr_summaries[\"FA\"] = {k: v.copy() for k, v in attr_summary.items()}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Top params"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ------------------------------------------------------------------\n",
                "# ❶  PARAMETERS & SANITY-CHECK\n",
                "# ------------------------------------------------------------------\n",
                "top_k = 10\n",
                "methods    = list(attr_summaries.keys())                     # ['IG', 'DL_SHAP', 'FA']\n",
                "baselines  = list(attr_summaries[methods[0]].keys())         # ['zeros', 'median', 'pre_spike']\n",
                "\n",
                "print(\"Methods   :\", methods)\n",
                "print(\"Baselines :\", baselines)\n",
                "\n",
                "# ------------------------------------------------------------------\n",
                "# ❷  COLLECT UNIQUE INDICES PER BASELINE\n",
                "# ------------------------------------------------------------------\n",
                "baseline_to_idxs = {b: set() for b in baselines}             # { 'zeros': set(), ... }\n",
                "\n",
                "for method in methods:\n",
                "    for baseline, importance in attr_summaries[method].items():\n",
                "        top_idx = np.argsort(importance)[-top_k:]            # this method+baseline’s top-k\n",
                "        baseline_to_idxs[baseline].update(top_idx)           # accumulate in the set\n",
                "\n",
                "# ------------------------------------------------------------------\n",
                "# ❸  REPORT\n",
                "# ------------------------------------------------------------------\n",
                "for baseline in baselines:\n",
                "    idxs  = sorted(baseline_to_idxs[baseline])\n",
                "    names = [exogenous_vars[i] for i in idxs]\n",
                "\n",
                "    print(f\"\\nBaseline: {baseline}\")\n",
                "    print(f\"Unique top {top_k} features across {len(methods)} methods: {len(names)}\")\n",
                "    print(\"-\"*60)\n",
                "    for n, feat in enumerate(names, 1):\n",
                "        print(f\"{n:2d}. {feat}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Top params plot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib as mpl\n",
                "\n",
                "# --------------------------------------------------\n",
                "# ❶ CONFIG - Enhanced for professional presentation\n",
                "# --------------------------------------------------\n",
                "top_k = 10\n",
                "methods = [\"IG\", \"DL_SHAP\", \"FA\"]\n",
                "# Scientific color palette (colorblind-friendly)\n",
                "colors = [\"#4477AA\", \"#66CCEE\", \"#EE6677\"]  \n",
                "bar_width = 0.25\n",
                "font_family = 'serif'  # Academic standard\n",
                "\n",
                "# Set overall matplotlib style for thesis-quality figures\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "mpl.rcParams['font.family'] = font_family\n",
                "mpl.rcParams['font.size'] = 16\n",
                "mpl.rcParams['axes.titlesize'] = 18\n",
                "mpl.rcParams['axes.labelsize'] = 17\n",
                "mpl.rcParams['xtick.labelsize'] = 16\n",
                "mpl.rcParams['ytick.labelsize'] = 16\n",
                "mpl.rcParams['legend.fontsize'] = 16\n",
                "mpl.rcParams['figure.titlesize'] = 20\n",
                "\n",
                "# --------------------------------------------------\n",
                "# ❷ COLLECT UNIQUE TOP-k FEATURES PER BASELINE (exclude 'mean')\n",
                "# --------------------------------------------------\n",
                "all_baselines = list(attr_summaries[methods[0]].keys())\n",
                "baselines = [b for b in all_baselines if b != \"mean\"]\n",
                "baseline_to_idxs = {b: set() for b in baselines}\n",
                "\n",
                "for m in methods:\n",
                "    for b, imp in attr_summaries[m].items():\n",
                "        if b == \"mean\":\n",
                "            continue\n",
                "        top_idx = np.argsort(imp)[-top_k:]\n",
                "        baseline_to_idxs[b].update(top_idx)\n",
                "\n",
                "# --------------------------------------------------\n",
                "# ❸ BUILD THE FIGURE - Enhanced for thesis presentation\n",
                "# --------------------------------------------------\n",
                "fig, axes = plt.subplots(len(baselines), 1,\n",
                "                         figsize=(10, 3.5*len(baselines)),  # More compact, thesis-friendly ratio\n",
                "                         constrained_layout=True)  # Better spacing management\n",
                "\n",
                "# Title and subtitle removed per user request\n",
                "\n",
                "if len(baselines) == 1:\n",
                "    axes = [axes]\n",
                "\n",
                "# Corrected method names for legend\n",
                "method_names = {\n",
                "    \"IG\": \"Integrated Gradient\",\n",
                "    \"DL_SHAP\": \"DeepLIFT SHAP\",\n",
                "    \"FA\": \"Feature Ablation\"\n",
                "}\n",
                "\n",
                "for row, baseline in enumerate(baselines):\n",
                "    ax = axes[row]\n",
                "    idxs = sorted(baseline_to_idxs[baseline])\n",
                "    n = len(idxs)\n",
                "    \n",
                "    # x-locations for the centre of each \"feature group\"\n",
                "    x_centres = np.arange(n)\n",
                "    \n",
                "    # Draw bars with enhanced styling\n",
                "    for j, (method, color) in enumerate(zip(methods, colors)):\n",
                "        imp = attr_summaries[method][baseline] * 100\n",
                "        heights = imp[idxs]\n",
                "        \n",
                "        # Add bars with hatching for better distinction in grayscale printing\n",
                "        hatch_patterns = ['', '///', '...']\n",
                "        ax.bar(x_centres + (j-1)*bar_width, heights,\n",
                "               width=bar_width, color=color, alpha=0.85,\n",
                "               hatch=hatch_patterns[j], \n",
                "               label=method_names[method] if row==0 else \"\",\n",
                "               edgecolor='black', linewidth=0.5)\n",
                "    \n",
                "    # Enhanced subplot styling\n",
                "    ax.set_title(f\"Baseline: {baseline.capitalize()}\", loc='left', fontweight='normal')\n",
                "    ax.set_ylabel(\"Contribution (%)\", fontweight='normal')\n",
                "    ax.set_xlabel(\"Feature Index\", fontweight='normal')\n",
                "    ax.set_xticks(x_centres)\n",
                "\n",
                "    \n",
                "    # Improved tick labels with smaller rotation for readability\n",
                "    feature_labels = [f\"#{i+1}\" for i in idxs]\n",
                "    ax.set_xticklabels(feature_labels, rotation=45, ha='right')\n",
                "    \n",
                "    # Refined grid\n",
                "    ax.grid(axis=\"y\", alpha=0.3, linestyle='--')\n",
                "    \n",
                "    # Add spines for more professional look\n",
                "    for spine in ax.spines.values():\n",
                "        spine.set_visible(True)\n",
                "        spine.set_linewidth(0.75)\n",
                "    \n",
                "    # Add a horizontal line at y=0\n",
                "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8, alpha=0.5)\n",
                "\n",
                "# Create legend with enhanced styling - top right with frame\n",
                "handles, labels = axes[0].get_legend_handles_labels()\n",
                "legend = fig.legend(handles, labels, loc='upper center', \n",
                "                   bbox_to_anchor=(0.5, 0.01),  # Below all subplots\n",
                "                   ncol=len(methods), frameon=True, \n",
                "                   fancybox=True, shadow=True)\n",
                "\n",
                "# Figure caption removed as requested\n",
                "\n",
                "plt.savefig('feature_importance_comparison.pdf', bbox_inches='tight', dpi=300)\n",
                "plt.savefig('feature_importance_comparison.png', bbox_inches='tight', dpi=300)\n",
                "plt.show()\n",
                "\n",
                "# --------------------------------------------------\n",
                "# ❹ PRINT THE \"# – Feature name\" LISTS (with enhanced formatting)\n",
                "# --------------------------------------------------\n",
                "print(\"\\nTable X: Feature Index Mapping\")\n",
                "print(\"=\" * 50)\n",
                "for baseline in baselines:\n",
                "    idxs = sorted(baseline_to_idxs[baseline])\n",
                "    print(f\"\\nBaseline: {baseline.capitalize()}\")\n",
                "    print(\"-\" * 40)\n",
                "    print(f\"{'Index':<8} {'Feature Name':<30}\")\n",
                "    print(\"-\" * 40)\n",
                "    for i in idxs:\n",
                "        print(f\"#{i+1:<7} {exogenous_vars[i]:<30}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%\n",
                "# Print mapping from index (starting at 1) to feature name as a dictionary\n",
                "\n",
                "feature_dict = {i + 1: name for i, name in enumerate(exogenous_vars)}\n",
                "print(\"Index-to-Feature Mapping:\")\n",
                "print(feature_dict)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature plots"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib as mpl\n",
                "from math import ceil\n",
                "import matplotlib.dates as mdates\n",
                "from matplotlib.ticker import MaxNLocator\n",
                "import textwrap\n",
                "\n",
                "# --------------------------------------------------\n",
                "# ❶ USER CONFIG - Enhanced for thesis presentation\n",
                "# --------------------------------------------------\n",
                "top_n = 6\n",
                "ATTR_START = pd.Timestamp(\"2024-04-03 10:00\", tz=\"UTC\")\n",
                "ATTR_END = pd.Timestamp(\"2024-04-11 16:00\", tz=\"UTC\")\n",
                "PEAK_START = pd.Timestamp(\"2024-04-07 10:00\", tz=\"UTC\")\n",
                "PEAK_END = pd.Timestamp(\"2024-04-07 16:00\", tz=\"UTC\")\n",
                "\n",
                "\n",
                "plot_style = \"subplots\"\n",
                "use_zscore = False\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "font_family = 'serif'\n",
                "mpl.rcParams['font.family'] = font_family\n",
                "mpl.rcParams['font.size'] = 12\n",
                "mpl.rcParams['axes.titlesize'] = 14\n",
                "mpl.rcParams['axes.labelsize'] = 13\n",
                "mpl.rcParams['xtick.labelsize'] = 11\n",
                "mpl.rcParams['ytick.labelsize'] = 11\n",
                "mpl.rcParams['legend.fontsize'] = 12\n",
                "mpl.rcParams['figure.titlesize'] = 16\n",
                "\n",
                "colors = plt.cm.tab10.colors\n",
                "\n",
                "\n",
                "methods = [\"IG\", \"DL_SHAP\", \"FA\"]\n",
                "method_names = {\n",
                "    \"IG\": \"Integrated Gradient\",\n",
                "    \"DL_SHAP\": \"DeepLIFT SHAP\",\n",
                "    \"FA\": \"Feature Ablation\"\n",
                "}\n",
                "all_baselines = list(attr_summaries[methods[0]].keys())\n",
                "baselines = [b for b in all_baselines if b != \"mean\"]\n",
                "\n",
                "attr_pct = {}\n",
                "for m in methods:\n",
                "    attr_pct[m] = {}\n",
                "    for b in baselines:\n",
                "        v = attr_summaries[m][b].astype(float)\n",
                "        v = v / (v.sum() + 1e-12)\n",
                "        attr_pct[m][b] = v\n",
                "\n",
                "all_vectors = [attr_pct[m][b] for m in methods for b in baselines]\n",
                "mean_imp = np.mean(all_vectors, axis=0)\n",
                "top_idx = np.argsort(mean_imp)[-top_n:][::-1]\n",
                "top_names = [exogenous_vars[i] for i in top_idx]\n",
                "\n",
                "slice_df = df.loc[ATTR_START:ATTR_END, top_names]\n",
                "if use_zscore and plot_style == \"overlay\":\n",
                "    slice_df = (slice_df - slice_df.mean()) / slice_df.std(ddof=0)\n",
                "\n",
                "# --------------------------------------------------\n",
                "# ❺ PLOT - Enhanced for thesis quality: 3x2 subplots, wrapped titles\n",
                "# --------------------------------------------------\n",
                "# ----- THIS FUNCTION CHOOSES THE BEST LOCATOR BASED ON DURATION -----\n",
                "def get_time_locator(start, end):\n",
                "    duration_hours = (end - start).total_seconds() / 3600\n",
                "    if duration_hours <= 48:\n",
                "        return mdates.HourLocator(interval=2), mdates.DateFormatter('%H:%M\\n%d-%b')\n",
                "    elif duration_hours <= 96:\n",
                "        return mdates.HourLocator(interval=6), mdates.DateFormatter('%d-%b\\n%H:%M')\n",
                "    elif duration_hours <= 168:\n",
                "        return mdates.HourLocator(interval=12), mdates.DateFormatter('%d-%b\\n%H:%M')\n",
                "    else:\n",
                "        return mdates.DayLocator(interval=1), mdates.DateFormatter('%d-%b')\n",
                "# --------------------------------------------------------------------\n",
                "\n",
                "if plot_style == \"overlay\":\n",
                "    fig, ax = plt.subplots(figsize=(10, 6))\n",
                "    ax.axvspan(PEAK_START, PEAK_END, alpha=0.15, color='gray', label='Peak Period')\n",
                "    line_styles = ['-', '--', ':', '-.', '-', '--']\n",
                "    for i, (name, style) in enumerate(zip(top_names, line_styles)):\n",
                "        ax.plot(slice_df.index, slice_df[name],\n",
                "                linewidth=2.5,\n",
                "                color=colors[top_idx[i] % len(colors)],  # Changed here\n",
                "                linestyle=style,\n",
                "                label=f\"#{top_idx[i]+1} {name}\")\n",
                "    locator, formatter = get_time_locator(ATTR_START, ATTR_END) # <-- FIXED\n",
                "    ax.xaxis.set_major_locator(locator)\n",
                "    ax.xaxis.set_major_formatter(formatter)\n",
                "    ax.set_title(\"Temporal Evolution of Top Feature Contributions\", fontweight='normal', pad=15)\n",
                "    ax.set_ylabel(\"Z-Score\" if use_zscore else \"Feature Value\", fontweight='normal')\n",
                "    ax.set_xlabel(\"Date\", fontweight='normal')\n",
                "    ax.grid(alpha=0.3, linestyle='--')\n",
                "    for spine in ax.spines.values():\n",
                "        spine.set_visible(True)\n",
                "        spine.set_linewidth(0.75)\n",
                "    handles, labels = ax.get_legend_handles_labels()\n",
                "    if 'Peak Period' not in labels:\n",
                "        from matplotlib.patches import Patch\n",
                "        handles.append(Patch(facecolor='gray', alpha=0.15))\n",
                "        labels.append('Peak Period (23:00-05:00)')\n",
                "    ax.legend(handles, labels, bbox_to_anchor=(1.01, 1), loc=\"bottom left\", frameon=True, fancybox=True, shadow=True)\n",
                "    fig.tight_layout()\n",
                "\n",
                "elif plot_style == \"subplots\":\n",
                "    nrows, ncols = 3, 2\n",
                "    fig, axes = plt.subplots(nrows, ncols, figsize=(13, 9), sharex=True, constrained_layout=True)\n",
                "    axes = axes.flatten()\n",
                "    fig.suptitle(\"Temporal Evolution of Key Feature Contributions (Case 2)\", fontweight='normal', y=1.06)\n",
                "    wrap_width = 65\n",
                "\n",
                "    locator, formatter = get_time_locator(ATTR_START, ATTR_END)  # <-- FIXED\n",
                "\n",
                "    for i, (ax, idx, name) in enumerate(zip(axes, top_idx, top_names)):\n",
                "        ax.axvspan(PEAK_START, PEAK_END, alpha=0.2, color='gray')\n",
                "        ax.plot(slice_df.index, slice_df[name], linewidth=2.5, color=colors[idx % len(colors)])  # Changed here\n",
                "        feature_title = f\"#{idx+1}: {name} ({mean_imp[idx]*100:.2f}%)\"\n",
                "        feature_title_wrapped = \"\\n\".join(textwrap.wrap(feature_title, wrap_width))\n",
                "        ax.set_title(feature_title_wrapped, loc='left', fontweight='normal', fontsize=11)\n",
                "        ax.set_ylabel(\"Value\", fontweight='normal')\n",
                "        ax.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
                "        ax.grid(alpha=0.3, linestyle='--')\n",
                "        for spine in ax.spines.values():\n",
                "            spine.set_visible(True)\n",
                "            spine.set_linewidth(0.75)\n",
                "        val_min = slice_df[name].min()\n",
                "        val_max = slice_df[name].max()\n",
                "        if val_min == val_max:\n",
                "            delta = val_min * 0.01 if val_min != 0 else 0.01\n",
                "            ax.axhspan(val_min - delta, val_max + delta, alpha=0.1, color=colors[idx % len(colors)])  # Changed here\n",
                "        else:\n",
                "            ax.axhspan(val_min, val_max, alpha=0.1, color=colors[idx % len(colors)])  # Changed here\n",
                "        if i == 0:\n",
                "            ax.text(0.98, 0.95, 'Peak Period', transform=ax.transAxes,\n",
                "                    bbox=dict(facecolor='gray', alpha=0.2, edgecolor='none', pad=3),\n",
                "                    ha='right', va='top', fontsize=10)\n",
                "    # Turn off any unused axes (if top_n < nrows*ncols)\n",
                "    for j in range(top_n, nrows * ncols):\n",
                "        fig.delaxes(axes[j])\n",
                "    # Format x-axis for time data on the bottom row\n",
                "    for ax in axes[-ncols:]:\n",
                "        ax.xaxis.set_major_locator(locator)      # <-- FIXED\n",
                "        ax.xaxis.set_major_formatter(formatter)  # <-- FIXED\n",
                "        ax.set_xlabel(\"Date\", fontweight='normal')\n",
                "    period_text = (f\"Period: {ATTR_START.strftime('%Y-%m-%d %H:%M')} to {ATTR_END.strftime('%Y-%m-%d %H:%M')} UTC | \"\n",
                "                  f\"Peak Activity: {PEAK_START.strftime('%Y-%m-%d %H:%M')} to {PEAK_END.strftime('%Y-%m-%d %H:%M')} UTC\")\n",
                "    fig.text(0.5, 1.02, period_text, ha='center', fontstyle='italic', fontsize=11)\n",
                "\n",
                "else:\n",
                "    raise ValueError(\"plot_style must be 'subplots' or 'overlay'\")\n",
                "\n",
                "plt.savefig('feature_timeseries_analysis.pdf', bbox_inches='tight', dpi=300)\n",
                "plt.savefig('feature_timeseries_analysis.png', bbox_inches='tight', dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib as mpl\n",
                "from math import ceil\n",
                "import matplotlib.dates as mdates\n",
                "from matplotlib.ticker import MaxNLocator\n",
                "import textwrap\n",
                "\n",
                "# ----- YOUR CONFIG -----\n",
                "top_n = 6\n",
                "ATTR_START = pd.Timestamp(\"2024-04-03 10:00\", tz=\"UTC\")\n",
                "ATTR_END   = pd.Timestamp(\"2024-04-11 16:00\", tz=\"UTC\")\n",
                "PEAK_START = pd.Timestamp(\"2024-04-07 10:00\", tz=\"UTC\")\n",
                "PEAK_END   = pd.Timestamp(\"2024-04-07 16:00\", tz=\"UTC\")\n",
                "PEAK_PRICE = pd.Timestamp(\"2024-04-07 13:00\", tz=\"UTC\")  # New peak price line\n",
                "\n",
                "plot_style = \"subplots\"\n",
                "use_zscore = False\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "font_family = 'serif'\n",
                "mpl.rcParams['font.family'] = font_family\n",
                "mpl.rcParams['font.size'] = 16\n",
                "mpl.rcParams['axes.titlesize'] = 18\n",
                "mpl.rcParams['axes.labelsize'] = 15\n",
                "mpl.rcParams['xtick.labelsize'] = 14\n",
                "mpl.rcParams['ytick.labelsize'] = 14\n",
                "mpl.rcParams['legend.fontsize'] = 16\n",
                "mpl.rcParams['figure.titlesize'] = 20\n",
                "\n",
                "# --- EXPLICIT HIGH-CONTRAST COLORS FOR YOUR INDEXES ---\n",
                "SPECIAL_INDEX_COLORS = { # index nb - 1\n",
                "    6:   \"#1f77b4\",  # blue\n",
                "    14:  \"#ff7f0e\",  # orange\n",
                "    16:  \"#e377c2\",  # pink\n",
                "    17:  \"#bcbd22\",  # olive/lime\n",
                "    29:  \"#8c564b\",  # brown\n",
                "    32:  \"#9467bd\",  # purple\n",
                "    33:  \"#7f7f7f\",  # gray\n",
                "    34:  \"#d62728\",  # red\n",
                "    35:  \"#2ca02c\",  # green\n",
                "}\n",
                "\n",
                "def get_feature_color(idx, default_palette=plt.cm.tab10.colors):\n",
                "    if idx in SPECIAL_INDEX_COLORS:\n",
                "        return SPECIAL_INDEX_COLORS[idx]\n",
                "    return default_palette[idx % len(default_palette)]\n",
                "\n",
                "# --- YOUR DATA/PREPROCESSING ---\n",
                "\n",
                "methods = [\"IG\", \"DL_SHAP\", \"FA\"]\n",
                "all_baselines = list(attr_summaries[methods[0]].keys())\n",
                "baselines = [b for b in all_baselines if b != \"mean\"]\n",
                "\n",
                "attr_pct = {}\n",
                "for m in methods:\n",
                "    attr_pct[m] = {}\n",
                "    for b in baselines:\n",
                "        v = attr_summaries[m][b].astype(float)\n",
                "        v = v / (v.sum() + 1e-12)\n",
                "        attr_pct[m][b] = v\n",
                "\n",
                "all_vectors = [attr_pct[m][b] for m in methods for b in baselines]\n",
                "mean_imp = np.mean(all_vectors, axis=0)\n",
                "top_idx = np.argsort(mean_imp)[-top_n:][::-1]\n",
                "top_names = [exogenous_vars[i] for i in top_idx]\n",
                "\n",
                "slice_df = df.loc[ATTR_START:ATTR_END, top_names]\n",
                "if use_zscore and plot_style == \"overlay\":\n",
                "    slice_df = (slice_df - slice_df.mean()) / slice_df.std(ddof=0)\n",
                "\n",
                "def get_time_locator(start, end):\n",
                "    duration_hours = (end - start).total_seconds() / 3600\n",
                "    if duration_hours <= 48:\n",
                "        return mdates.HourLocator(interval=2), mdates.DateFormatter('%H:%M\\n%d-%b')\n",
                "    elif duration_hours <= 96:\n",
                "        return mdates.HourLocator(interval=6), mdates.DateFormatter('%d-%b\\n%H:%M')\n",
                "    elif duration_hours <= 168:\n",
                "        return mdates.HourLocator(interval=12), mdates.DateFormatter('%d-%b\\n%H:%M')\n",
                "    else:\n",
                "        return mdates.DayLocator(interval=1), mdates.DateFormatter('%d-%b')\n",
                "\n",
                "if plot_style == \"overlay\":\n",
                "    fig, ax = plt.subplots(figsize=(10, 6))\n",
                "    ax.axvspan(PEAK_START, PEAK_END, alpha=0.15, color='gray', label='Peak Period')\n",
                "    ax.axvline(PEAK_PRICE, color='red', linestyle='--', linewidth=2, label='Peak Price')\n",
                "    line_styles = ['-', '--', ':', '-.', '-', '--']\n",
                "    for i, (name, style) in enumerate(zip(top_names, line_styles)):\n",
                "        color = get_feature_color(top_idx[i])\n",
                "        ax.plot(slice_df.index, slice_df[name],\n",
                "                linewidth=2.5,\n",
                "                color=color,\n",
                "                linestyle=style,\n",
                "                label=f\"#{top_idx[i]+1} {name}\")\n",
                "    locator, formatter = get_time_locator(ATTR_START, ATTR_END)\n",
                "    ax.xaxis.set_major_locator(locator)\n",
                "    ax.xaxis.set_major_formatter(formatter)\n",
                "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
                "    ax.set_ylabel(\"Z-Score\" if use_zscore else \"Feature Value\", fontweight='normal')\n",
                "    ax.set_xlabel(\"Date\", fontweight='normal')\n",
                "    ax.grid(alpha=0.3, linestyle='--')\n",
                "    for spine in ax.spines.values():\n",
                "        spine.set_visible(True)\n",
                "        spine.set_linewidth(0.75)\n",
                "    handles, labels = ax.get_legend_handles_labels()\n",
                "    if 'Peak Period' not in labels:\n",
                "        from matplotlib.patches import Patch\n",
                "        handles.append(Patch(facecolor='gray', alpha=0.15))\n",
                "        labels.append('Peak Period (23:00-05:00)')\n",
                "    ax.legend(handles, labels, bbox_to_anchor=(1.01, 1), loc=\"bottom left\", frameon=True, fancybox=True, shadow=True)\n",
                "    fig.tight_layout()\n",
                "\n",
                "elif plot_style == \"subplots\":\n",
                "    nrows, ncols = 3, 2\n",
                "    fig, axes = plt.subplots(nrows, ncols, figsize=(13, 9), sharex=True, constrained_layout=True)\n",
                "    axes = axes.flatten()\n",
                "    wrap_width = 41\n",
                "\n",
                "    locator, formatter = get_time_locator(ATTR_START, ATTR_END)\n",
                "\n",
                "    for i, (ax, idx, name) in enumerate(zip(axes, top_idx, top_names)):\n",
                "        color = get_feature_color(idx)\n",
                "        ax.axvspan(PEAK_START, PEAK_END, alpha=0.3, color='gray')\n",
                "        ax.axvline(PEAK_PRICE, color='red', linestyle='--', linewidth=2)\n",
                "        ax.plot(slice_df.index, slice_df[name], linewidth=2.5, color=color)\n",
                "        feature_title = f\"#{idx+1}: {name} ({mean_imp[idx]*100:.2f}%)\"\n",
                "        feature_title_wrapped = \"\\n\".join(textwrap.wrap(feature_title, wrap_width))\n",
                "        ax.set_title(feature_title_wrapped, loc='left', fontweight='normal', fontsize=16)\n",
                "        ax.set_ylabel(\"Value\", fontweight='normal')\n",
                "        ax.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
                "        ax.grid(alpha=0.3, linestyle='--')\n",
                "        for spine in ax.spines.values():\n",
                "            spine.set_visible(True)\n",
                "            spine.set_linewidth(0.75)\n",
                "        val_min = slice_df[name].min()\n",
                "        val_max = slice_df[name].max()\n",
                "        # --------- THIS IS THE ONLY CHANGED PART -----------\n",
                "        # If idx == 36 (for index #36 Transmission_PhysicalFlows_FROM_SE2_FlowValue), \n",
                "        # don't include zero in the y-limits:\n",
                "        if idx == 35:\n",
                "            ax.set_ylim(val_min, val_max)\n",
                "            ax.axhspan(val_min  + 300 , val_max, alpha=0.1, color=color)\n",
                "        else:\n",
                "            if val_min == val_max:\n",
                "                delta = val_min * 0.01 if val_min != 0 else 0.01\n",
                "                ax.axhspan(val_min - delta, val_max + delta, alpha=0.1, color=color)\n",
                "            else:\n",
                "                ax.axhspan(val_min, val_max, alpha=0.1, color=color)\n",
                "                # Default: don't force to zero, unless you want to do so for all others (you currently don't)\n",
                "        # ----------------------------------------------------\n",
                "        if i == 0:\n",
                "            ax.text(0.98, 0.95, 'Peak Period', transform=ax.transAxes,\n",
                "                    bbox=dict(facecolor='gray', alpha=0.3, edgecolor='none', pad=3),\n",
                "                    ha='right', va='top', fontsize=14)\n",
                "            ax.text(0.98, 0.80, 'Peak Price', transform=ax.transAxes,\n",
                "                    bbox=dict(facecolor='red', alpha=0.3, edgecolor='none', pad=3),\n",
                "                    ha='right', va='top', fontsize=14)\n",
                "    for j in range(top_n, nrows * ncols):\n",
                "        fig.delaxes(axes[j])\n",
                "    for ax in axes[-ncols:]:\n",
                "        ax.xaxis.set_major_locator(locator)\n",
                "        ax.xaxis.set_major_formatter(formatter)\n",
                "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
                "        ax.set_xlabel(\"Date\", fontweight='normal')\n",
                "\n",
                "\n",
                "else:\n",
                "    raise ValueError(\"plot_style must be 'subplots' or 'overlay'\")\n",
                "\n",
                "plt.savefig('feature_timeseries_analysis.pdf', bbox_inches='tight', dpi=300)\n",
                "plt.savefig('feature_timeseries_analysis.png', bbox_inches='tight', dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Val set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import joblib\n",
                "import plotly.graph_objects as go\n",
                "\n",
                "# ---- PATHS ----\n",
                "MODEL_PATH = \"/new_model_test/best_model_SE2_Down_Volume.pth\"\n",
                "SCALER_X_PATH = \"/new_model_test/scaler_X_SE2_Down_Volume.joblib\"\n",
                "SCALER_Y_PATH = \"/new_model_test/scaler_y_SE2_Down_Volume.joblib\"\n",
                "FILE_PATH = \"/results/results_merged.csv\"\n",
                "\n",
                "REGION = \"SE2\"\n",
                "target_vars = [f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownActivatedVolume\"]\n",
                "\n",
                "# -- Model class --\n",
                "class MaskedLSTM(torch.nn.Module):\n",
                "    def __init__(self, input_size, hidden_size=32, num_layers=1, dense_size=32, output_size=1, horizon=1, dropout=0.08, dropout_lstm=0.27, bidirectional=True):\n",
                "        super().__init__()\n",
                "        self.horizon = horizon\n",
                "        self.output_size = output_size\n",
                "        actual_in = input_size * 2\n",
                "        self.lstm = torch.nn.LSTM(\n",
                "            input_size=actual_in,\n",
                "            hidden_size=hidden_size,\n",
                "            num_layers=num_layers,\n",
                "            dropout=dropout_lstm if num_layers > 1 else 0.0,\n",
                "            bidirectional=bidirectional,\n",
                "            batch_first=True,\n",
                "        )\n",
                "        hidden_out = hidden_size * (2 if bidirectional else 1)\n",
                "        self.fc1 = torch.nn.Linear(hidden_out, dense_size)\n",
                "        self.fc2 = torch.nn.Linear(dense_size, horizon * output_size)\n",
                "        self.relu = torch.nn.ReLU()\n",
                "        self.drop = torch.nn.Dropout(dropout)\n",
                "        self.drop_lstm = torch.nn.Dropout(dropout_lstm)\n",
                "    def forward(self, x, mask):\n",
                "        x = torch.cat((x, mask), dim=2)\n",
                "        out, _ = self.lstm(x)\n",
                "        out = self.drop_lstm(out)\n",
                "        out = self.relu(self.fc1(out[:, -1, :]))\n",
                "        out = self.drop(out)\n",
                "        out = self.fc2(out)\n",
                "        return out.view(out.size(0), self.horizon, self.output_size)\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "scaler_X = joblib.load(SCALER_X_PATH)\n",
                "scaler_y = joblib.load(SCALER_Y_PATH)\n",
                "\n",
                "best_params = {\n",
                "    \"hidden_size\": 32,\n",
                "    \"num_layers\": 1,\n",
                "    \"bidirectional\": True,\n",
                "    \"dense_size\": 32,\n",
                "    \"dropout\": 0.08137184695714653,\n",
                "    \"dropout_lstm\": 0.2675438586833995,\n",
                "    \"learning_rate\": 0.0013843186375837144,\n",
                "    \"batch_size\": 128,\n",
                "    \"seq_length\": 216,\n",
                "}\n",
                "seq_length = best_params[\"seq_length\"]\n",
                "\n",
                "# --- Load and preprocess data ---\n",
                "df_all = pd.read_csv(FILE_PATH)\n",
                "price_column_name_to_plot = f\"BalancingER_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownPrice\"\n",
                "\n",
                "# Store price data before dropping columns\n",
                "price_data_full = None\n",
                "if price_column_name_to_plot in df_all.columns:\n",
                "    price_data_full = df_all[['DateTime', price_column_name_to_plot]].copy()\n",
                "    price_data_full['DateTime'] = pd.to_datetime(price_data_full['DateTime'], utc=True)\n",
                "    price_data_full.set_index('DateTime', inplace=True)\n",
                "    print(f\"Stored '{price_column_name_to_plot}' separately for plotting.\")\n",
                "else:\n",
                "    print(f\"Warning: Price column '{price_column_name_to_plot}' not found in df_all. Will not be plotted.\")\n",
                "\n",
                "from filter_features import pick_region_filter\n",
                "BDZ_filter = pick_region_filter(region=REGION, remove_balancing=True)\n",
                "drop_list = [\n",
                "    f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedUpPrice\",\n",
                "    f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownPrice\",\n",
                "    f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedUpActivatedVolume\",\n",
                "    f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownActivatedVolume\",\n",
                "]\n",
                "BDZ_filter = [c for c in BDZ_filter if c not in drop_list or c in target_vars]\n",
                "df_all[\"DateTime\"] = pd.to_datetime(df_all[\"DateTime\"], utc=True)\n",
                "df_all.set_index(\"DateTime\", inplace=True)\n",
                "df_all = df_all.asfreq(\"h\")\n",
                "df = df_all[BDZ_filter].copy()\n",
                "df = df.asfreq(\"h\")\n",
                "\n",
                "def fill_ahead_features(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Forward-fills 0/NaN gaps in *-ahead* columns within their natural period\n",
                "    (day-ahead → daily, week-ahead → Monday-anchored weeks, etc.).\n",
                "\n",
                "    A value is propagated only up to the end of the period, and only while the\n",
                "    entries being filled are 0 or NaN.\n",
                "    \"\"\"\n",
                "    df = df.copy()\n",
                "\n",
                "    period_freq = {\n",
                "        \"dayahead\":  \"D\",        # daily groups\n",
                "        \"weekahead\": \"W-MON\",    # ISO weeks starting on Monday\n",
                "        \"monthahead\":\"MS\",       # month start\n",
                "        \"yearahead\": \"AS\"        # year start\n",
                "    }\n",
                "\n",
                "    for period, freq in period_freq.items():\n",
                "        ahead_cols = [c for c in df.columns if period in c.lower()]\n",
                "        if not ahead_cols:\n",
                "            continue\n",
                "\n",
                "        for col in ahead_cols:\n",
                "            s = df[col]\n",
                "\n",
                "            # treat *strictly* 0 as missing, but preserve genuine zeros by\n",
                "            # forward-filling only into 0/NaN slots\n",
                "            s_filled = (\n",
                "                s.replace(0, np.nan)                       # step 1: 0 → NaN\n",
                "                 .groupby(pd.Grouper(freq=freq))           # step 2: group\n",
                "                 .ffill()                                  # step 3: ffill inside group\n",
                "                 .fillna(0)                                # step 4: keep leading zeros\n",
                "            )\n",
                "\n",
                "            df[col] = np.where(\n",
                "                (s == 0) | s.isna(),        # fill only where original was 0/NaN\n",
                "                s_filled,                   #   …with the forward-filled value\n",
                "                s                           # keep genuine entries untouched\n",
                "            )\n",
                "\n",
                "    return df\n",
                "\n",
                "# \n",
                "\n",
                "\n",
                "CREATE_INDICATOR_FEATURES = False\n",
                "CREATE_CYCLICAL_FEATURES = False\n",
                "CREATE_LAGGED_FEATURES = False\n",
                "\n",
                "new_indicator_features = []\n",
                "if CREATE_INDICATOR_FEATURES:\n",
                "    indicator_features_to_create = [col for col in BDZ_filter + target_vars if col in df.columns]\n",
                "    for col in indicator_features_to_create:\n",
                "        df[f'{col}_was_missing'] = df[col].isnull().astype(int)\n",
                "    new_indicator_features = [f'{col}_was_missing' for col in indicator_features_to_create]\n",
                "\n",
                "new_time_features = []\n",
                "if CREATE_CYCLICAL_FEATURES:\n",
                "    df['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24.0)\n",
                "    df['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24.0)\n",
                "    df['dayofweek_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7.0)\n",
                "    df['dayofweek_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7.0)\n",
                "    df['weekofyear_sin'] = np.sin(2 * np.pi * df.index.isocalendar().week / 52.0)\n",
                "    df['weekofyear_cos'] = np.cos(2 * np.pi * df.index.isocalendar().week / 52.0)\n",
                "    new_time_features = ['hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'weekofyear_sin', 'weekofyear_cos']\n",
                "\n",
                "new_lagged_features = []\n",
                "if CREATE_LAGGED_FEATURES:\n",
                "    for target_var in target_vars:\n",
                "        df[f'{target_var}_lag1'] = df[target_var].shift(1)\n",
                "        df[f'{target_var}_lag24'] = df[target_var].shift(24)\n",
                "        df[f'{target_var}_lag168'] = df[target_var].shift(168)\n",
                "    for target_var in target_vars:\n",
                "        new_lagged_features.extend([f'{target_var}_lag1', f'{target_var}_lag24', f'{target_var}_lag168'])\n",
                "\n",
                "df.dropna(axis=1, how=\"all\", inplace=True)\n",
                "\n",
                "exogenous_vars = [c for c in BDZ_filter if c not in target_vars]\n",
                "if CREATE_CYCLICAL_FEATURES:\n",
                "    exogenous_vars.extend(new_time_features)\n",
                "if CREATE_LAGGED_FEATURES:\n",
                "    exogenous_vars.extend(new_lagged_features)\n",
                "if CREATE_INDICATOR_FEATURES:\n",
                "    exogenous_vars.extend(new_indicator_features)\n",
                "exogenous_vars = sorted(list(set(var for var in exogenous_vars if var in df.columns)))\n",
                "\n",
                "# --- Instantiate the model ---\n",
                "model = MaskedLSTM(\n",
                "    input_size=len(exogenous_vars),\n",
                "    hidden_size=best_params[\"hidden_size\"],\n",
                "    num_layers=best_params[\"num_layers\"],\n",
                "    dense_size=best_params[\"dense_size\"],\n",
                "    output_size=len(target_vars),\n",
                "    dropout=best_params[\"dropout\"],\n",
                "    dropout_lstm=best_params[\"dropout_lstm\"],\n",
                "    bidirectional=best_params[\"bidirectional\"],\n",
                ").to(DEVICE)\n",
                "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
                "model.eval()\n",
                "\n",
                "# --- Robust window selection: will always work for any seq_length and date range ---\n",
                "\n",
                "# desired_start = pd.Timestamp(\"2023-09-05\", tz=\"UTC\")\n",
                "# desired_end = pd.Timestamp(\"2023-09-08\", tz=\"UTC\")\n",
                "\n",
                "desired_start = pd.Timestamp(\"2025-02-04\", tz=\"UTC\")\n",
                "desired_end = pd.Timestamp(\"2025-02-20\", tz=\"UTC\")\n",
                "\n",
                "\n",
                "all_dates = df.index\n",
                "try:\n",
                "    first_pred_idx = all_dates.get_loc(desired_start)\n",
                "except KeyError:\n",
                "    raise ValueError(f\"Desired start {desired_start} not in DataFrame index!\")\n",
                "\n",
                "if first_pred_idx < seq_length:\n",
                "    raise ValueError(f\"Not enough data before {desired_start} to build input sequence with seq_length={seq_length}.\")\n",
                "\n",
                "window_start = all_dates[first_pred_idx - seq_length]\n",
                "window_end = desired_end\n",
                "\n",
                "df_pred_window = df.loc[window_start:window_end].copy()\n",
                "\n",
                "# -- Scale features --\n",
                "X_scaled = scaler_X.transform(df_pred_window[exogenous_vars])\n",
                "y_true = df_pred_window[target_vars].values\n",
                "\n",
                "# -- Build sequences --\n",
                "def create_sequences(X, lookback, horizon=1):\n",
                "    num_samples = X.shape[0] - lookback - horizon + 1\n",
                "    X_seq = np.zeros((num_samples, lookback, X.shape[1]), dtype=np.float32)\n",
                "    mask_seq = np.ones((num_samples, lookback, X.shape[1]), dtype=np.float32)\n",
                "    for i in range(num_samples):\n",
                "        X_seq[i] = X[i : i + lookback]\n",
                "    return X_seq, mask_seq\n",
                "\n",
                "X_seq, mask_seq = create_sequences(X_scaled, seq_length, 1)\n",
                "nan_pos = np.isnan(X_seq)\n",
                "mask_seq[nan_pos] = 0.0\n",
                "X_seq[nan_pos] = 0.0\n",
                "\n",
                "dates_seq = df_pred_window.index[seq_length : seq_length + len(X_seq)]\n",
                "y_true_aligned = y_true[seq_length : seq_length + len(X_seq)]\n",
                "\n",
                "# -- Filter for predictions within the requested window only --\n",
                "mask = (dates_seq >= desired_start) & (dates_seq <= desired_end)\n",
                "dates_seq = dates_seq[mask]\n",
                "y_true_aligned = y_true_aligned[mask]\n",
                "\n",
                "with torch.no_grad():\n",
                "    X_tensor = torch.tensor(X_seq, dtype=torch.float32).to(DEVICE)\n",
                "    mask_tensor = torch.tensor(mask_seq, dtype=torch.float32).to(DEVICE)\n",
                "    y_pred_scaled = model(X_tensor, mask_tensor).cpu().numpy()[:, 0, :]  # (n, target_dim)\n",
                "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
                "y_pred = y_pred[mask]\n",
                "\n",
                "# --- Add price data ---\n",
                "price_data_to_plot = None\n",
                "if price_data_full is not None and price_column_name_to_plot in price_data_full.columns:\n",
                "    try:\n",
                "        # Align price data with dates_seq\n",
                "        price_data_to_plot = price_data_full.loc[dates_seq, price_column_name_to_plot].values.astype(float)\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not align price data with dates_seq: {e}\")\n",
                "        price_data_to_plot = None\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.dates as mdates\n",
                "\n",
                "# --- Colors and markers ---\n",
                "true_color = \"#636efa\"    # Plotly default blue\n",
                "pred_color = \"#ef553b\"    # Plotly default red\n",
                "price_color = \"orange\"\n",
                "\n",
                "# --- Matplotlib plot ---\n",
                "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
                "\n",
                "# True values\n",
                "ax1.plot(dates_seq, y_true_aligned[:, 0], label='True', color=true_color, marker='o', markersize=5, linewidth=2)\n",
                "\n",
                "# Predicted values\n",
                "ax1.plot(dates_seq, y_pred[:, 0], label='Predicted', color=pred_color, marker='x', markersize=5, linewidth=2)\n",
                "\n",
                "ax1.set_xlabel(\"Date\", fontsize=13)\n",
                "ax1.set_ylabel(\"Volume (MW)\", fontsize=13, color=true_color)\n",
                "ax1.tick_params(axis='y', labelcolor=true_color)\n",
                "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%d %b %Y\\n%H:%M'))\n",
                "plt.xticks(rotation=45)\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Add price on secondary y-axis\n",
                "if price_data_to_plot is not None:\n",
                "    ax2 = ax1.twinx()\n",
                "    ax2.plot(dates_seq, price_data_to_plot, label='mFRR Down Price', color=price_color, linewidth=2)\n",
                "    ax2.set_ylabel(\"mFRR Down Price (€/MWh)\", fontsize=13, color=price_color)\n",
                "    ax2.tick_params(axis='y', labelcolor=price_color)\n",
                "    ax2.grid(False)\n",
                "else:\n",
                "    ax2 = None\n",
                "\n",
                "# Title and legend\n",
                "fig.suptitle(\n",
                "    f\"True vs Predicted for mFRR Down Volume\",\n",
                "    fontsize=15, fontweight='normal', y=1.0\n",
                ")\n",
                "\n",
                "# Combined legend\n",
                "lines_labels = [ax.get_legend_handles_labels() for ax in [ax1, ax2] if ax is not None]\n",
                "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
                "plt.legend(lines, labels, loc='lower center', bbox_to_anchor=(0.5, -0.4), ncol=3, fontsize=12, frameon=False)\n",
                "\n",
                "# --- Add analysis period annotation (NEW) ---\n",
                "period_text = (\n",
                "    f\"Validation set: {desired_start.strftime('%Y-%m-%d %H:%M')} to \"\n",
                "    f\"{desired_end.strftime('%Y-%m-%d %H:%M')} UTC\"\n",
                ")\n",
                "fig.text(0.5, 0.94, period_text, ha='center', fontstyle='italic', fontsize=11)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "df_read = pd.read_csv(\"/processed_data/results/resultsset_v2.csv\")\n",
                "df_read.columns = df_read.columns.str.replace(' ', '', regex=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- CONFIG ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"/master_data/master_data_merged.csv\"\n",
    "REGION = \"SE2\"\n",
    "target_vars = [f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownActivatedVolume\"]\n",
    "\n",
    "USE_OPTUNA = True          \n",
    "N_TRIALS = 50              # Number of Optuna trials INCREASE IN COLLAB\n",
    "MAX_EPOCHS_OPTUNA = 10     # Epochs *per trial* INCREASE IN COLLAB\n",
    "PATIENCE_OPTUNA = 5        # Early‑stopping within a trial\n",
    "EPOCHS_FINAL_TRAINING = 100\n",
    "PATIENCE_FINAL_TRAINING = 10\n",
    "RANDOM_SEED = 69\n",
    "\n",
    "# Flags – kept `False` for system parameters only\n",
    "CREATE_INDICATOR_FEATURES = False\n",
    "CREATE_CYCLICAL_FEATURES = False\n",
    "CREATE_LAGGED_FEATURES = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- DEVICE ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- PERSISTENCE -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, joblib, platform, sys, getpass, socket, datetime\n",
    "\n",
    "def save_best_params(best_params: dict, fname: str = \"best_params.json\"):\n",
    "    \"\"\"Augment hyper-params with reproducibility metadata and write to JSON.\"\"\"\n",
    "    meta = {\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"device\": str(DEVICE),\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"user\": getpass.getuser(),\n",
    "        \"timestamp_utc\": datetime.datetime.now(datetime.timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"deterministic_cudnn\": True, \n",
    "        \"benchmark_cudnn\": False, \n",
    "    }\n",
    "    with open(fname, \"w\") as f:\n",
    "        json.dump({**best_params, **meta}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- REPRODUCIBILITY ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark     = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- PRE-DEFINED PLOTS ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- NEW PLOTTING FUNCTIONS -------------------------\n",
    "import plotly.express as px\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "# Functions from the provided plots.py\n",
    "# ------------------------------------------------------------\n",
    "def plot_loss_curve(train_history_list, val_history_list): # Renamed to avoid conflict if variables are named the same\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=train_history_list, name=\"Train\"))\n",
    "    fig.add_trace(go.Scatter(y=val_history_list,   name=\"Val\"))\n",
    "    fig.update_layout(title=\"Training vs Validation Loss\",\n",
    "                      xaxis_title=\"Epoch\", yaxis_title=\"Loss\",\n",
    "                      template=\"plotly_white\")\n",
    "    fig.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "def plot_optuna_history(study_obj: optuna.study.Study): # Renamed to avoid conflict\n",
    "    fig = optuna.visualization.plot_optimization_history(study_obj)\n",
    "    fig.update_layout(title=\"Optuna Optimisation History\")\n",
    "    fig.show()\n",
    "\n",
    "def plot_optuna_importance(study_obj: optuna.study.Study): # Renamed to avoid conflict\n",
    "    fig = optuna.visualization.plot_param_importances(study_obj)\n",
    "    fig.update_layout(title=\"Hyper-parameter Importance (Optuna)\")\n",
    "    fig.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "def scatter_true_vs_pred(y_true, y_pred, title_suffix=\"\"):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=y_true, y=y_pred,\n",
    "                             mode=\"markers\", opacity=0.4, name=\"Samples\"))\n",
    "    min_v, max_v = float(np.min(y_true)), float(np.max(y_true)) # Ensure y_true is not empty\n",
    "    fig.add_shape(type=\"line\", x0=min_v, y0=min_v, x1=max_v, y1=max_v,\n",
    "                  line=dict(dash=\"dash\", color=\"grey\"))\n",
    "    fig.update_layout(title=f\"Scatter True vs Pred{title_suffix}\",\n",
    "                      xaxis_title=\"True\", yaxis_title=\"Predicted\",\n",
    "                      template=\"plotly_white\")\n",
    "    fig.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "def residuals_vs_fitted(y_pred, residuals, title_suffix=\"\"):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=y_pred, y=residuals,\n",
    "                             mode=\"markers\", opacity=0.4, name=\"Residuals\"))\n",
    "    fig.update_layout(title=f\"Residuals vs Fitted{title_suffix}\",\n",
    "                      xaxis_title=\"Predicted\",\n",
    "                      yaxis_title=\"Residual (True − Pred)\",\n",
    "                      template=\"plotly_white\",\n",
    "                      shapes=[dict(type=\"line\", x0=min(y_pred) if len(y_pred) > 0 else 0, \n",
    "                                   x1=max(y_pred) if len(y_pred) > 0 else 0,\n",
    "                                   y0=0, y1=0, line=dict(dash=\"dash\", color=\"grey\"))])\n",
    "    fig.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "def plot_acf_residuals(residuals, lags=48, title_suffix=\"\"):\n",
    "    # Uses matplotlib – quick static figure for appendix\n",
    "    if len(residuals) > lags and len(residuals) > 0 : # Check if there's enough data\n",
    "        plt.figure(figsize=(6,3))\n",
    "        plot_acf(residuals, lags=lags)\n",
    "        plt.title(f\"ACF of Residuals{title_suffix}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Not enough data to plot ACF of residuals{title_suffix}. Need > {lags} data points, got {len(residuals)}.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "def precision_recall_curve_plot(y_true_bool, y_score, title=\"PR curve (spike detection)\"): # Renamed y_true to y_true_bool\n",
    "    \"\"\"Assumes y_true_bool is boolean spike indicator; y_score = |error| or model score.\"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve, auc # Local import for clarity\n",
    "    precision, recall, thresh = precision_recall_curve(y_true_bool, y_score)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=recall, y=precision, mode=\"lines\",\n",
    "                             name=f\"PR curve (AUC={pr_auc:.3f})\"))\n",
    "    fig.update_layout(title=title, xaxis_title=\"Recall\", yaxis_title=\"Precision\",\n",
    "                      template=\"plotly_white\")\n",
    "    fig.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "def rmse_bar(models: dict, title=\"RMSE Comparison\"):\n",
    "    \"\"\"\n",
    "    models = {\"LSTM\": rmse1, \"Naive\": rmse2, ...}\n",
    "    \"\"\"\n",
    "    fig = px.bar(x=list(models.keys()), y=list(models.values()),\n",
    "                 labels={\"x\":\"Model\", \"y\":\"RMSE\"},\n",
    "                 title=title)\n",
    "    fig.update_layout(template=\"plotly_white\")\n",
    "    fig.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "def radar_multi_metric(metric_dict: dict, title=\"Radar Comparison\"):\n",
    "    \"\"\"\n",
    "    metric_dict:\n",
    "        {\"LSTM\": {\"RMSE\":0.1,\"MAE\":0.2,\"R2\":0.9},\n",
    "         \"Naive\":{...}, ...}\n",
    "    \"\"\"\n",
    "    metrics = list(next(iter(metric_dict.values())).keys()) if metric_dict else []\n",
    "    if not metrics:\n",
    "        print(\"Metric dictionary is empty or malformed for radar plot.\")\n",
    "        return\n",
    "    fig = go.Figure()\n",
    "    for name, vals in metric_dict.items():\n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=[vals.get(m, np.nan) for m in metrics], # Use .get for safety\n",
    "            theta=metrics,\n",
    "            fill=\"toself\",\n",
    "            name=name))\n",
    "    fig.update_layout(title=title,\n",
    "                      polar=dict(radialaxis=dict(visible=True)),\n",
    "                      template=\"plotly_white\",\n",
    "                      showlegend=True)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- DATA LOADING ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data …\")\n",
    "df_all = pd.read_csv(FILE_PATH)\n",
    "\n",
    "price_column_name_to_plot = f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownPrice\"\n",
    "\n",
    "# Store price data separately before modifying df_all or creating df\n",
    "price_data_full = None\n",
    "if price_column_name_to_plot in df_all.columns:\n",
    "    price_data_full = df_all[['DateTime', price_column_name_to_plot]].copy()\n",
    "    price_data_full['DateTime'] = pd.to_datetime(price_data_full['DateTime'], utc=True)\n",
    "    price_data_full.set_index('DateTime', inplace=True)\n",
    "    print(f\"Stored '{price_column_name_to_plot}' separately for plotting.\")\n",
    "else:\n",
    "    print(f\"Warning: Price column '{price_column_name_to_plot}' not found in df_all. Will not be plotted.\")\n",
    "\n",
    "# Bidding zone filter\n",
    "from filter_features import pick_region_filter\n",
    "BDZ_filter = pick_region_filter(region=REGION, remove_balancing=True)\n",
    "\n",
    "drop_list = [\n",
    "    f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedUpPrice\",\n",
    "    f\"Balancing_PricesOfActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownPrice\",\n",
    "    f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedUpActivatedVolume\",\n",
    "    f\"Balancing_ActivatedBalancingEnergy_{REGION}_mFRR_NotSpecifiedDownActivatedVolume\",\n",
    "]\n",
    "BDZ_filter = [c for c in BDZ_filter if c not in drop_list or c in target_vars]\n",
    "\n",
    "df = df_all[[\"DateTime\"] + BDZ_filter].copy()\n",
    "df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"], utc=True)\n",
    "df.set_index(\"DateTime\", inplace=True)\n",
    "df = df.asfreq(\"h\")\n",
    "\n",
    "\n",
    "# Feature Engineering Logic\n",
    "new_indicator_features = []\n",
    "if CREATE_INDICATOR_FEATURES:\n",
    "    # Feature Engineering: Add indicator for missing values (before other feature engineering)\n",
    "    # These indicators capture missingness introduced by asfreq or originally present\n",
    "    indicator_features_to_create = [col for col in BDZ_filter + target_vars if col in df.columns]\n",
    "    for col in indicator_features_to_create:\n",
    "        df[f'{col}_was_missing'] = df[col].isnull().astype(int)\n",
    "    new_indicator_features = [f'{col}_was_missing' for col in indicator_features_to_create]\n",
    "\n",
    "new_time_features = []\n",
    "if CREATE_CYCLICAL_FEATURES:\n",
    "    # Feature Engineering: Add cyclical time features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24.0)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24.0)\n",
    "    df['dayofweek_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7.0)\n",
    "    df['dayofweek_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7.0)\n",
    "    df['weekofyear_sin'] = np.sin(2 * np.pi * df.index.isocalendar().week / 52.0) # Using isocalendar().week\n",
    "    df['weekofyear_cos'] = np.cos(2 * np.pi * df.index.isocalendar().week / 52.0)\n",
    "    new_time_features = ['hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'weekofyear_sin', 'weekofyear_cos']\n",
    "\n",
    "new_lagged_features = []\n",
    "if CREATE_LAGGED_FEATURES:\n",
    "    # Feature Engineering: Add lagged target features\n",
    "    # Ensure target_vars is defined and contains the column names to lag\n",
    "    for target_var in target_vars: # Iterate if multiple targets, though current setup has one\n",
    "        df[f'{target_var}_lag1'] = df[target_var].shift(1)\n",
    "        df[f'{target_var}_lag24'] = df[target_var].shift(24)\n",
    "        df[f'{target_var}_lag168'] = df[target_var].shift(168)\n",
    "    for target_var in target_vars:\n",
    "        new_lagged_features.extend([f'{target_var}_lag1', f'{target_var}_lag24', f'{target_var}_lag168'])\n",
    "\n",
    "df.dropna(axis=1, how=\"all\", inplace=True)  # Drop columns with all NaNs\n",
    "\n",
    "exogenous_vars = [c for c in BDZ_filter if c not in target_vars]\n",
    "\n",
    "# Build the full list of exogenous variables based on flags\n",
    "if CREATE_CYCLICAL_FEATURES:\n",
    "    exogenous_vars.extend(new_time_features)\n",
    "if CREATE_LAGGED_FEATURES:\n",
    "    exogenous_vars.extend(new_lagged_features)\n",
    "if CREATE_INDICATOR_FEATURES:\n",
    "    exogenous_vars.extend(new_indicator_features)\n",
    "\n",
    "# Ensure all exogenous_vars are present in df.columns and are unique\n",
    "exogenous_vars = sorted(list(set(var for var in exogenous_vars if var in df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- SPLIT BY DATE ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.loc[:\"2023-12-31\"].copy()\n",
    "df_val   = df.loc[\"2024-01-01\":\"2025-01-10\"].copy()\n",
    "df_val2  = df.loc[\"2024-01-11\":\"2025-01-31\"].copy() # New validation set for final training early stopping\n",
    "df_test  = df.loc[\"2025-02-01\":].copy()\n",
    "\n",
    "print(\n",
    "    f\"Train {df_train.shape}, Val {df_val.shape}, Val2 {df_val2.shape}, Test {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- SCALERS ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = RobustScaler(quantile_range=(5.0, 95.0))\n",
    "scaler_y = RobustScaler(quantile_range=(5.0, 95.0))\n",
    "scaler_X.fit(df_train[exogenous_vars])\n",
    "scaler_y.fit(df_train[target_vars])\n",
    "\n",
    "# Helper to scale a split -----------------------------------------------------\n",
    "\n",
    "def scale_split(split_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X_scaled = scaler_X.transform(split_df[exogenous_vars])\n",
    "    y_scaled = scaler_y.transform(split_df[target_vars])\n",
    "    return pd.DataFrame(\n",
    "        np.hstack([X_scaled, y_scaled]),\n",
    "        index=split_df.index,\n",
    "        columns=exogenous_vars + target_vars,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- SEQUENCE BUILDER ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_torch(data_df: pd.DataFrame, target_idx, feature_idx, lookback: int, horizon: int = 1):\n",
    "    data_tensor = torch.tensor(data_df.values, dtype=torch.float32)\n",
    "    num_samples = data_tensor.shape[0] - lookback - horizon\n",
    "    X = torch.zeros((num_samples, lookback, len(feature_idx)), dtype=torch.float32)\n",
    "    y = torch.zeros((num_samples, horizon, len(target_idx)), dtype=torch.float32)\n",
    "    for i in range(num_samples):\n",
    "        X[i] = data_tensor[i : i + lookback, feature_idx]\n",
    "        y[i] = data_tensor[i + lookback : i + lookback + horizon, target_idx]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- MODEL & LOSS ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaskedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=1, dense_size=64, output_size=1, horizon=1, dropout=0.2, dropout_lstm=0.2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.output_size = output_size\n",
    "        actual_in = input_size * 2\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=actual_in,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_lstm if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        hidden_out = hidden_size * (2 if bidirectional else 1)\n",
    "        self.fc1 = nn.Linear(hidden_out, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size, horizon * output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.drop_lstm = nn.Dropout(dropout_lstm)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = torch.cat((x, mask), dim=2)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.drop_lstm(out)\n",
    "        out = self.relu(self.fc1(out[:, -1, :]))\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        return out.view(out.size(0), self.horizon, self.output_size)\n",
    "\n",
    "\n",
    "# def masked_mse_loss(y_pred, y_true, mask, alpha: float = 1.0):\n",
    "#     se = (y_pred - y_true) ** 2\n",
    "#     weights = 1.0 + alpha * torch.abs(y_true.detach())\n",
    "#     loss = se * weights * mask\n",
    "#     valid = mask.sum()\n",
    "#     return loss.sum() / valid if valid > 0 else torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
    "\n",
    "\n",
    "# def masked_mse_loss(y_pred, y_true_scaled, mask, alpha=1.0):\n",
    "#     scale  = torch.tensor(scaler_y.scale_,  device=y_pred.device,\n",
    "#                           dtype=y_pred.dtype).view(1, 1, -1)\n",
    "#     center = torch.tensor(scaler_y.center_, device=y_pred.device,\n",
    "#                           dtype=y_pred.dtype).view(1, 1, -1)\n",
    "\n",
    "#     y_true_unscaled = y_true_scaled * scale + center          # (B,H,D)\n",
    "#     weights = 1.0 + alpha * torch.abs(y_true_unscaled).detach()\n",
    "\n",
    "#     se   = (y_pred - y_true_scaled) ** 2\n",
    "#     loss = (se * weights * mask).sum() / mask.sum()\n",
    "#     return loss\n",
    "\n",
    "scale_t  = torch.tensor(scaler_y.scale_[0],  device=DEVICE, dtype=torch.float32)\n",
    "center_t = torch.tensor(scaler_y.center_[0], device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "def weighted_mse_loss(y_pred, y_true_scaled, mask, alpha=10.0):\n",
    "    \"\"\"\n",
    "    y shapes  (B, 1)  because you still have one target\n",
    "    mask same shape, 1 for valid, 0 for padded\n",
    "    \"\"\"\n",
    "    y_true_unscaled = y_true_scaled * scale_t + center_t\n",
    "    weights = 1.0 + alpha * torch.abs(y_true_unscaled)\n",
    "    se      = (y_pred - y_true_scaled) ** 2\n",
    "    loss    = (se * weights * mask).sum() / mask.sum()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- OPTUNA OBJECTIVE ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optuna_objective(trial: optuna.Trial) -> float:\n",
    "    # Sample hyper‑params\n",
    "    params = {\n",
    "        \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256]),\n",
    "        \"num_layers\": trial.suggest_int(\"num_layers\", 1, 3),\n",
    "        \"bidirectional\": trial.suggest_categorical(\"bidirectional\", [False, True]),\n",
    "        \"dense_size\": trial.suggest_categorical(\"dense_size\", [32, 64, 128]),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.5),\n",
    "        \"dropout_lstm\": trial.suggest_float(\"dropout_lstm\", 0.0, 0.5),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 5e-3, log=True),\n",
    "        # \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512]),\n",
    "        \"seq_length\": trial.suggest_categorical(\"seq_length\", choices=[24, 72, 168]),\n",
    "    }\n",
    "\n",
    "    # Build scaled splits each trial to allow varying seq_length\n",
    "    train_scaled = scale_split(df_train)\n",
    "    val_scaled   = scale_split(df_val)\n",
    "\n",
    "    target_idx  = [train_scaled.columns.get_loc(c) for c in target_vars]\n",
    "    feature_idx = [train_scaled.columns.get_loc(c) for c in exogenous_vars]\n",
    "\n",
    "    X_train, y_train = create_sequences_torch(train_scaled, target_idx, feature_idx, params[\"seq_length\"], 1)\n",
    "    X_val, y_val     = create_sequences_torch(val_scaled,   target_idx, feature_idx, params[\"seq_length\"], 1)\n",
    "\n",
    "    # Masks & NaN replacement (there shouldn't be NaNs after fillna, but keep generic)\n",
    "    X_mask_train = (~torch.isnan(X_train)).float(); X_train = torch.nan_to_num(X_train)\n",
    "    y_mask_train = (~torch.isnan(y_train)).float(); y_train = torch.nan_to_num(y_train)\n",
    "    X_mask_val   = (~torch.isnan(X_val)).float();   X_val   = torch.nan_to_num(X_val)\n",
    "    y_mask_val   = (~torch.isnan(y_val)).float();   y_val   = torch.nan_to_num(y_val)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, X_mask_train, y_train, y_mask_train), batch_size=params[\"batch_size\"], shuffle=False)\n",
    "    val_loader   = DataLoader(TensorDataset(X_val,   X_mask_val,   y_val,   y_mask_val),   batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    model = MaskedLSTM(\n",
    "        input_size=len(feature_idx),\n",
    "        hidden_size=params[\"hidden_size\"],\n",
    "        num_layers=params[\"num_layers\"],\n",
    "        dense_size=params[\"dense_size\"],\n",
    "        output_size=len(target_vars),\n",
    "        dropout=params[\"dropout\"],\n",
    "        dropout_lstm=params[\"dropout_lstm\"],\n",
    "        bidirectional=params[\"bidirectional\"],\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimiser = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode=\"min\", patience=2, factor=0.5)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS_OPTUNA):\n",
    "        # ---- train ----\n",
    "        model.train(); train_loss = 0.0\n",
    "        for Xb, Xm, yb, ym in train_loader:\n",
    "            Xb, Xm, yb, ym = Xb.to(DEVICE), Xm.to(DEVICE), yb.to(DEVICE), ym.to(DEVICE)\n",
    "            optimiser.zero_grad()\n",
    "            y_pred = model(Xb, Xm)\n",
    "            \n",
    "            # loss = masked_mse_loss(y_pred, yb, ym)\n",
    "            loss = weighted_mse_loss(y_pred, yb, ym)\n",
    "            \n",
    "            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimiser.step(); train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ---- val ----\n",
    "        model.eval(); val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for Xb, Xm, yb, ym in val_loader:\n",
    "                Xb, Xm, yb, ym = Xb.to(DEVICE), Xm.to(DEVICE), yb.to(DEVICE), ym.to(DEVICE)\n",
    "                y_pred = model(Xb, Xm)\n",
    "\n",
    "                # val_loss += masked_mse_loss(y_pred, yb, ym).item()\n",
    "                val_loss += weighted_mse_loss(y_pred, yb, ym).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss; patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= PATIENCE_OPTUNA:\n",
    "                break\n",
    "\n",
    "    return best_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- HYPER‑PARAM SELECTION  ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_OPTUNA:\n",
    "    print(\"\\n=== Running Optuna study ===\")\n",
    "    sampler = TPESampler(seed=RANDOM_SEED)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(optuna_objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "    best_params = study.best_trial.params\n",
    "    print(\"Best params:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    save_best_params(best_params)\n",
    "    joblib.dump(study, \"study.pkl\")\n",
    "\n",
    "    # Plot Optuna optimization history\n",
    "    fig_opt_hist = optuna.visualization.plot_optimization_history(study)\n",
    "    fig_opt_hist.show()\n",
    "\n",
    "    # The existing call optuna.visualization.plot_optimization_history(study).show() is fine.\n",
    "    # The new plot_optuna_history(study) can be used if preferred or if the layout modification is desired.\n",
    "    # For now, let's add the new hyper-parameter importance plot call:\n",
    "    plot_optuna_importance(study) # Call the new function for hyper-parameter importance\n",
    "else:\n",
    "    # Manual defaults from your notebook\n",
    "    best_params = {\n",
    "        \"hidden_size\": 256,\n",
    "        \"num_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"dense_size\": 128,\n",
    "        \"dropout\": 0.33194731105235564,\n",
    "        \"dropout_lstm\": 0.3447348446997174,\n",
    "        \"learning_rate\": 0.0006857135933869154,\n",
    "        \"batch_size\": 256,\n",
    "        \"seq_length\": 36,\n",
    "    }\n",
    "    save_best_params(best_params) # Save even if not using Optuna, for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- DATA REBUILD WITH BEST PARAMS ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRe‑building datasets with best hyper‑parameters …\")\n",
    "# Train on train + original val, validate final model on val2\n",
    "train_val_combined_df = pd.concat([df_train, df_val])\n",
    "train_val_scaled = scale_split(train_val_combined_df) # Scale the combined train+val\n",
    "val2_scaled = scale_split(df_val2) # Scale val2\n",
    "\n",
    "target_idx  = [train_val_scaled.columns.get_loc(c) for c in target_vars]\n",
    "feature_idx = [train_val_scaled.columns.get_loc(c) for c in exogenous_vars]\n",
    "\n",
    "X_tv, y_tv = create_sequences_torch(train_val_scaled, target_idx, feature_idx, best_params[\"seq_length\"], 1)\n",
    "X_val2_tensor, y_val2_tensor = create_sequences_torch(val2_scaled, target_idx, feature_idx, best_params[\"seq_length\"], 1) # Sequences for val2\n",
    "X_test_scaled = scale_split(df_test)\n",
    "X_test_tensor, y_test_tensor = create_sequences_torch(X_test_scaled, target_idx, feature_idx, best_params[\"seq_length\"], 1)\n",
    "\n",
    "# Masks & NaNs\n",
    "X_mask_tv     = (~torch.isnan(X_tv)).float();     X_tv     = torch.nan_to_num(X_tv)\n",
    "y_mask_tv     = (~torch.isnan(y_tv)).float();     y_tv     = torch.nan_to_num(y_tv)\n",
    "X_mask_val2   = (~torch.isnan(X_val2_tensor)).float(); X_val2_tensor = torch.nan_to_num(X_val2_tensor) # Mask for val2\n",
    "y_mask_val2   = (~torch.isnan(y_val2_tensor)).float(); y_val2_tensor = torch.nan_to_num(y_val2_tensor) # Mask for val2\n",
    "X_mask_test   = (~torch.isnan(X_test_tensor)).float(); X_test_tensor = torch.nan_to_num(X_test_tensor)\n",
    "y_mask_test   = (~torch.isnan(y_test_tensor)).float(); y_test_tensor = torch.nan_to_num(y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_tv, X_mask_tv, y_tv, y_mask_tv),\n",
    "                          batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "val2_loader = DataLoader(TensorDataset(X_val2_tensor, X_mask_val2, y_val2_tensor, y_mask_val2), # Loader for val2\n",
    "                         batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, X_mask_test, y_test_tensor, y_mask_test),\n",
    "                         batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- FINAL MODEL TRAIN ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nTraining final model …\")\n",
    "model = MaskedLSTM(\n",
    "    input_size=len(feature_idx),\n",
    "    hidden_size=best_params[\"hidden_size\"],\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    dense_size=best_params[\"dense_size\"],\n",
    "    output_size=len(target_vars),\n",
    "    dropout=best_params[\"dropout\"],\n",
    "    dropout_lstm=best_params[\"dropout_lstm\"],\n",
    "    bidirectional=best_params[\"bidirectional\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "# Training parameters\n",
    "epochs = EPOCHS_FINAL_TRAINING\n",
    "patience_early_stop = PATIENCE_FINAL_TRAINING\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_model_path = 'best_model.pth'\n",
    "scaler_X_path = 'scaler_X.joblib'\n",
    "scaler_y_path = 'scaler_y.joblib'\n",
    "loss_alpha = 1.0  # Alpha for weighted MSE\n",
    "\n",
    "train_history, val_history = [], []        \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, X_mask_batch, y_batch, y_mask_batch in train_loader:\n",
    "        X_batch, X_mask_batch, y_batch, y_mask_batch = (\n",
    "            X_batch.to(DEVICE),\n",
    "            X_mask_batch.to(DEVICE),\n",
    "            y_batch.to(DEVICE),\n",
    "            y_mask_batch.to(DEVICE),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch, X_mask_batch)\n",
    "\n",
    "        # loss = masked_mse_loss(y_pred, y_batch, y_mask_batch, alpha=loss_alpha)\n",
    "        loss = weighted_mse_loss(y_pred, y_batch, y_mask_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation - use val2_loader for early stopping and scheduler\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, X_mask_batch, y_batch, y_mask_batch in val2_loader: # Use val2_loader\n",
    "            X_batch, X_mask_batch, y_batch, y_mask_batch = (\n",
    "                X_batch.to(DEVICE),\n",
    "                X_mask_batch.to(DEVICE),\n",
    "                y_batch.to(DEVICE),\n",
    "                y_mask_batch.to(DEVICE),\n",
    "            )\n",
    "            y_pred = model(X_batch, X_mask_batch)\n",
    "            \n",
    "            # val_loss += masked_mse_loss(y_pred, y_batch, y_mask_batch, alpha=loss_alpha).item()\n",
    "            val_loss += weighted_mse_loss(y_pred, y_batch, y_mask_batch).item()\n",
    "\n",
    "\n",
    "    avg_val_loss = val_loss / len(val2_loader)\n",
    "\n",
    "    # Print the losses for this epoch\n",
    "    print(f\"Epoch {epoch+1:02d}: Train Loss = {avg_train_loss:.10f}, Val Loss = {avg_val_loss:.10f}\")\n",
    "\n",
    "    train_history.append(avg_train_loss)     # \n",
    "    val_history.append(avg_val_loss)         # \n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # ----- Early Stopping Check -----\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        joblib.dump(scaler_X, scaler_X_path)\n",
    "        joblib.dump(scaler_y, scaler_y_path)\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"New best model saved with validation loss: {best_val_loss:.10f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience_early_stop:\n",
    "            print(f\"Early stopping triggered. No improvement in validation loss for {patience_early_stop} consecutive epochs.\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Persist the curves – for convergence plots\n",
    "pd.DataFrame({\"train_loss\": train_history,\n",
    "              \"val_loss\":   val_history}).to_csv(\"loss_history.csv\", index_label=\"epoch\")\n",
    "\n",
    "# Call the new loss curve plot function\n",
    "plot_loss_curve(train_history, val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- PREDICTION & EVALUATION ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Generate predictions for test set\n",
    "all_preds_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, X_mask_batch, _, _ in test_loader:\n",
    "        # Move data to the same device as the model\n",
    "        X_batch = X_batch.to(DEVICE)\n",
    "        X_mask_batch = X_mask_batch.to(DEVICE)\n",
    "        \n",
    "        # Generate predictions\n",
    "        y_pred = model(X_batch, X_mask_batch)\n",
    "        \n",
    "        # Append predictions (move to CPU and convert to numpy)\n",
    "        all_preds_list.append(y_pred.cpu().numpy())\n",
    "\n",
    "# Concatenate batches along the first axis (samples)\n",
    "all_preds_array = np.concatenate(all_preds_list, axis=0)  # Shape: (num_samples, horizon, num_model_outputs)\n",
    "\n",
    "# For plotting, let's select the first time step prediction for each sequence\n",
    "model_predictions_raw = all_preds_array[:, 0, :]  # Shape: (num_samples, num_model_outputs)\n",
    "\n",
    "# Invert the scaling on predictions if a scaler was used for y\n",
    "model_predictions_unscaled = scaler_y.inverse_transform(model_predictions_raw)\n",
    "\n",
    "# Get the datetime indices for the predictions\n",
    "# The test index starts at seq_length timesteps after the start of the test set\n",
    "test_dates = df_test.index[best_params[\"seq_length\"]:best_params[\"seq_length\"]+len(all_preds_array)]\n",
    "\n",
    "# Get the actual values directly from the original dataframe for the TARGET_VARS\n",
    "true_values_from_df_test = df_test.loc[test_dates, target_vars].values\n",
    "\n",
    "# Ensure true_series_scaled is 2D, especially if target_vars has only one element\n",
    "if true_values_from_df_test.ndim == 1:\n",
    "    true_values_from_df_test = true_values_from_df_test.reshape(-1, 1)\n",
    "\n",
    "# Now, inverse transform the SCALED true values to get UN SCALED true values\n",
    "# CORRECTED: true_values_from_df_test already contains unscaled values from df_test.\n",
    "# No inverse_transform needed here.\n",
    "true_series_unscaled = true_values_from_df_test\n",
    "\n",
    "# Ensure no nans in the final unscaled true values for plotting/metrics\n",
    "true_series_unscaled = np.nan_to_num(true_series_unscaled, nan=0.0)\n",
    "\n",
    "# These are the names corresponding to ALL of the model's output dimensions, in their output order.\n",
    "model_output_all_target_names = target_vars\n",
    "\n",
    "# Dump predictions vs. ground truth\n",
    "pred_df  = pd.DataFrame(model_predictions_unscaled,\n",
    "                        index=test_dates,\n",
    "                        columns=[f\"pred_{c}\" for c in target_vars])\n",
    "truth_df = pd.DataFrame(true_series_unscaled,\n",
    "                        index=test_dates,\n",
    "                        columns=[f\"true_{c}\" for c in target_vars])\n",
    "\n",
    "pd.concat([pred_df, truth_df], axis=1).to_parquet(\"predictions.parquet\")\n",
    "print(\"Saved test-set predictions → predictions.parquet\")\n",
    "\n",
    "# --------------------------- PLOTTING & METRICS ----------------------------\n",
    "print(\"\\nEvaluating and plotting results...\")\n",
    "\n",
    "# Now, plot the predictions vs. true values for each target variable specified in `target_vars`\n",
    "print(f\"Evaluating targets: {target_vars}\")\n",
    "\n",
    "for i, current_target_name_to_eval in enumerate(target_vars):\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # True values: column i from true_series_unscaled corresponds to target_vars[i]\n",
    "    current_true_values = true_series_unscaled[:, i]\n",
    "    \n",
    "    # Predicted values: find the column in model_predictions_unscaled that corresponds to current_target_name_to_eval\n",
    "    try:\n",
    "        # Find the index of the current evaluation target in the list of all model output names\n",
    "        model_output_idx = model_output_all_target_names.index(current_target_name_to_eval)\n",
    "        current_predicted_values = model_predictions_unscaled[:, model_output_idx]\n",
    "    except ValueError:\n",
    "        print(f\"Error: The target '{current_target_name_to_eval}' (from your `target_vars`) \"\n",
    "              f\"was not found in `model_output_all_target_names`: {model_output_all_target_names}. \"\n",
    "              \"Cannot plot or calculate metrics for this target. Please check your variable names.\")\n",
    "        continue # Skip to the next target in target_vars\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test_dates,\n",
    "        y=current_true_values,\n",
    "        mode='lines+markers',\n",
    "        name=\"True\",\n",
    "        marker=dict(symbol='circle', size=6)\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test_dates,\n",
    "        y=current_predicted_values,\n",
    "        mode='lines+markers',\n",
    "        name=\"Predicted\",\n",
    "        marker=dict(symbol='x', size=6)\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=f\"January 2025: True vs Predicted for {current_target_name_to_eval}\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Volume (MW)\",\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    fig.update_xaxes(tickformat=\"%d %b %Y %H:%M\")\n",
    "    fig.show()\n",
    "\n",
    "# Create a figure showing all evaluated targets (from target_vars) on the same timeline\n",
    "fig_combined = go.Figure()\n",
    "plotted_any_combined = False\n",
    "\n",
    "for i, current_target_name_to_eval in enumerate(target_vars):\n",
    "    current_true_values = true_series_unscaled[:, i]\n",
    "    try:\n",
    "        model_output_idx = model_output_all_target_names.index(current_target_name_to_eval)\n",
    "        current_predicted_values = model_predictions_unscaled[:, model_output_idx]\n",
    "    except ValueError:\n",
    "        # Error already printed in the loop above\n",
    "        continue\n",
    "\n",
    "    fig_combined.add_trace(go.Scatter(\n",
    "        x=test_dates,\n",
    "        y=current_true_values,\n",
    "        mode='lines',\n",
    "        name=f\"{current_target_name_to_eval} (True)\",\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "    fig_combined.add_trace(go.Scatter(\n",
    "        x=test_dates,\n",
    "        y=current_predicted_values,\n",
    "        mode='lines',\n",
    "        line=dict(dash='dash', width=2),\n",
    "        name=f\"{current_target_name_to_eval} (Predicted)\"\n",
    "    ))\n",
    "    plotted_any_combined = True\n",
    "\n",
    "# Add price data trace if available\n",
    "price_column_name = price_column_name_to_plot\n",
    "price_trace_added = False\n",
    "plot_name_suffix = \" (Original)\"\n",
    "\n",
    "# Access the pre-stored price data using the test_dates index\n",
    "if price_data_full is not None and price_column_name in price_data_full.columns:\n",
    "    try:\n",
    "        # Align price data with test_dates using the index\n",
    "        price_data_to_plot = price_data_full.loc[test_dates, price_column_name].values.astype(float)\n",
    "\n",
    "        # Check if alignment resulted in all NaNs (can happen with index gaps)\n",
    "        if np.isnan(price_data_to_plot).all():\n",
    "             print(f\"Warning: Price data for test dates resulted in all NaNs after alignment.\")\n",
    "             price_data_to_plot = None # Prevent plotting if all NaNs\n",
    "\n",
    "    except KeyError:\n",
    "        # This might happen if test_dates contains dates not present in the original price_data_full index\n",
    "        print(f\"Warning: Could not align price data with test_dates. Index mismatch between price_data_full and test_dates.\")\n",
    "        # Attempt reindexing as a fallback, dropping dates not found in price_data_full\n",
    "        try:\n",
    "            aligned_prices = price_data_full.reindex(test_dates)[price_column_name]\n",
    "            if not aligned_prices.isnull().all():\n",
    "                price_data_to_plot = aligned_prices.values.astype(float)\n",
    "                print(\"Successfully aligned price data using reindex.\")\n",
    "            else:\n",
    "                print(\"Reindexing price data resulted in all NaNs.\")\n",
    "                price_data_to_plot = None\n",
    "        except Exception as reindex_e:\n",
    "            print(f\"Error during fallback reindexing of price data: {reindex_e}\")\n",
    "            price_data_to_plot = None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred accessing or aligning stored price data: {e}\")\n",
    "        price_data_to_plot = None\n",
    "else:\n",
    "     # This case is hit if price_data_full was None initially\n",
    "     # The warning was already printed when price_data_full was defined\n",
    "     price_data_to_plot = None # Ensure it's None\n",
    "\n",
    "if price_data_to_plot is not None:\n",
    "    try:\n",
    "        fig_combined.add_trace(go.Scatter(\n",
    "            x=test_dates,\n",
    "            y=price_data_to_plot,\n",
    "            mode='lines',\n",
    "            name=f\"mFRR Down Price ({REGION}){plot_name_suffix}\",\n",
    "            yaxis=\"y2\",\n",
    "            line=dict(width=1.5, color='rgba(255,165,0,0.7)') # Orange color for price\n",
    "        ))\n",
    "        price_trace_added = True\n",
    "        plotted_any_combined = True # Ensure combined plot is shown if price trace is added\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred attempting to add price trace to plot: {e}\")\n",
    "        price_trace_added = False # Ensure flag is false if adding trace fails\n",
    "\n",
    "# If plotting failed or data was None, print a message\n",
    "if not price_trace_added:\n",
    "    print(f\"Info: Price data ('{price_column_name}') could not be prepared or added to the plot.\")\n",
    "\n",
    "# ADDITION END\n",
    "\n",
    "if plotted_any_combined:\n",
    "    layout_args = dict(\n",
    "        title=f\"January 2025 ({REGION}): True vs Predicted Targets, with Price Data\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis=dict(title=\"Volume (MW)\"), # This is yaxis (y1)\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02, # Default y for legend\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    if price_trace_added:\n",
    "        layout_args['yaxis2'] = dict(\n",
    "            title=\"Price (€/MWh)\", # Adjust unit if known, placeholder\n",
    "            overlaying='y',\n",
    "            side='right',\n",
    "            showgrid=False, # Optional: hide grid for secondary axis\n",
    "            rangemode='tozero' # Assumes prices are generally positive or start from zero\n",
    "        )\n",
    "        # Adjust legend position slightly if yaxis2 is present, to avoid overlap with its title\n",
    "        layout_args['legend']['y'] = 1.08 \n",
    "    \n",
    "    fig_combined.update_layout(**layout_args)\n",
    "    fig_combined.update_xaxes(tickformat=\"%d %b %Y %H:%M\")\n",
    "    fig_combined.show()\n",
    "else:\n",
    "    print(\"No targets were plotted in the combined figure due to previous errors or empty `target_vars`.\")\n",
    "\n",
    "\n",
    "# Calculate performance metrics\n",
    "for i, current_target_name_to_eval in enumerate(target_vars):\n",
    "    current_true_values = true_series_unscaled[:, i]\n",
    "    try:\n",
    "        model_output_idx = model_output_all_target_names.index(current_target_name_to_eval)\n",
    "        current_predicted_values = model_predictions_unscaled[:, model_output_idx]\n",
    "    except ValueError:\n",
    "        # Error already printed, skip metrics for this target\n",
    "        continue\n",
    "        \n",
    "    mse = mean_squared_error(current_true_values, current_predicted_values)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(current_true_values, current_predicted_values)\n",
    "    r2 = r2_score(current_true_values, current_predicted_values)\n",
    "    \n",
    "    print(f\"\\nPerformance Metrics for {current_target_name_to_eval}:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "    # Spike Metrics Calculation\n",
    "    # 1. Determine spike threshold from unscaled training data for the current target\n",
    "    # We need to access the original df_train for this target *before* it was scaled.\n",
    "    # This requires us to re-establish df_train or have saved its original values.\n",
    "    # For simplicity, let's use the P90 of the absolute values of the *current test set's true values* as a proxy for now.\n",
    "    # A more robust approach would be to calculate this on df_train and pass it down or re-calculate properly.\n",
    "    # However, to avoid complex data refetching at this stage, we use current_true_values for threshold calc.\n",
    "    if len(current_true_values) > 0:\n",
    "        spike_threshold = np.percentile(np.abs(current_true_values), 90) \n",
    "        print(f\"Spike Threshold (P90 of test set abs values) for {current_target_name_to_eval}: {spike_threshold:.4f}\")\n",
    "\n",
    "        true_spikes = np.abs(current_true_values) > spike_threshold\n",
    "        pred_spikes = np.abs(current_predicted_values) > spike_threshold\n",
    "\n",
    "        tp = np.sum(true_spikes & pred_spikes)\n",
    "        fp = np.sum(~true_spikes & pred_spikes)\n",
    "        fn = np.sum(true_spikes & ~pred_spikes)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        print(f\"Spike Metrics for {current_target_name_to_eval} (Threshold > {spike_threshold:.2f}):\")\n",
    "        print(f\"  True Positives (Spikes correctly predicted): {tp}\")\n",
    "        print(f\"  False Positives (Non-spikes predicted as spikes): {fp}\")\n",
    "        print(f\"  False Negatives (Spikes missed): {fn}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1-Score: {f1_score:.4f}\")\n",
    "    else:\n",
    "        print(f\"Not enough data to calculate spike metrics for {current_target_name_to_eval}.\")\n",
    "\n",
    "# Additional analysis to check for potential data leakage indicators\n",
    "all_errors_data = {} # To store errors for potential later use (e.g. constructing error_df)\n",
    "\n",
    "for i, current_target_name_to_eval in enumerate(target_vars):\n",
    "    current_true_values = true_series_unscaled[:, i]\n",
    "    try:\n",
    "        model_output_idx = model_output_all_target_names.index(current_target_name_to_eval)\n",
    "        current_predicted_values = model_predictions_unscaled[:, model_output_idx]\n",
    "    except ValueError:\n",
    "        continue # Skip if target name not found in model outputs\n",
    "    \n",
    "    errors = current_true_values - current_predicted_values\n",
    "    all_errors_data[f'error_{current_target_name_to_eval}'] = errors # Store errors\n",
    "    \n",
    "    # Call new plotting functions for current target\n",
    "    scatter_true_vs_pred(current_true_values, current_predicted_values, title_suffix=f\" for {current_target_name_to_eval}\")\n",
    "    residuals_vs_fitted(current_predicted_values, errors, title_suffix=f\" for {current_target_name_to_eval}\")\n",
    "    plot_acf_residuals(errors, title_suffix=f\" for {current_target_name_to_eval}\")\n",
    "    \n",
    "    # Plot error distribution\n",
    "    fig_err_dist = go.Figure()\n",
    "    fig_err_dist.add_trace(go.Histogram(x=errors, nbinsx=50))\n",
    "    fig_err_dist.update_layout(\n",
    "        title=f\"Error Distribution for {current_target_name_to_eval}\",\n",
    "        xaxis_title=\"Error (True - Predicted)\",\n",
    "        yaxis_title=\"Frequency\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    fig_err_dist.show()\n",
    "    \n",
    "    # Test for normal distribution of errors\n",
    "    if len(errors) >= 8: # normaltest requires at least 8 samples\n",
    "        k2, p_norm_test = stats.normaltest(errors)\n",
    "        print(f\"\\nNormality test for {current_target_name_to_eval} errors:\")\n",
    "        print(f\"K²: {k2:.4f}, p-value: {p_norm_test:.4f}\")\n",
    "        if p_norm_test < 0.05:\n",
    "            print(\"Error distribution is not normal - may indicate issues with model\")\n",
    "        else:\n",
    "            print(\"Error distribution appears normal\")\n",
    "    else:\n",
    "        print(f\"\\nNormality test for {current_target_name_to_eval} errors: Not enough samples (need >= 8, got {len(errors)})\")\n",
    "        \n",
    "    # Plot error time series\n",
    "    fig_err_ts = go.Figure()\n",
    "    fig_err_ts.add_trace(go.Scatter(\n",
    "        x=test_dates,\n",
    "        y=errors,\n",
    "        mode='lines',\n",
    "        name=\"Prediction Error\"\n",
    "    ))\n",
    "    fig_err_ts.update_layout(\n",
    "        title=f\"Prediction Errors Over Time for {current_target_name_to_eval}\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Error (True - Predicted)\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    fig_err_ts.show()\n",
    "\n",
    "# If you need an error DataFrame for further analysis (e.g., autocorrelation with statsmodels)\n",
    "if all_errors_data:\n",
    "    error_df_evaluated_targets = pd.DataFrame(all_errors_data, index=test_dates)\n",
    "else:\n",
    "    print(\"\\nNo errors were collected for DataFrame construction (all_errors_data is empty).\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Download ENTSOE-e data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "Download every 2025*.zip found inside each subfolder of /TP_export/zip\n",
                "and save it locally under the same subfolder name.\n",
                "\"\"\"\n",
                "\n",
                "import paramiko\n",
                "import os\n",
                "import stat\n",
                "from pathlib import Path\n",
                "from dotenv import load_dotenv # Added import\n",
                "\n",
                "HOST = \"sftp-transparency.entsoe.eu\"\n",
                "PORT = 22\n",
                "# Updated to load from environment variables\n",
                "USERNAME = os.getenv(\"USERNAME\")\n",
                "PASSWORD = os.getenv(\"PASSWORD\")\n",
                "REMOTE_BASE = \"/TP_export/zip\"\n",
                "LOCAL_BASE = Path(\"TP_export_zip\")\n",
                "\n",
                "\n",
                "def ensure_dir(path: str) -> None:\n",
                "    os.makedirs(path, exist_ok=True)\n",
                "\n",
                "\n",
                "def main() -> None:\n",
                "    load_dotenv() # Load .env file\n",
                "    ssh = paramiko.SSHClient()\n",
                "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # auto-trust host key once\n",
                "    ssh.connect(HOST, port=PORT, username=USERNAME, password=PASSWORD)\n",
                "\n",
                "    sftp = ssh.open_sftp()\n",
                "\n",
                "    for entry in sftp.listdir_attr(REMOTE_BASE):\n",
                "        if stat.S_ISDIR(entry.st_mode):\n",
                "            remote_dir = f\"{REMOTE_BASE}/{entry.filename}\"\n",
                "            local_dir = os.path.join(LOCAL_BASE, entry.filename)\n",
                "            ensure_dir(local_dir)\n",
                "\n",
                "            for file_attr in sftp.listdir_attr(remote_dir):\n",
                "                name = file_attr.filename\n",
                "                if name.startswith(\"2025\") and name.endswith(\".zip\"):\n",
                "                    remote_file = f\"{remote_dir}/{name}\"\n",
                "                    local_file = os.path.join(local_dir, name)\n",
                "                    print(f\"⬇︎ {remote_file} → {local_file}\")\n",
                "                    sftp.get(remote_file, local_file)\n",
                "\n",
                "    sftp.close()\n",
                "    ssh.close()\n",
                "    print(\"Done!\")\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Unzip and sort"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import zipfile\n",
                "import pandas as pd\n",
                "import concurrent.futures\n",
                "from pathlib import Path\n",
                "from typing import List, Tuple\n",
                "import pyarrow as pa\n",
                "import pyarrow.csv as csv\n",
                "from functools import partial\n",
                "\n",
                "def extract_from_zip(zip_file: Path, folder_path: Path) -> List[str]:\n",
                "    \"\"\"Extract CSV files from a ZIP file and return list of extracted filenames.\"\"\"\n",
                "    extracted_files = []\n",
                "    try:\n",
                "        with zipfile.ZipFile(zip_file, 'r') as z:\n",
                "            for member in z.namelist():\n",
                "                if member.lower().endswith(\".csv\"):\n",
                "                    z.extract(member, path=folder_path)\n",
                "                    extracted_files.append(member)\n",
                "                    print(f\"Extracted {member} from {zip_file.name}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error processing zip file {zip_file.name}: {e}\")\n",
                "    return extracted_files\n",
                "\n",
                "def process_csv_chunk(csv_file: Path) -> Tuple[pd.DataFrame, str]:\n",
                "    \"\"\"Process a single CSV file handling both comma and tab separators.\"\"\"\n",
                "    try:\n",
                "        # Try reading with tab separator first\n",
                "        try:\n",
                "            df = pd.read_csv(csv_file, sep='\\t', engine='python')\n",
                "        except:\n",
                "            # If tab separator fails, try comma separator\n",
                "            df = pd.read_csv(csv_file, sep=',', engine='python')\n",
                "        \n",
                "        # Clean column names by stripping whitespace\n",
                "        df.columns = df.columns.str.strip()\n",
                "        return df, csv_file.name\n",
                "    except Exception as e:\n",
                "        print(f\"Error processing {csv_file.name}: {e}\")\n",
                "        return pd.DataFrame(), csv_file.name\n",
                "\n",
                "def process_folder(folder_path: Path):\n",
                "    \"\"\"Optimized version of process_folder using parallel processing and efficient I/O.\"\"\"\n",
                "    print(f\"\\nProcessing folder: {folder_path.name}\")\n",
                "    \n",
                "    # Look for CSV files in the folder\n",
                "    csv_files = list(folder_path.glob(\"*.csv\"))\n",
                "    \n",
                "    # Handle ZIP files if no CSVs found\n",
                "    if not csv_files:\n",
                "        zip_files = list(folder_path.glob(\"*.zip\"))\n",
                "        if zip_files:\n",
                "            print(\"No CSV files found. Checking ZIP files for CSV extraction...\")\n",
                "            # Process ZIP files in parallel\n",
                "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
                "                futures = [\n",
                "                    executor.submit(extract_from_zip, zip_file, folder_path)\n",
                "                    for zip_file in zip_files\n",
                "                ]\n",
                "                concurrent.futures.wait(futures)\n",
                "            \n",
                "            # Remove ZIP files after extraction\n",
                "            for zip_file in zip_files:\n",
                "                try:\n",
                "                    os.remove(zip_file)\n",
                "                    print(f\"Removed zip file: {zip_file.name}\")\n",
                "                except Exception as e:\n",
                "                    print(f\"Failed to remove zip file {zip_file.name}: {e}\")\n",
                "            \n",
                "            # Update CSV files list\n",
                "            csv_files = list(folder_path.glob(\"*.csv\"))\n",
                "        else:\n",
                "            print(\"No CSV or ZIP files found in folder, skipping processing.\")\n",
                "            return\n",
                "\n",
                "    if not csv_files:\n",
                "        print(\"No CSV files could be found or extracted, skipping processing.\")\n",
                "        return\n",
                "\n",
                "    # Process CSV files in parallel using ThreadPoolExecutor\n",
                "    dfs = []\n",
                "    csv_names = []\n",
                "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
                "        future_to_csv = {\n",
                "            executor.submit(process_csv_chunk, csv_file): csv_file\n",
                "            for csv_file in csv_files\n",
                "        }\n",
                "        for future in concurrent.futures.as_completed(future_to_csv):\n",
                "            df, name = future.result()\n",
                "            if not df.empty:\n",
                "                dfs.append(df)\n",
                "                csv_names.append(name)\n",
                "\n",
                "    if not dfs:\n",
                "        print(\"No valid CSV data was found, skipping processing.\")\n",
                "        return\n",
                "\n",
                "    # Combine all DataFrames efficiently\n",
                "    combined_df = pd.concat(dfs, ignore_index=True, copy=False)\n",
                "\n",
                "    # Sort by date if date column exists\n",
                "    date_cols = [col for col in combined_df.columns if 'date' in col.lower()]\n",
                "    if date_cols:\n",
                "        combined_df.sort_values(by=date_cols[0], inplace=True, ignore_index=True)\n",
                "\n",
                "    # Prepare output directories\n",
                "    processed_dir = Path(\"processed_data\")\n",
                "    filtered_dir = Path(\"filtered_data\")\n",
                "    processed_dir.mkdir(exist_ok=True)\n",
                "    filtered_dir.mkdir(exist_ok=True)\n",
                "\n",
                "    # Save combined CSV efficiently using pyarrow\n",
                "    combined_csv_path = processed_dir / f\"{folder_path.name}.csv\"\n",
                "    try:\n",
                "        table = pa.Table.from_pandas(combined_df)\n",
                "        csv.write_csv(table, combined_csv_path)\n",
                "        print(f\"Combined CSV saved to: {combined_csv_path}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to write combined CSV: {e}\")\n",
                "        return\n",
                "\n",
                "    # Create log details\n",
                "    details = (\n",
                "        f\"Combined CSV file from folder: {folder_path.name}\\n\"\n",
                "        f\"Files combined: {', '.join(csv_names)}\\n\"\n",
                "        f\"Total rows: {len(combined_df)}\\n\"\n",
                "    )\n",
                "\n",
                "    # Remove original CSVs and create log file\n",
                "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
                "        executor.map(os.remove, csv_files)\n",
                "    \n",
                "    log_path = folder_path / f\"{folder_path.name}.txt\"\n",
                "    with open(log_path, \"w\") as log_file:\n",
                "        log_file.write(details)\n",
                "    print(f\"Log file created at: {log_path}\")\n",
                "\n",
                "    # Handle MapCode filtering or\n",
                "    # OutAreaMapCode and InAreaMapCode \n",
                "    # OutMapCode and InMapCode\n",
                "    # Check for any of the possible map code columns\n",
                "    possible_columns = ['mapcode', 'outareamapcode', 'inaremapcode', 'outmapcode', 'inmapcode', 'areamapcode', 'location']\n",
                "    map_columns = [col for col in combined_df.columns if col.strip().lower() in possible_columns]\n",
                "    \n",
                "    if map_columns:\n",
                "        try:\n",
                "            # Create a mask that checks for 'SE' in any of the map code columns\n",
                "            mask = pd.Series(False, index=combined_df.index)\n",
                "            for col in map_columns:\n",
                "                mask |= combined_df[col].astype(str).str.contains(\"SE\", na=False, case=False)\n",
                "            \n",
                "            # Apply the mask to filter rows\n",
                "            filtered_df = combined_df[mask]\n",
                "            filtered_csv_path = filtered_dir / f\"{folder_path.name}_SE.csv\"\n",
                "            \n",
                "            # Save filtered data efficiently using pyarrow\n",
                "            filtered_table = pa.Table.from_pandas(filtered_df)\n",
                "            csv.write_csv(filtered_table, filtered_csv_path)\n",
                "            print(f\"Filtered CSV saved to: {filtered_csv_path}\")\n",
                "            \n",
                "            filter_details = (\n",
                "                f\"Filtering successful for folder: {folder_path.name}\\n\"\n",
                "                f\"Columns used for filtering: {', '.join(map_columns)}\\n\"\n",
                "                f\"Filtered CSV (contains 'SE') rows: {len(filtered_df)}\\n\"\n",
                "                f\"Files combined: {', '.join(csv_names)}\\n\"\n",
                "            )\n",
                "            \n",
                "            # Clean up and create logs\n",
                "            os.remove(combined_csv_path)\n",
                "            filtering_log = combined_csv_path.with_suffix(\".txt\")\n",
                "            with open(filtering_log, \"w\") as f:\n",
                "                f.write(filter_details)\n",
                "            print(f\"Combined CSV replaced with filtering log: {filtering_log}\")\n",
                "            \n",
                "        except Exception as e:\n",
                "            failure_details = (\n",
                "                f\"Filtering failed for folder: {folder_path.name}\\n\"\n",
                "                f\"Error: {e}\\n\"\n",
                "                f\"Files combined: {', '.join(csv_names)}\\n\"\n",
                "                f\"Total rows in combined CSV: {len(combined_df)}\\n\"\n",
                "            )\n",
                "            filtering_log = combined_csv_path.with_suffix(\".txt\")\n",
                "            with open(filtering_log, \"w\") as f:\n",
                "                f.write(failure_details)\n",
                "            print(f\"Filtering failed; added failure log: {filtering_log}\")\n",
                "    else:\n",
                "        no_filter_details = (\n",
                "            f\"No MapCode columns found in combined CSV for folder: {folder_path.name}\\n\"\n",
                "            f\"Checked for columns: {', '.join(possible_columns)}\\n\"\n",
                "            f\"Available columns: {', '.join(combined_df.columns)}\\n\"\n",
                "            f\"Filtering was not applied.\\n\"\n",
                "        )\n",
                "        filtering_log = combined_csv_path.with_suffix(\".txt\")\n",
                "        with open(filtering_log, \"w\") as f:\n",
                "            f.write(no_filter_details)\n",
                "        print(f\"No MapCode columns; filtering log created: {filtering_log}\")\n",
                "\n",
                "def main():\n",
                "    base_dir = Path(\"TP_export_zip\")\n",
                "    if not base_dir.exists():\n",
                "        print(f\"Base directory '{base_dir}' does not exist.\")\n",
                "    else:\n",
                "        folders = [folder for folder in base_dir.iterdir() if folder.is_dir()]\n",
                "        # Process folders sequentially but with internal parallelization\n",
                "        for folder in folders:\n",
                "            process_folder(folder)\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Move into appropiate folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from pathlib import Path\n",
                "import re\n",
                "import shutil\n",
                "\n",
                "SECTION_TO_FOLDER = {\n",
                "    \"17\": \"Balancing\",\n",
                "    \"12.3\": \"Balancing\",          # special Balancing subsection\n",
                "    \"13\": \"Congestion Management\",\n",
                "    \"16\": \"Generation\",\n",
                "    \"14\": \"Generation\",\n",
                "    \"6\":  \"Load\",\n",
                "    \"8.1\": \"Load\",\n",
                "    \"7\":  \"Outages\",\n",
                "    \"10\": \"Outages\",\n",
                "    \"15\": \"Outages\",\n",
                "    \"11\": \"Transmission\",\n",
                "    \"12.1\": \"Transmission\",\n",
                "}\n",
                "\n",
                "# pattern:  <anything>_<digits[.digits][.letter]>  e.g. _17.1.D or _6.1 or _12.3.E\n",
                "CODE_RX = re.compile(r\"_(\\d+(?:\\.\\d+)?)(?:\\.[A-Z])?_\")   # group(1) = 17, 12.3, 6 …\n",
                "\n",
                "HERE = Path(\"filtered_data\")\n",
                "\n",
                "moved, skipped = 0, 0\n",
                "for csv in HERE.glob(\"*.csv\"):\n",
                "    m = CODE_RX.search(csv.name)\n",
                "    if not m:\n",
                "        skipped += 1\n",
                "        print(f\"couldnt recognise code in {csv.name}; leaving it in place\")\n",
                "        continue\n",
                "\n",
                "    section = m.group(1)          # e.g. '17', '12.3', '6', '8.1'\n",
                "    # pick the longest matching key (so 12.3 wins over 12)\n",
                "    dest_folder = next(\n",
                "        (SECTION_TO_FOLDER[k] for k in sorted(SECTION_TO_FOLDER, key=len, reverse=True)\n",
                "         if section.startswith(k)),\n",
                "        None,\n",
                "    )\n",
                "\n",
                "    if dest_folder is None:\n",
                "        skipped += 1\n",
                "        print(f\"no rule for section {section} → {csv.name}\")\n",
                "        continue\n",
                "\n",
                "    dest_dir = HERE / dest_folder\n",
                "    dest_dir.mkdir(exist_ok=True)\n",
                "    shutil.move(str(csv), dest_dir / csv.name)\n",
                "    moved += 1\n",
                "    print(f\"{csv.name}  →  {dest_folder}/\")\n",
                "\n",
                "print(f\"\\nDone — moved {moved} file(s), skipped {skipped}.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Proccess long to wide format"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Balancing folder\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "# Configuration for each file\n",
                "config = [\n",
                "    {\n",
                "        \"Balancing folder\": \"AcceptedAggregatedOffers_17.1.D_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode, ReserveType\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, NotSpecifiedUpAcceptedVolume, NotSpecifiedDownAcceptedVolume, NotSpecifiedOfferedVolumeSymmetric, NotSpecifiedAcceptedVolumeSymmetric\"\n",
                "    },\n",
                "    {\n",
                "        \"Balancing folder\": \"ActivatedBalancingEnergy_17.1.E_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode, ReserveType\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, NotSpecifiedUpActivatedVolume, NotSpecifiedDownActivatedVolume\"\n",
                "    },\n",
                "    {\n",
                "        \"Balancing folder\": \"AggregatedBalancingEnergyBids_12.3.E_r3_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime(UTC)\",\n",
                "        \"Merging column\": \"MapCode, ReserveType\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, OfferedUpBidVolume[MW], OfferedDownBidVolume[MW], ActivatedDownBidVolume[MW], ActivatedUpBidVolume[MW]\"\n",
                "    },\n",
                "    {\n",
                "        \"Balancing folder\": \"ImbalancePrices_17.1.G_r2.1_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime(UTC)\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, PositiveImbalancePrice, NegativeImbalancePrice\"\n",
                "    },\n",
                "    {\n",
                "        \"Balancing folder\": \"PricesOfActivatedBalancingEnergy_17.1.F_r3_SE.csv\",\n",
                "        \"Index/date column name\": \"ISP(UTC)\",\n",
                "        \"Merging column\": \"MapCode, ReserveType\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, TypeOfProduct, GenerationUpPrice, GenerationDownPrice, NotSpecifiedUpPrice, NotSpecifiedDownPrice, Currency\"\n",
                "    },\n",
                "    # {\n",
                "    #     \"Balancing folder\": \"PricesOfProcuredBalancingReserves_17.1.C_SE.csv\",\n",
                "    #     \"Index/date column name\": \"DateTime\",\n",
                "    #     \"Merging column\": \"MapCode, ReserveType, Direction\",\n",
                "    #     \"Keep columns\": \"ResolutionCode, AreaTypeCode, ContractedPrice\"\n",
                "    # },\n",
                "    {\n",
                "        \"Balancing folder\": \"TotalimbalanceVolumes_17.1.H_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, TotalImbalanceVolume\"\n",
                "    },\n",
                "    {\n",
                "        \"Balancing folder\": \"FinancialExpensesAndincomeForBalancing_17.1.I_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, Expenses, Income\"\n",
                "    },\n",
                "]\n",
                "\n",
                "\n",
                "def process_file(file_path, file_config):\n",
                "    \"\"\"\n",
                "    Process a single file to convert from long format to wide format\n",
                "    based on the specified configuration.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "    except UnicodeDecodeError:\n",
                "        try:\n",
                "            df = pd.read_csv(file_path, encoding='latin1', low_memory=False)\n",
                "        except Exception as e:\n",
                "            print(f\"Error reading {file_path}: {str(e)}\")\n",
                "            return None\n",
                "    \n",
                "    date_col = file_config[\"Index/date column name\"]\n",
                "    \n",
                "    # Get configured ID columns and value columns\n",
                "    merging_cols_from_config_str = file_config[\"Merging column\"]\n",
                "    # Handle if merging column string is empty or just spaces\n",
                "    if not merging_cols_from_config_str.strip():\n",
                "        merging_cols_list_config = []\n",
                "    else:\n",
                "        merging_cols_list_config = [col.strip() for col in merging_cols_from_config_str.split(\",\")]\n",
                "        \n",
                "    keep_cols_list_config = [col.strip() for col in file_config[\"Keep columns\"].split(\",\")]\n",
                "\n",
                "    # Filter these to only include columns that actually exist in the DataFrame\n",
                "    # These id_cols and value_cols are the ones that will be used.\n",
                "    id_cols = [col for col in merging_cols_list_config if col in df.columns]\n",
                "    value_cols = [col for col in keep_cols_list_config if col in df.columns]\n",
                "\n",
                "    # Check if date_col exists (critical for indexing and pivot)\n",
                "    if date_col not in df.columns:\n",
                "        print(f\"Critical: Date column '{date_col}' missing in {file_path}. Skipping file.\")\n",
                "        return None\n",
                "\n",
                "    # Determine the minimal set of columns to keep in the initial DataFrame\n",
                "    # This includes the date column, all (existing) ID columns, and all (existing) value columns.\n",
                "    # Using a set to avoid duplicates, then converting to list to preserve order from original df.columns (if possible) or a defined order.\n",
                "    actual_cols_to_load = {date_col}\n",
                "    actual_cols_to_load.update(id_cols)\n",
                "    actual_cols_to_load.update(value_cols)\n",
                "    \n",
                "    # Filter df to only these necessary columns, preserving original order as much as possible\n",
                "    cols_present_in_df = [col for col in df.columns if col in actual_cols_to_load]\n",
                "    df = df[cols_present_in_df]\n",
                "    \n",
                "    # Convert date column to datetime\n",
                "    df[date_col] = pd.to_datetime(df[date_col])\n",
                "    \n",
                "    # 1. build the ID safely (as per user's patch structure)\n",
                "    # id_cols here is the filtered list of *existing* merging column names.\n",
                "    if id_cols:\n",
                "        # turn each row into \"_\".join([...]) → always a scalar string\n",
                "        df[\"id\"] = (\n",
                "            df[id_cols]\n",
                "            .fillna(\"\")                        # don't drop rows because of NaNs\n",
                "            .astype(str)\n",
                "            .apply(lambda r: \"_\".join(r).strip(\"_\"), axis=1)\n",
                "        )\n",
                "    else:                                      # no merging columns configured/found\n",
                "        df[\"id\"] = \"value\"\n",
                "\n",
                "    # 2. initialise the result index once (as per user's patch structure)\n",
                "    # df now contains 'id' (if id_cols was not empty) and 'date_col'\n",
                "    # Ensure date_col is still present (should be, due to earlier check)\n",
                "    # And 'id' column is also used by pivot_table if id_cols was present.\n",
                "    if date_col not in df.columns: # Should not happen if initial check passed\n",
                "         print(f\"Critical error: date_col '{date_col}' lost before result initialization for {file_path}.\")\n",
                "         return None\n",
                "    \n",
                "    # Create the base result DataFrame with a unique sorted datetime index\n",
                "    # If 'id' column was not created (because id_cols was empty), pivot_table will use 'value' as id.\n",
                "    result_index_df = df[[date_col]].copy() # Operate on a copy to avoid SettingWithCopyWarning\n",
                "    result_index_df.drop_duplicates(subset=[date_col], inplace=True)\n",
                "    result_index_df.set_index(date_col, inplace=True)\n",
                "    result_index_df.sort_index(inplace=True)\n",
                "    result = result_index_df \n",
                "    # result.index = pd.to_datetime(result.index) # Index is already datetime\n",
                "\n",
                "    # 3. wide-format one value column at a time (as per user's patch structure)\n",
                "    # value_cols is the filtered list of *existing* value column names.\n",
                "    for col in value_cols:\n",
                "        if col not in df.columns: # Should not happen if value_cols are derived from df.columns\n",
                "            print(f\"Warning: Value column '{col}' unexpectedly missing in df for {file_path}. Skipping.\")\n",
                "            continue\n",
                "        if 'id' not in df.columns: # Check if 'id' column exists for pivot\n",
                "             print(f\"Critical error: 'id' column missing before pivot for {file_path} (value col: {col}).\")\n",
                "             continue\n",
                "\n",
                "\n",
                "        pivot = df.pivot_table(\n",
                "            index=date_col,\n",
                "            columns=\"id\", # Uses the 'id' column created above\n",
                "            values=col,   # Current value column to pivot\n",
                "            aggfunc=\"first\" # Aggregation function for duplicates\n",
                "        )\n",
                "        \n",
                "        # Rename columns: original_id_value_CellValueColumnName\n",
                "        pivot.columns = [f\"{id_val}_{col}\" for id_val in pivot.columns]\n",
                "        \n",
                "        result = result.join(pivot) # Join pivoted data to the main result DataFrame\n",
                "    \n",
                "    # Filter out columns containing \"_SEM_\" (remains from original script logic)\n",
                "    sem_columns = [col for col in result.columns if \"_SEM_\" in col]\n",
                "    if sem_columns:\n",
                "        print(f\"  Removing {len(sem_columns)} columns containing '_SEM_'\")\n",
                "        result = result.drop(columns=sem_columns)\n",
                "    \n",
                "    if result.empty and not value_cols:\n",
                "        print(f\"  Processed {file_path}. No value columns were specified or found, result is empty based on index.\")\n",
                "    elif result.empty and value_cols :\n",
                "        print(f\"  Processed {file_path}. Resulting DataFrame is empty despite value columns being present. Check data and pivoting logic.\")\n",
                "    else:\n",
                "        print(f\"  Processed {file_path} into {result.shape[1]} columns\")\n",
                "    return result\n",
                "\n",
                "\n",
                "def main():\n",
                "    input_folder_path = Path(\"filtered_data/Balancing\")\n",
                "    output_folder_path = Path(\"pivoted_data/Balancing\")\n",
                "    os.makedirs(output_folder_path, exist_ok=True)\n",
                "    \n",
                "    for file_config in config:\n",
                "        file_name = file_config[\"Balancing folder\"]\n",
                "        file_path = input_folder_path / file_name # Use Path object for joining\n",
                "        \n",
                "        if file_path.exists():\n",
                "            print(f\"Processing {file_name}...\")\n",
                "            result_df = process_file(str(file_path), file_config) # process_file expects string path\n",
                "            \n",
                "            if result_df is not None and not result_df.empty:\n",
                "                base_name = file_path.stem # Get filename without extension\n",
                "                output_file = f\"{base_name}_pivoted.csv\"\n",
                "                output_path = output_folder_path / output_file\n",
                "                \n",
                "                result_df.to_csv(output_path)\n",
                "                \n",
                "                print(f\"  Saved to {output_path}\")\n",
                "                print(f\"  Shape: {result_df.shape}\")\n",
                "                if not result_df.index.empty:\n",
                "                    print(f\"  Date range: {result_df.index.min()} to {result_df.index.max()}\")\n",
                "                else:\n",
                "                    print(f\"  Date range: Index is empty.\")\n",
                "                if result_df.size > 0 :\n",
                "                    missing_percentage = 100 * result_df.isna().sum().sum() / result_df.size\n",
                "                    print(f\"  Missing values: {result_df.isna().sum().sum()}/{result_df.size} ({missing_percentage:.2f}%)\")\n",
                "                else:\n",
                "                    print(f\"  Missing values: DataFrame is empty, no values.\")\n",
                "                print(f\"  Sample columns: {list(result_df.columns)[:min(3, len(result_df.columns))]}\")\n",
                "                print(\"\")\n",
                "            elif result_df is not None and result_df.empty:\n",
                "                print(f\"  Processed {file_name}, but the resulting DataFrame is empty. No file saved.\")\n",
                "            else: # result_df is None\n",
                "                print(f\"  Failed to process {file_name} or an error occurred.\")\n",
                "        else:\n",
                "            print(f\"File not found: {file_path}\")\n",
                "    \n",
                "    print(\"\\nAll files processed.\")\n",
                "    print(f\"Output folder: {output_folder_path.resolve()}\")\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "import os\n",
                "\n",
                "# Configuration for each file\n",
                "config = [\n",
                "    {\n",
                "        \"Balancing folder\": \"AmountOfBalancingReservesUnderContract_17.1.B_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode, ReserveType, Direction\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, ContractedVolume, SourceName, ContractType\",\n",
                "        \"Aggregation methods\": {\n",
                "            \"ContractedVolume\": \"sum\"\n",
                "        }\n",
                "    },\n",
                "]\n",
                "\n",
                "def process_file(file_path, file_config):\n",
                "    \"\"\"\n",
                "    Process a single file to convert from long format to wide format\n",
                "    based on the specified configuration, handling duplicate entries.\n",
                "    \"\"\"\n",
                "    # Read CSV (UTF-8 fallback to latin1)\n",
                "    try:\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "    except UnicodeDecodeError:\n",
                "        df = pd.read_csv(file_path, encoding='latin1', low_memory=False)\n",
                "\n",
                "    # Extract configuration\n",
                "    date_col = file_config[\"Index/date column name\"]\n",
                "    id_cols = [col.strip() for col in file_config[\"Merging column\"].split(\",\")]\n",
                "    value_cols = [col.strip() for col in file_config[\"Keep columns\"].split(\",\")]\n",
                "    agg_methods = file_config.get(\"Aggregation methods\", {})\n",
                "\n",
                "    # Warn if any columns are missing\n",
                "    missing = set(id_cols + [date_col] + value_cols) - set(df.columns)\n",
                "    if missing:\n",
                "        print(f\"Warning: Missing columns in {file_path}: {missing}\")\n",
                "        id_cols = [c for c in id_cols if c in df.columns]\n",
                "        value_cols = [c for c in value_cols if c in df.columns]\n",
                "\n",
                "    # Convert date column\n",
                "    df[date_col] = pd.to_datetime(df[date_col])\n",
                "\n",
                "    # Optional duplicate check\n",
                "    if id_cols:\n",
                "        dupes = df.groupby([date_col] + id_cols).size()\n",
                "        dup_count = (dupes > 1).sum()\n",
                "        if dup_count > 0:\n",
                "            print(f\"  ⚠️ {dup_count} duplicate date+ID combinations found\")\n",
                "\n",
                "    # Prepare result DataFrame indexed by all timestamps\n",
                "    timestamps = sorted(df[date_col].unique())\n",
                "    result = pd.DataFrame(index=timestamps)\n",
                "    result.index.name = date_col\n",
                "\n",
                "    # Pivot each value column\n",
                "    for col in value_cols:\n",
                "        method = agg_methods.get(col, 'first')\n",
                "        pivot = df.pivot_table(\n",
                "            index=date_col,\n",
                "            columns=id_cols if id_cols else None,\n",
                "            values=col,\n",
                "            aggfunc=method\n",
                "        )\n",
                "        # Flatten MultiIndex or single index\n",
                "        if isinstance(pivot.columns, pd.MultiIndex):\n",
                "            pivot.columns = [\"_\".join(map(str, key)) + f\"_{col}\" for key in pivot.columns]\n",
                "        else:\n",
                "            pivot.columns = [str(key) + f\"_{col}\" for key in pivot.columns]\n",
                "        result = result.join(pivot)\n",
                "\n",
                "    # Remove any '_SEM_' columns\n",
                "    sem_cols = [c for c in result.columns if '_SEM_' in c]\n",
                "    if sem_cols:\n",
                "        result.drop(columns=sem_cols, inplace=True)\n",
                "\n",
                "    print(f\"  Processed {file_path}: {result.shape[1]} columns\")\n",
                "    return result\n",
                "\n",
                "\n",
                "def main():\n",
                "    input_folder = Path(\"filtered_data/Balancing\")\n",
                "    output_folder = Path(\"pivoted_data/Balancing\")\n",
                "    output_folder.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "    for cfg in config:\n",
                "        fname = cfg[\"Balancing folder\"]\n",
                "        in_path = input_folder / fname\n",
                "        if not in_path.exists():\n",
                "            print(f\"File not found: {in_path}\")\n",
                "            continue\n",
                "\n",
                "        print(f\"Processing {fname}...\")\n",
                "        df_wide = process_file(in_path, cfg)\n",
                "        if df_wide is None:\n",
                "            continue\n",
                "\n",
                "        out_name = in_path.stem + '_pivoted.csv'\n",
                "        out_path = output_folder / out_name\n",
                "        df_wide.to_csv(out_path)\n",
                "\n",
                "        # Report\n",
                "        total = df_wide.size\n",
                "        missing = df_wide.isna().sum().sum()\n",
                "        print(f\"  Saved to {out_path}\")\n",
                "        print(f\"  Shape: {df_wide.shape}\")\n",
                "        print(f\"  Date range: {df_wide.index.min()} to {df_wide.index.max()}\")\n",
                "        print(f\"  Missing values: {missing}/{total} ({100*missing/total:.2f}%)\")\n",
                "        print(\"\")\n",
                "\n",
                "    print(\"All files processed.\")\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Congestion Management"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "import json\n",
                "\n",
                "\n",
                "config = [\n",
                "    {\n",
                "        \"Congestion Management folder\": \"Countertrading_13.1.B_r2.1_SE.csv\",\n",
                "        \"Index/date column name\": \"StartTimeSeries(UTC)\",\n",
                "        # \"Merging column\": \"OutAreaMapCode, InAreaMapCode, \",\n",
                "        \"Keep columns\": \"Resolution, ChangeInCrosszonalExchange(MW), Action\"\n",
                "    },\n",
                "    {\n",
                "        \"Congestion Management folder\": \"RedispatchingInternal_13.1.A_r2.1_SE.csv\",\n",
                "        \"Index/date column name\": \"StartTimeSeries(UTC)\",\n",
                "        # \"Merging column\": \"OutAreaMapCode, InAreaMapCode, \",\n",
                "        \"Keep columns\": \"CapacityImpact(MWh/MTU), ActionDirection(MW)\"\n",
                "    }\n",
                "    \n",
                "\n",
                "]\n",
                "\n",
                "\n",
                "def process_file(file_path, file_config):\n",
                "    \"\"\"\n",
                "    Process a single file to convert from long format to wide format\n",
                "    based on the specified configuration with special handling for SE regions.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Try reading with UTF-8\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "    except UnicodeDecodeError:\n",
                "        try:\n",
                "            # If UTF-8 fails, try 'latin1'\n",
                "            df = pd.read_csv(file_path, encoding='latin1', low_memory=False)\n",
                "        except Exception as e:\n",
                "            print(f\"Error reading {file_path}: {str(e)}\")\n",
                "            return None\n",
                "    \n",
                "    # Extract configuration\n",
                "    date_col = file_config[\"Index/date column name\"]\n",
                "    value_cols = [col.strip() for col in file_config[\"Keep columns\"].split(\",\")]\n",
                "    \n",
                "    # Check if required columns exist\n",
                "    required_cols = [date_col, \"OutAreaMapCode\", \"InAreaMapCode\"] + value_cols\n",
                "    missing_cols = set(required_cols) - set(df.columns)\n",
                "    if missing_cols:\n",
                "        print(f\"Warning: Missing columns in {file_path}: {missing_cols}\")\n",
                "        return None\n",
                "    \n",
                "    # Only keep needed columns\n",
                "    cols_to_keep = [date_col, \"OutAreaMapCode\", \"InAreaMapCode\"] + value_cols\n",
                "    df = df[cols_to_keep]\n",
                "    \n",
                "    # Convert date column to datetime\n",
                "    df[date_col] = pd.to_datetime(df[date_col])\n",
                "    \n",
                "    # Define Swedish bidding zones\n",
                "    swedish_zones = ['SE', 'SE1', 'SE2', 'SE3', 'SE4']\n",
                "    \n",
                "    # Filter for rows where either OutAreaMapCode or InAreaMapCode is a Swedish zone\n",
                "    df_se = df[(df['OutAreaMapCode'].isin(swedish_zones)) | (df['InAreaMapCode'].isin(swedish_zones))]\n",
                "    \n",
                "    # Create a direction-based identifier\n",
                "    df_se['flow_id'] = df_se.apply(\n",
                "        lambda row: f\"FROM_{row['OutAreaMapCode']}_TO_{row['InAreaMapCode']}\", \n",
                "        axis=1\n",
                "    )\n",
                "    \n",
                "    # Check for duplicates with the new identifier\n",
                "    duplicate_check = df_se.duplicated(subset=[date_col, 'flow_id'], keep=False)\n",
                "    if duplicate_check.any():\n",
                "        dup_count = duplicate_check.sum()\n",
                "        print(f\"  Warning: Found {dup_count} duplicate entries with the same date and flow ID\")\n",
                "        \n",
                "        # Detect numeric and non-numeric columns\n",
                "        numeric_cols = []\n",
                "        non_numeric_cols = []\n",
                "        \n",
                "        for col in value_cols:\n",
                "            try:\n",
                "                pd.to_numeric(df_se[col])\n",
                "                numeric_cols.append(col)\n",
                "            except (ValueError, TypeError):\n",
                "                non_numeric_cols.append(col)\n",
                "        \n",
                "        # Create an aggregation dictionary\n",
                "        agg_dict = {}\n",
                "        for col in numeric_cols:\n",
                "            agg_dict[col] = 'mean'\n",
                "        for col in non_numeric_cols:\n",
                "            agg_dict[col] = 'first'\n",
                "        \n",
                "        # Apply the appropriate aggregation to each column\n",
                "        print(\"  Aggregating duplicate values with mixed methods (mean for numeric, first for non-numeric)\")\n",
                "        df_se = df_se.groupby([date_col, 'flow_id']).agg(agg_dict).reset_index()\n",
                "    \n",
                "    # Create a wide-format dataframe\n",
                "    result = pd.DataFrame(index=df_se[date_col].unique())\n",
                "    result.index = pd.to_datetime(result.index)\n",
                "    result.sort_index(inplace=True)\n",
                "    result.index.name = date_col\n",
                "    \n",
                "    # Process each value column\n",
                "    for col in value_cols:\n",
                "        # Pivot to get values for each flow ID\n",
                "        pivot = df_se.pivot(index=date_col, columns='flow_id', values=col)\n",
                "        \n",
                "        # Rename columns with a more descriptive format\n",
                "        pivot.columns = [f\"{flow_id}_{col}\" for flow_id in pivot.columns]\n",
                "        \n",
                "        # Join with the result\n",
                "        result = result.join(pivot)\n",
                "    \n",
                "    print(f\"  Processed {file_path} into {result.shape[1]} columns\")\n",
                "    return result\n",
                "\n",
                "def main():\n",
                "    # Main processing logic\n",
                "    # input_folder_path = \"../../processed_data/grouped_data/Congestion Management\"\n",
                "    # output_folder_path = \"../../processed_data/pivoted_data/Congestion Management\"\n",
                "    input_folder_path = Path(\"filtered_data/Congestion Management\")\n",
                "    output_folder_path = Path(\"pivoted_data/Congestion Management\")\n",
                "    \n",
                "    # Create output directory if it doesn't exist\n",
                "    os.makedirs(output_folder_path, exist_ok=True)\n",
                "    \n",
                "    # Process each file according to its configuration\n",
                "    for file_config in config:\n",
                "        file_name = file_config[\"Congestion Management folder\"]\n",
                "        file_path = os.path.join(input_folder_path, file_name)\n",
                "        \n",
                "        if os.path.exists(file_path):\n",
                "            print(f\"Processing {file_name}...\")\n",
                "            result_df = process_file(file_path, file_config)\n",
                "            \n",
                "            if result_df is not None:\n",
                "                # Generate output filename - replace the original extension with '_pivoted.csv'\n",
                "                base_name = os.path.splitext(file_name)[0]\n",
                "                output_file = f\"{base_name}_pivoted.csv\"\n",
                "                output_path = os.path.join(output_folder_path, output_file)\n",
                "                \n",
                "                # Save individual pivoted file\n",
                "                result_df.to_csv(output_path)\n",
                "                \n",
                "                # Print statistics for this file\n",
                "                print(f\"  Saved to {output_path}\")\n",
                "                print(f\"  Shape: {result_df.shape}\")\n",
                "                print(f\"  Date range: {result_df.index.min()} to {result_df.index.max()}\")\n",
                "                print(f\"  Missing values: {result_df.isna().sum().sum()}/{result_df.size} \" \n",
                "                     f\"({100*result_df.isna().sum().sum()/result_df.size:.2f}%)\")\n",
                "                print(f\"  Sample columns: {list(result_df.columns)[:3]}\")\n",
                "                print(\"\")\n",
                "        else:\n",
                "            print(f\"File not found: {file_path}\")\n",
                "    \n",
                "    print(\"\\nAll files processed and saved to individual pivoted CSVs in:\")\n",
                "    print(output_folder_path)\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "import json\n",
                "\n",
                "config = [\n",
                "    # {\n",
                "    #     \"Generation folder\": \"ActualGenerationOutputPerGenerationUnit_16.1.A_r2.1_SE.csv\",\n",
                "    #     \"Index/date column name\": \"DateTime (UTC)\",\n",
                "    #     \"Merging column\": \"MapCode, GenerationUnitName\",\n",
                "    #     \"Keep columns\": \"ResolutionCode, AreaTypeCode, ActualGenerationOutput(MW), GenerationUnitInstalledCapacity(MW)\"\n",
                "    # },\n",
                "    {\n",
                "        \"Generation folder\": \"AggregatedFillingRateOfWaterReservoirsAndHydroStoragePlants_16.1.D_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, StoredEnergy\"\n",
                "    },\n",
                "    {\n",
                "        \"Generation folder\": \"AggregatedGenerationPerType_16.1.B_C_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, ProductionType, ActualGenerationOutput\"\n",
                "    },\n",
                "    {\n",
                "        \"Generation folder\": \"CurrentGenerationForecastForWindAndSolar_14.1.D_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, ProductionType, AggregatedGenerationForecast\"\n",
                "    },\n",
                "    {\n",
                "        \"Generation folder\": \"DayAheadAggregatedGeneration_14.1.C_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, ScheduledGeneration\"\n",
                "    },\n",
                "    {\n",
                "        \"Generation folder\": \"DayAheadGenerationForecastForWindAndSolar_14.1.D_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, ProductionType, AggregatedGenerationForecast\"\n",
                "    },\n",
                "    {\n",
                "        \"Generation folder\": \"InstalledGenerationCapacityAggregated_14.1.A_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, ProductionType, AggregatedInstalledCapacity\"\n",
                "    },\n",
                "    {\n",
                "        \"Generation folder\": \"IntradayGenerationForecastForWindAndSolar_14.1.D_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, ProductionType, AggregatedGenerationForecast\"\n",
                "    }\n",
                "    \n",
                "\n",
                "]\n",
                "\n",
                "\n",
                "\n",
                "def process_file(file_path, file_config):\n",
                "    \"\"\"\n",
                "    Process a single file to convert from long format to wide format\n",
                "    based on the specified configuration.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Try reading with UTF-8\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "    except UnicodeDecodeError:\n",
                "        try:\n",
                "            # If UTF-8 fails, try 'latin1'\n",
                "            df = pd.read_csv(file_path, encoding='latin1', low_memory=False)\n",
                "        except Exception as e:\n",
                "            print(f\"Error reading {file_path}: {str(e)}\")\n",
                "            return None\n",
                "    \n",
                "    # Extract configuration\n",
                "    date_col = file_config[\"Index/date column name\"]\n",
                "    id_cols = [col.strip() for col in file_config[\"Merging column\"].split(\",\")]\n",
                "    value_cols = [col.strip() for col in file_config[\"Keep columns\"].split(\",\")]\n",
                "    \n",
                "    # Check if all required columns exist\n",
                "    missing_cols = set(id_cols + [date_col] + value_cols) - set(df.columns)\n",
                "    if missing_cols:\n",
                "        print(f\"Warning: Missing columns in {file_path}: {missing_cols}\")\n",
                "        # Only keep columns that exist\n",
                "        id_cols = [col for col in id_cols if col in df.columns]\n",
                "        value_cols = [col for col in value_cols if col in df.columns]\n",
                "    \n",
                "    # Only keep needed columns\n",
                "    cols_to_keep = [date_col] + id_cols + value_cols\n",
                "    df = df[cols_to_keep]\n",
                "    \n",
                "    # Convert date column to datetime\n",
                "    df[date_col] = pd.to_datetime(df[date_col])\n",
                "    \n",
                "    # Create a combined ID from the merging columns\n",
                "    if len(id_cols) > 0:\n",
                "        df['id'] = df[id_cols].astype(str).agg('_'.join, axis=1)\n",
                "    else:\n",
                "        # If no ID columns specified, use a constant\n",
                "        df['id'] = 'value'\n",
                "    \n",
                "    # Check for and report duplicates\n",
                "    duplicate_check = df.duplicated(subset=[date_col, 'id'], keep=False)\n",
                "    if duplicate_check.any():\n",
                "        dup_count = duplicate_check.sum()\n",
                "        print(f\"  Warning: Found {dup_count} duplicate entries with the same date and ID combination\")\n",
                "        \n",
                "        # Display a sample of duplicates for debugging\n",
                "        if dup_count > 0:\n",
                "            print(\"  Sample of duplicates:\")\n",
                "            duplicates = df[duplicate_check].sort_values(by=[date_col, 'id']).head(min(5, dup_count))\n",
                "            print(duplicates[[date_col, 'id'] + value_cols])\n",
                "        \n",
                "        # Detect numeric and non-numeric columns\n",
                "        numeric_cols = []\n",
                "        non_numeric_cols = []\n",
                "        \n",
                "        for col in value_cols:\n",
                "            try:\n",
                "                pd.to_numeric(df[col])\n",
                "                numeric_cols.append(col)\n",
                "            except (ValueError, TypeError):\n",
                "                non_numeric_cols.append(col)\n",
                "        \n",
                "        # Report column types\n",
                "        if numeric_cols:\n",
                "            print(f\"  Numeric columns that will be averaged: {numeric_cols}\")\n",
                "        if non_numeric_cols:\n",
                "            print(f\"  Non-numeric columns that will use 'first' value: {non_numeric_cols}\")\n",
                "        \n",
                "        # Create an aggregation dictionary\n",
                "        agg_dict = {}\n",
                "        for col in numeric_cols:\n",
                "            agg_dict[col] = 'mean'\n",
                "        for col in non_numeric_cols:\n",
                "            agg_dict[col] = 'first'\n",
                "        \n",
                "        # Apply the appropriate aggregation to each column\n",
                "        print(\"  Aggregating duplicate values with mixed methods (mean for numeric, first for non-numeric)\")\n",
                "        df = df.groupby([date_col, 'id']).agg(agg_dict).reset_index()\n",
                "    \n",
                "    # Create a wide-format dataframe\n",
                "    result = pd.DataFrame(index=df[date_col].unique())\n",
                "    result.index = pd.to_datetime(result.index)\n",
                "    result.sort_index(inplace=True)\n",
                "    result.index.name = date_col\n",
                "    \n",
                "    # Get file type (simple name without version info)\n",
                "    file_type = os.path.basename(file_path).split('_')[0]\n",
                "    \n",
                "    # Process each value column\n",
                "    for col in value_cols:\n",
                "        # Pivot to get values for each ID\n",
                "        pivot = df.pivot(index=date_col, columns='id', values=col)\n",
                "        \n",
                "        # Rename columns with a more descriptive format: FileType_ID_Column  \n",
                "        pivot.columns = [f\"{id_val}_{col}\" for id_val in pivot.columns]\n",
                "        \n",
                "        # Join with the result\n",
                "        result = result.join(pivot)\n",
                "    \n",
                "    # Filter out columns containing \"_SEM_\"\n",
                "    sem_columns = [col for col in result.columns if \"_SEM_\" in col]\n",
                "    if sem_columns:\n",
                "        print(f\"  Removing {len(sem_columns)} columns containing '_SEM_'\")\n",
                "        result = result.drop(columns=sem_columns)\n",
                "    \n",
                "    print(f\"  Processed {file_path} into {result.shape[1]} columns\")\n",
                "    return result\n",
                "\n",
                "def main():\n",
                "    # Main processing logic\n",
                "    # input_folder_path = \"../../processed_data/grouped_data/Generation\"\n",
                "    # output_folder_path = \"../../processed_data/pivoted_data/Generation\"\n",
                "    input_folder_path = Path(\"filtered_data/Generation\")\n",
                "    output_folder_path = Path(\"pivoted_data/Generation\")\n",
                "    \n",
                "    \n",
                "    # Create output directory if it doesn't exist\n",
                "    os.makedirs(output_folder_path, exist_ok=True)\n",
                "    \n",
                "    # Process each file according to its configuration\n",
                "    for file_config in config:\n",
                "        file_name = file_config[\"Generation folder\"]\n",
                "        file_path = os.path.join(input_folder_path, file_name)\n",
                "        \n",
                "        if os.path.exists(file_path):\n",
                "            print(f\"Processing {file_name}...\")\n",
                "            result_df = process_file(file_path, file_config)\n",
                "            \n",
                "            if result_df is not None:\n",
                "                # Generate output filename - replace the original extension with '_pivoted.csv'\n",
                "                base_name = os.path.splitext(file_name)[0]\n",
                "                output_file = f\"{base_name}_pivoted.csv\"\n",
                "                output_path = os.path.join(output_folder_path, output_file)\n",
                "                \n",
                "                # Save individual pivoted file\n",
                "                result_df.to_csv(output_path)\n",
                "                \n",
                "                # Print statistics for this file\n",
                "                print(f\"  Saved to {output_path}\")\n",
                "                print(f\"  Shape: {result_df.shape}\")\n",
                "                print(f\"  Date range: {result_df.index.min()} to {result_df.index.max()}\")\n",
                "                print(f\"  Missing values: {result_df.isna().sum().sum()}/{result_df.size} \" \n",
                "                     f\"({100*result_df.isna().sum().sum()/result_df.size:.2f}%)\")\n",
                "                print(f\"  Sample columns: {list(result_df.columns)[:3]}\")\n",
                "                print(\"\")\n",
                "        else:\n",
                "            print(f\"File not found: {file_path}\")\n",
                "    \n",
                "    print(\"\\nAll files processed and saved to individual pivoted CSVs in:\")\n",
                "    print(output_folder_path)\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import pandas as pd\n",
                "import os\n",
                "import json\n",
                "\n",
                "# Configuration for each file\n",
                "# Load folder\n",
                "\n",
                "config = [\n",
                "    {\n",
                "        \"Load folder\": \"ActualTotalLoad_6.1.A_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, TotalLoadValue\"\n",
                "    },\n",
                "    {\n",
                "        \"Load folder\": \"DayAheadTotalLoadForecast_6.1.B_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, TotalLoadValue\"\n",
                "    },\n",
                "    {\n",
                "        \"Load folder\": \"MonthAheadTotalLoadForecast_6.1.D_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, MinimumTotalLoadValue, MaximumTotalLoadValue\"\n",
                "    },\n",
                "    {\n",
                "        \"Load folder\": \"WeekAheadTotalLoadForecast_6.1.C_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, MinimumTotalLoadValue, MaximumTotalLoadValue\"\n",
                "    },\n",
                "    {\n",
                "        \"Load folder\": \"YearAheadTotalLoadForecast_6.1.E_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, MinimumTotalLoadValue, MaximumTotalLoadValue\"\n",
                "    },\n",
                "    {\n",
                "        \"Load folder\": \"YearAheadForecastMargin_8.1_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, ForecastedMargin\"\n",
                "    }\n",
                "\n",
                "]\n",
                "\n",
                "\n",
                "\n",
                "def process_file(file_path, file_config):\n",
                "    \"\"\"\n",
                "    Process a single file to convert from long format to wide format\n",
                "    based on the specified configuration.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Try reading with UTF-8\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "    except UnicodeDecodeError:\n",
                "        try:\n",
                "            # If UTF-8 fails, try 'latin1'\n",
                "            df = pd.read_csv(file_path, encoding='latin1', low_memory=False)\n",
                "        except Exception as e:\n",
                "            print(f\"Error reading {file_path}: {str(e)}\")\n",
                "            return None\n",
                "    \n",
                "    # Extract configuration\n",
                "    date_col = file_config[\"Index/date column name\"]\n",
                "    id_cols = [col.strip() for col in file_config[\"Merging column\"].split(\",\")]\n",
                "    value_cols = [col.strip() for col in file_config[\"Keep columns\"].split(\",\")]\n",
                "    \n",
                "    # Check if all required columns exist\n",
                "    missing_cols = set(id_cols + [date_col] + value_cols) - set(df.columns)\n",
                "    if missing_cols:\n",
                "        print(f\"Warning: Missing columns in {file_path}: {missing_cols}\")\n",
                "        # Only keep columns that exist\n",
                "        id_cols = [col for col in id_cols if col in df.columns]\n",
                "        value_cols = [col for col in value_cols if col in df.columns]\n",
                "    \n",
                "    # Only keep needed columns\n",
                "    cols_to_keep = [date_col] + id_cols + value_cols\n",
                "    df = df[cols_to_keep]\n",
                "    \n",
                "    # Convert date column to datetime\n",
                "    df[date_col] = pd.to_datetime(df[date_col])\n",
                "    \n",
                "    # Create a combined ID from the merging columns\n",
                "    if len(id_cols) > 0:\n",
                "        df['id'] = df[id_cols].astype(str).agg('_'.join, axis=1)\n",
                "    else:\n",
                "        # If no ID columns specified, use a constant\n",
                "        df['id'] = 'value'\n",
                "    \n",
                "    # Check for and report duplicates\n",
                "    duplicate_check = df.duplicated(subset=[date_col, 'id'], keep=False)\n",
                "    if duplicate_check.any():\n",
                "        dup_count = duplicate_check.sum()\n",
                "        print(f\"  Warning: Found {dup_count} duplicate entries with the same date and ID combination\")\n",
                "        \n",
                "        # Display a sample of duplicates for debugging\n",
                "        if dup_count > 0:\n",
                "            print(\"  Sample of duplicates:\")\n",
                "            duplicates = df[duplicate_check].sort_values(by=[date_col, 'id']).head(min(5, dup_count))\n",
                "            print(duplicates[[date_col, 'id'] + value_cols])\n",
                "        \n",
                "        # Detect numeric and non-numeric columns\n",
                "        numeric_cols = []\n",
                "        non_numeric_cols = []\n",
                "        \n",
                "        for col in value_cols:\n",
                "            try:\n",
                "                pd.to_numeric(df[col])\n",
                "                numeric_cols.append(col)\n",
                "            except (ValueError, TypeError):\n",
                "                non_numeric_cols.append(col)\n",
                "        \n",
                "        # Report column types\n",
                "        if numeric_cols:\n",
                "            print(f\"  Numeric columns that will be averaged: {numeric_cols}\")\n",
                "        if non_numeric_cols:\n",
                "            print(f\"  Non-numeric columns that will use 'first' value: {non_numeric_cols}\")\n",
                "        \n",
                "        # Create an aggregation dictionary\n",
                "        agg_dict = {}\n",
                "        for col in numeric_cols:\n",
                "            agg_dict[col] = 'mean'\n",
                "        for col in non_numeric_cols:\n",
                "            agg_dict[col] = 'first'\n",
                "        \n",
                "        # Apply the appropriate aggregation to each column\n",
                "        print(\"  Aggregating duplicate values with mixed methods (mean for numeric, first for non-numeric)\")\n",
                "        df = df.groupby([date_col, 'id']).agg(agg_dict).reset_index()\n",
                "    \n",
                "    # Create a wide-format dataframe\n",
                "    result = pd.DataFrame(index=df[date_col].unique())\n",
                "    result.index = pd.to_datetime(result.index)\n",
                "    result.sort_index(inplace=True)\n",
                "    result.index.name = date_col\n",
                "    \n",
                "    # Get file type (simple name without version info)\n",
                "    file_type = os.path.basename(file_path).split('_')[0]\n",
                "    \n",
                "    # Process each value column\n",
                "    for col in value_cols:\n",
                "        # Pivot to get values for each ID\n",
                "        pivot = df.pivot(index=date_col, columns='id', values=col)\n",
                "        \n",
                "        # Rename columns with a more descriptive format: FileType_ID_Column  \n",
                "        pivot.columns = [f\"{id_val}_{col}\" for id_val in pivot.columns]\n",
                "        \n",
                "        # Join with the result\n",
                "        result = result.join(pivot)\n",
                "    \n",
                "    # Filter out columns containing \"_SEM_\"\n",
                "    sem_columns = [col for col in result.columns if \"_SEM_\" in col]\n",
                "    if sem_columns:\n",
                "        print(f\"  Removing {len(sem_columns)} columns containing '_SEM_'\")\n",
                "        result = result.drop(columns=sem_columns)\n",
                "    \n",
                "    print(f\"  Processed {file_path} into {result.shape[1]} columns\")\n",
                "    return result\n",
                "\n",
                "def main():\n",
                "    # Main processing logic\n",
                "    # input_folder_path = \"../../processed_data/grouped_data/Load\"\n",
                "    # output_folder_path = \"../../processed_data/pivoted_data/Load\"\n",
                "    input_folder_path = Path(\"filtered_data/Load\")\n",
                "    output_folder_path = Path(\"pivoted_data/Load\")\n",
                "    \n",
                "    \n",
                "    # Create output directory if it doesn't exist\n",
                "    os.makedirs(output_folder_path, exist_ok=True)\n",
                "    \n",
                "    # Process each file according to its configuration\n",
                "    for file_config in config:\n",
                "        file_name = file_config[\"Load folder\"]\n",
                "        file_path = os.path.join(input_folder_path, file_name)\n",
                "        \n",
                "        if os.path.exists(file_path):\n",
                "            print(f\"Processing {file_name}...\")\n",
                "            result_df = process_file(file_path, file_config)\n",
                "            \n",
                "            if result_df is not None:\n",
                "                # Generate output filename - replace the original extension with '_pivoted.csv'\n",
                "                base_name = os.path.splitext(file_name)[0]\n",
                "                output_file = f\"{base_name}_pivoted.csv\"\n",
                "                output_path = os.path.join(output_folder_path, output_file)\n",
                "                \n",
                "                # Save individual pivoted file\n",
                "                result_df.to_csv(output_path)\n",
                "                \n",
                "                # Print statistics for this file\n",
                "                print(f\"  Saved to {output_path}\")\n",
                "                print(f\"  Shape: {result_df.shape}\")\n",
                "                print(f\"  Date range: {result_df.index.min()} to {result_df.index.max()}\")\n",
                "                print(f\"  Missing values: {result_df.isna().sum().sum()}/{result_df.size} \" \n",
                "                     f\"({100*result_df.isna().sum().sum()/result_df.size:.2f}%)\")\n",
                "                print(f\"  Sample columns: {list(result_df.columns)[:3]}\")\n",
                "                print(\"\")\n",
                "        else:\n",
                "            print(f\"File not found: {file_path}\")\n",
                "    \n",
                "    print(\"\\nAll files processed and saved to individual pivoted CSVs in:\")\n",
                "    print(output_folder_path)\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Outages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import pandas as pd\n",
                "import os\n",
                "import json\n",
                "\n",
                "# Configuration for each file\n",
                "# Outages folder\n",
                "\n",
                "config = [\n",
                "    {\n",
                "        \"Outages folder\": \"ChangesInActualAvailabilityOfConsumptionUnits_7.1.B_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"MapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, UnavailableCapacity\"\n",
                "    }\n",
                "        \n",
                "\n",
                "]\n",
                "\n",
                "\n",
                "\n",
                "def process_file(file_path, file_config):\n",
                "    \"\"\"\n",
                "    Process a single file to convert from long format to wide format\n",
                "    based on the specified configuration.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Try reading with UTF-8\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "    except UnicodeDecodeError:\n",
                "        try:\n",
                "            # If UTF-8 fails, try 'latin1'\n",
                "            df = pd.read_csv(file_path, encoding='latin1', low_memory=False)\n",
                "        except Exception as e:\n",
                "            print(f\"Error reading {file_path}: {str(e)}\")\n",
                "            return None\n",
                "    \n",
                "    # Extract configuration\n",
                "    date_col = file_config[\"Index/date column name\"]\n",
                "    id_cols = [col.strip() for col in file_config[\"Merging column\"].split(\",\")]\n",
                "    value_cols = [col.strip() for col in file_config[\"Keep columns\"].split(\",\")]\n",
                "    \n",
                "    # Check if all required columns exist\n",
                "    missing_cols = set(id_cols + [date_col] + value_cols) - set(df.columns)\n",
                "    if missing_cols:\n",
                "        print(f\"Warning: Missing columns in {file_path}: {missing_cols}\")\n",
                "        # Only keep columns that exist\n",
                "        id_cols = [col for col in id_cols if col in df.columns]\n",
                "        value_cols = [col for col in value_cols if col in df.columns]\n",
                "    \n",
                "    # Only keep needed columns\n",
                "    cols_to_keep = [date_col] + id_cols + value_cols\n",
                "    df = df[cols_to_keep]\n",
                "    \n",
                "    # Convert date column to datetime\n",
                "    df[date_col] = pd.to_datetime(df[date_col])\n",
                "    \n",
                "    # Create a combined ID from the merging columns\n",
                "    if len(id_cols) > 0:\n",
                "        df['id'] = df[id_cols].astype(str).agg('_'.join, axis=1)\n",
                "    else:\n",
                "        # If no ID columns specified, use a constant\n",
                "        df['id'] = 'value'\n",
                "    \n",
                "    # Check for and report duplicates\n",
                "    duplicate_check = df.duplicated(subset=[date_col, 'id'], keep=False)\n",
                "    if duplicate_check.any():\n",
                "        dup_count = duplicate_check.sum()\n",
                "        print(f\"  Warning: Found {dup_count} duplicate entries with the same date and ID combination\")\n",
                "        \n",
                "        # Display a sample of duplicates for debugging\n",
                "        if dup_count > 0:\n",
                "            print(\"  Sample of duplicates:\")\n",
                "            duplicates = df[duplicate_check].sort_values(by=[date_col, 'id']).head(min(5, dup_count))\n",
                "            print(duplicates[[date_col, 'id'] + value_cols])\n",
                "        \n",
                "        # Detect numeric and non-numeric columns\n",
                "        numeric_cols = []\n",
                "        non_numeric_cols = []\n",
                "        \n",
                "        for col in value_cols:\n",
                "            try:\n",
                "                pd.to_numeric(df[col])\n",
                "                numeric_cols.append(col)\n",
                "            except (ValueError, TypeError):\n",
                "                non_numeric_cols.append(col)\n",
                "        \n",
                "        # Report column types\n",
                "        if numeric_cols:\n",
                "            print(f\"  Numeric columns that will be averaged: {numeric_cols}\")\n",
                "        if non_numeric_cols:\n",
                "            print(f\"  Non-numeric columns that will use 'first' value: {non_numeric_cols}\")\n",
                "        \n",
                "        # Create an aggregation dictionary\n",
                "        agg_dict = {}\n",
                "        for col in numeric_cols:\n",
                "            agg_dict[col] = 'mean'\n",
                "        for col in non_numeric_cols:\n",
                "            agg_dict[col] = 'first'\n",
                "        \n",
                "        # Apply the appropriate aggregation to each column\n",
                "        print(\"  Aggregating duplicate values with mixed methods (mean for numeric, first for non-numeric)\")\n",
                "        df = df.groupby([date_col, 'id']).agg(agg_dict).reset_index()\n",
                "    \n",
                "    # Create a wide-format dataframe\n",
                "    result = pd.DataFrame(index=df[date_col].unique())\n",
                "    result.index = pd.to_datetime(result.index)\n",
                "    result.sort_index(inplace=True)\n",
                "    result.index.name = date_col\n",
                "    \n",
                "    # Get file type (simple name without version info)\n",
                "    file_type = os.path.basename(file_path).split('_')[0]\n",
                "    \n",
                "    # Process each value column\n",
                "    for col in value_cols:\n",
                "        # Pivot to get values for each ID\n",
                "        pivot = df.pivot(index=date_col, columns='id', values=col)\n",
                "        \n",
                "        # Rename columns with a more descriptive format: FileType_ID_Column  \n",
                "        pivot.columns = [f\"{id_val}_{col}\" for id_val in pivot.columns]\n",
                "        \n",
                "        # Join with the result\n",
                "        result = result.join(pivot)\n",
                "    \n",
                "    # Filter out columns containing \"_SEM_\"\n",
                "    sem_columns = [col for col in result.columns if \"_SEM_\" in col]\n",
                "    if sem_columns:\n",
                "        print(f\"  Removing {len(sem_columns)} columns containing '_SEM_'\")\n",
                "        result = result.drop(columns=sem_columns)\n",
                "    \n",
                "    print(f\"  Processed {file_path} into {result.shape[1]} columns\")\n",
                "    return result\n",
                "\n",
                "def main():\n",
                "    # Main processing logic\n",
                "    # input_folder_path = \"../../processed_data/grouped_data/Outages\"\n",
                "    # output_folder_path = \"../../processed_data/pivoted_data/Outages\"\n",
                "    \n",
                "    input_folder_path = Path(\"filtered_data/Outages\")\n",
                "    output_folder_path = Path(\"pivoted_data/Outages\")\n",
                "    \n",
                "    \n",
                "    # Create output directory if it doesn't exist\n",
                "    os.makedirs(output_folder_path, exist_ok=True)\n",
                "    \n",
                "    # Process each file according to its configuration\n",
                "    for file_config in config:\n",
                "        file_name = file_config[\"Outages folder\"]\n",
                "        file_path = os.path.join(input_folder_path, file_name)\n",
                "        \n",
                "        if os.path.exists(file_path):\n",
                "            print(f\"Processing {file_name}...\")\n",
                "            result_df = process_file(file_path, file_config)\n",
                "            \n",
                "            if result_df is not None:\n",
                "                # Generate output filename - replace the original extension with '_pivoted.csv'\n",
                "                base_name = os.path.splitext(file_name)[0]\n",
                "                output_file = f\"{base_name}_pivoted.csv\"\n",
                "                output_path = os.path.join(output_folder_path, output_file)\n",
                "                \n",
                "                # Save individual pivoted file\n",
                "                result_df.to_csv(output_path)\n",
                "                \n",
                "                # Print statistics for this file\n",
                "                print(f\"  Saved to {output_path}\")\n",
                "                print(f\"  Shape: {result_df.shape}\")\n",
                "                print(f\"  Date range: {result_df.index.min()} to {result_df.index.max()}\")\n",
                "                print(f\"  Missing values: {result_df.isna().sum().sum()}/{result_df.size} \" \n",
                "                     f\"({100*result_df.isna().sum().sum()/result_df.size:.2f}%)\")\n",
                "                print(f\"  Sample columns: {list(result_df.columns)[:3]}\")\n",
                "                print(\"\")\n",
                "        else:\n",
                "            print(f\"File not found: {file_path}\")\n",
                "    \n",
                "    print(\"\\nAll files processed and saved to individual pivoted CSVs in:\")\n",
                "    print(output_folder_path)\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transmission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "import pandas as pd\n",
                "import os\n",
                "import json\n",
                "\n",
                "# Configuration for each file\n",
                "# Transmission folder\n",
                "\n",
                "config = [\n",
                "    {\n",
                "        \"Transmission folder\": \"ImplicitAllocationsNetPositions_12.1.E_r3_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime(UTC)\",\n",
                "        \"Merging column\": \"MapCode, ContractType, Direction\",\n",
                "        \"Keep columns\": \"ResolutionCode, AreaTypeCode, NetPosition[MW]\"\n",
                "    },\n",
                "\n",
                "\n",
                "]\n",
                "\n",
                "\n",
                "\n",
                "def process_file(file_path, file_config):\n",
                "    \"\"\"\n",
                "    Process a single file to convert from long format to wide format\n",
                "    based on the specified configuration.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Try reading with UTF-8\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "    except UnicodeDecodeError:\n",
                "        try:\n",
                "            # If UTF-8 fails, try 'latin1'\n",
                "            df = pd.read_csv(file_path, encoding='latin1', low_memory=False)\n",
                "        except Exception as e:\n",
                "            print(f\"Error reading {file_path}: {str(e)}\")\n",
                "            return None\n",
                "    \n",
                "    # Extract configuration\n",
                "    date_col = file_config[\"Index/date column name\"]\n",
                "    id_cols = [col.strip() for col in file_config[\"Merging column\"].split(\",\")]\n",
                "    value_cols = [col.strip() for col in file_config[\"Keep columns\"].split(\",\")]\n",
                "    \n",
                "    # Check if all required columns exist\n",
                "    missing_cols = set(id_cols + [date_col] + value_cols) - set(df.columns)\n",
                "    if missing_cols:\n",
                "        print(f\"Warning: Missing columns in {file_path}: {missing_cols}\")\n",
                "        # Only keep columns that exist\n",
                "        id_cols = [col for col in id_cols if col in df.columns]\n",
                "        value_cols = [col for col in value_cols if col in df.columns]\n",
                "    \n",
                "    # Only keep needed columns\n",
                "    cols_to_keep = [date_col] + id_cols + value_cols\n",
                "    df = df[cols_to_keep]\n",
                "    \n",
                "    # Convert date column to datetime\n",
                "    df[date_col] = pd.to_datetime(df[date_col])\n",
                "    \n",
                "    # Create a combined ID from the merging columns\n",
                "    if len(id_cols) > 0:\n",
                "        df['id'] = df[id_cols].astype(str).agg('_'.join, axis=1)\n",
                "    else:\n",
                "        # If no ID columns specified, use a constant\n",
                "        df['id'] = 'value'\n",
                "    \n",
                "    # Check for and report duplicates\n",
                "    duplicate_check = df.duplicated(subset=[date_col, 'id'], keep=False)\n",
                "    if duplicate_check.any():\n",
                "        dup_count = duplicate_check.sum()\n",
                "        print(f\"  Warning: Found {dup_count} duplicate entries with the same date and ID combination\")\n",
                "        \n",
                "        # Display a sample of duplicates for debugging\n",
                "        if dup_count > 0:\n",
                "            print(\"  Sample of duplicates:\")\n",
                "            duplicates = df[duplicate_check].sort_values(by=[date_col, 'id']).head(min(5, dup_count))\n",
                "            print(duplicates[[date_col, 'id'] + value_cols])\n",
                "        \n",
                "        # Detect numeric and non-numeric columns\n",
                "        numeric_cols = []\n",
                "        non_numeric_cols = []\n",
                "        \n",
                "        for col in value_cols:\n",
                "            try:\n",
                "                pd.to_numeric(df[col])\n",
                "                numeric_cols.append(col)\n",
                "            except (ValueError, TypeError):\n",
                "                non_numeric_cols.append(col)\n",
                "        \n",
                "        # Report column types\n",
                "        if numeric_cols:\n",
                "            print(f\"  Numeric columns that will be averaged: {numeric_cols}\")\n",
                "        if non_numeric_cols:\n",
                "            print(f\"  Non-numeric columns that will use 'first' value: {non_numeric_cols}\")\n",
                "        \n",
                "        # Create an aggregation dictionary\n",
                "        agg_dict = {}\n",
                "        for col in numeric_cols:\n",
                "            agg_dict[col] = 'mean'\n",
                "        for col in non_numeric_cols:\n",
                "            agg_dict[col] = 'first'\n",
                "        \n",
                "        # Apply the appropriate aggregation to each column\n",
                "        print(\"  Aggregating duplicate values with mixed methods (mean for numeric, first for non-numeric)\")\n",
                "        df = df.groupby([date_col, 'id']).agg(agg_dict).reset_index()\n",
                "    \n",
                "    # Create a wide-format dataframe\n",
                "    result = pd.DataFrame(index=df[date_col].unique())\n",
                "    result.index = pd.to_datetime(result.index)\n",
                "    result.sort_index(inplace=True)\n",
                "    result.index.name = date_col\n",
                "    \n",
                "    # Get file type (simple name without version info)\n",
                "    file_type = os.path.basename(file_path).split('_')[0]\n",
                "    \n",
                "    # Process each value column\n",
                "    for col in value_cols:\n",
                "        # Pivot to get values for each ID\n",
                "        pivot = df.pivot(index=date_col, columns='id', values=col)\n",
                "        \n",
                "        # Rename columns with a more descriptive format: FileType_ID_Column  \n",
                "        pivot.columns = [f\"{id_val}_{col}\" for id_val in pivot.columns]\n",
                "        \n",
                "        # Join with the result\n",
                "        result = result.join(pivot)\n",
                "    \n",
                "    # Filter out columns containing \"_SEM_\"\n",
                "    sem_columns = [col for col in result.columns if \"_SEM_\" in col]\n",
                "    if sem_columns:\n",
                "        print(f\"  Removing {len(sem_columns)} columns containing '_SEM_'\")\n",
                "        result = result.drop(columns=sem_columns)\n",
                "    \n",
                "    print(f\"  Processed {file_path} into {result.shape[1]} columns\")\n",
                "    return result\n",
                "\n",
                "def main():\n",
                "    # Main processing logic\n",
                "    input_folder_path = Path(\"filtered_data/Transmission\")\n",
                "    output_folder_path = Path(\"pivoted_data/Transmission\")\n",
                "    \n",
                "    \n",
                "    # Create output directory if it doesn't exist\n",
                "    os.makedirs(output_folder_path, exist_ok=True)\n",
                "    \n",
                "    # Process each file according to its configuration\n",
                "    for file_config in config:\n",
                "        file_name = file_config[\"Transmission folder\"]\n",
                "        file_path = os.path.join(input_folder_path, file_name)\n",
                "        \n",
                "        if os.path.exists(file_path):\n",
                "            print(f\"Processing {file_name}...\")\n",
                "            result_df = process_file(file_path, file_config)\n",
                "            \n",
                "            if result_df is not None:\n",
                "                # Generate output filename - replace the original extension with '_pivoted.csv'\n",
                "                base_name = os.path.splitext(file_name)[0]\n",
                "                output_file = f\"{base_name}_pivoted.csv\"\n",
                "                output_path = os.path.join(output_folder_path, output_file)\n",
                "                \n",
                "                # Save individual pivoted file\n",
                "                result_df.to_csv(output_path)\n",
                "                \n",
                "                # Print statistics for this file\n",
                "                print(f\"  Saved to {output_path}\")\n",
                "                print(f\"  Shape: {result_df.shape}\")\n",
                "                print(f\"  Date range: {result_df.index.min()} to {result_df.index.max()}\")\n",
                "                print(f\"  Missing values: {result_df.isna().sum().sum()}/{result_df.size} \" \n",
                "                     f\"({100*result_df.isna().sum().sum()/result_df.size:.2f}%)\")\n",
                "                print(f\"  Sample columns: {list(result_df.columns)[:3]}\")\n",
                "                print(\"\")\n",
                "        else:\n",
                "            print(f\"File not found: {file_path}\")\n",
                "    \n",
                "    print(\"\\nAll files processed and saved to individual pivoted CSVs in:\")\n",
                "    print(output_folder_path)\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "\n",
                "# Configuration for the file\n",
                "config = [\n",
                "\n",
                "    {\n",
                "        \"Transmission folder\": \"FlowBasedAllocationsCongestionincomeDaily_12.1.E_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"OutMapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, Revenue\"\n",
                "    },\n",
                "    {\n",
                "        \"Transmission folder\": \"ForecastedMonthAheadTransferCapacities_11.1_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"OutMapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, ForecastTransferCapacity\"\n",
                "    },\n",
                "    {\n",
                "        \"Transmission folder\": \"ForecastedWeekAheadTransferCapacities_11.1_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"OutMapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, ForecastTransferCapacity\"\n",
                "    },\n",
                "    {\n",
                "        \"Transmission folder\": \"ForecastedYearAheadTransferCapacities_11.1_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"OutMapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, ForecastTransferCapacity\"\n",
                "    },\n",
                "    {\n",
                "        \"Transmission folder\": \"ImplicitAllocationsCongestionIncomeDaily_12.1.E_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"OutMapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, Revenue\"\n",
                "    },\n",
                "    {\n",
                "        \"Transmission folder\": \"OfferedDayAheadTransferCapacitylmplicit_11.1_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"OutMapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, Capacity\"\n",
                "    },\n",
                "    {\n",
                "        \"Transmission folder\": \"OfferedIntradayTransferCapacityImplicit_11.1_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"OutMapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, Capacity\"\n",
                "    },\n",
                "    {\n",
                "        \"Transmission folder\": \"OfferedTransferCapacitiesContinuous_11.1_r3_SE.csv\",\n",
                "        \"Index/date column name\": \"MTU(UTC)\",\n",
                "        \"Merging column\": \"OutMapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, ContractType, Capacity[MW]\"\n",
                "    },\n",
                "    {\n",
                "        \"Transmission folder\": \"OfferedTransferCapacitiesContinuousEvolution_11.1_r3_SE.csv\",\n",
                "        \"Index/date column name\": \"MTU(UTC)\",\n",
                "        \"Merging column\": \"OutMapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, ContractType, Capacity[MW]\"\n",
                "    },\n",
                "    {\n",
                "        \"Transmission folder\": \"PhysicalFlows_12.1.G_SE.csv\",\n",
                "        \"Index/date column name\": \"DateTime\",\n",
                "        \"Merging column\": \"OutMapCode\",\n",
                "        \"Keep columns\": \"ResolutionCode, FlowValue\"\n",
                "    }\n",
                "\n",
                "]\n",
                "\n",
                "def process_file(file_path, file_config):\n",
                "    \"\"\"\n",
                "    Process a single file to convert from long format to wide format\n",
                "    based on the specified configuration with special handling for SE regions.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Try reading with UTF-8\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "    except UnicodeDecodeError:\n",
                "        try:\n",
                "            # If UTF-8 fails, try 'latin1'\n",
                "            df = pd.read_csv(file_path, encoding='latin1', low_memory=False)\n",
                "        except Exception as e:\n",
                "            print(f\"Error reading {file_path}: {str(e)}\")\n",
                "            return None\n",
                "    \n",
                "    # Extract configuration\n",
                "    date_col = file_config[\"Index/date column name\"]\n",
                "    value_cols = [col.strip() for col in file_config[\"Keep columns\"].split(\",\")]\n",
                "    \n",
                "    # Check if required columns exist\n",
                "    required_cols = [date_col, \"OutMapCode\", \"InMapCode\"] + value_cols\n",
                "    missing_cols = set(required_cols) - set(df.columns)\n",
                "    if missing_cols:\n",
                "        print(f\"Warning: Missing columns in {file_path}: {missing_cols}\")\n",
                "        return None\n",
                "    \n",
                "    # Only keep needed columns\n",
                "    cols_to_keep = [date_col, \"OutMapCode\", \"InMapCode\"] + value_cols\n",
                "    df = df[cols_to_keep]\n",
                "    \n",
                "    # Convert date column to datetime\n",
                "    df[date_col] = pd.to_datetime(df[date_col])\n",
                "    \n",
                "    # Define Swedish bidding zones\n",
                "    swedish_zones = ['SE', 'SE1', 'SE2', 'SE3', 'SE4']\n",
                "    \n",
                "    # Filter for rows where either OutMapCode or InMapCode is a Swedish zone\n",
                "    df_se = df[(df['OutMapCode'].isin(swedish_zones)) | (df['InMapCode'].isin(swedish_zones))]\n",
                "    \n",
                "    # Create a direction-based identifier\n",
                "    df_se['flow_id'] = df_se.apply(\n",
                "        lambda row: f\"FROM_{row['OutMapCode']}_TO_{row['InMapCode']}\", \n",
                "        axis=1\n",
                "    )\n",
                "    \n",
                "    # Check for duplicates with the new identifier\n",
                "    duplicate_check = df_se.duplicated(subset=[date_col, 'flow_id'], keep=False)\n",
                "    if duplicate_check.any():\n",
                "        dup_count = duplicate_check.sum()\n",
                "        print(f\"  Warning: Found {dup_count} duplicate entries with the same date and flow ID\")\n",
                "        \n",
                "        # Detect numeric and non-numeric columns\n",
                "        numeric_cols = []\n",
                "        non_numeric_cols = []\n",
                "        \n",
                "        for col in value_cols:\n",
                "            try:\n",
                "                pd.to_numeric(df_se[col])\n",
                "                numeric_cols.append(col)\n",
                "            except (ValueError, TypeError):\n",
                "                non_numeric_cols.append(col)\n",
                "        \n",
                "        # Create an aggregation dictionary\n",
                "        agg_dict = {}\n",
                "        for col in numeric_cols:\n",
                "            agg_dict[col] = 'mean'\n",
                "        for col in non_numeric_cols:\n",
                "            agg_dict[col] = 'first'\n",
                "        \n",
                "        # Apply the appropriate aggregation to each column\n",
                "        print(\"  Aggregating duplicate values with mixed methods (mean for numeric, first for non-numeric)\")\n",
                "        df_se = df_se.groupby([date_col, 'flow_id']).agg(agg_dict).reset_index()\n",
                "    \n",
                "    # Create a wide-format dataframe\n",
                "    result = pd.DataFrame(index=df_se[date_col].unique())\n",
                "    result.index = pd.to_datetime(result.index)\n",
                "    result.sort_index(inplace=True)\n",
                "    result.index.name = date_col\n",
                "    \n",
                "    # Process each value column\n",
                "    for col in value_cols:\n",
                "        # Pivot to get values for each flow ID\n",
                "        pivot = df_se.pivot(index=date_col, columns='flow_id', values=col)\n",
                "        \n",
                "        # Rename columns with a more descriptive format\n",
                "        pivot.columns = [f\"{flow_id}_{col}\" for flow_id in pivot.columns]\n",
                "        \n",
                "        # Join with the result\n",
                "        result = result.join(pivot)\n",
                "    \n",
                "    print(f\"  Processed {file_path} into {result.shape[1]} columns\")\n",
                "    return result\n",
                "\n",
                "def main():\n",
                "    # Main processing logic\n",
                "    input_folder_path = Path(\"filtered_data/Transmission\")\n",
                "    output_folder_path = Path(\"pivoted_data/Transmission\")\n",
                "    \n",
                "    # Create output directory if it doesn't exist\n",
                "    os.makedirs(output_folder_path, exist_ok=True)\n",
                "    \n",
                "    # Process each file according to its configuration\n",
                "    for file_config in config:\n",
                "        file_name = file_config[\"Transmission folder\"]\n",
                "        file_path = os.path.join(input_folder_path, file_name)\n",
                "        \n",
                "        if os.path.exists(file_path):\n",
                "            print(f\"Processing {file_name}...\")\n",
                "            result_df = process_file(file_path, file_config)\n",
                "            \n",
                "            if result_df is not None:\n",
                "                # Generate output filename - replace the original extension with '_pivoted.csv'\n",
                "                base_name = os.path.splitext(file_name)[0]\n",
                "                output_file = f\"{base_name}_pivoted.csv\"\n",
                "                output_path = os.path.join(output_folder_path, output_file)\n",
                "                \n",
                "                # Save individual pivoted file\n",
                "                result_df.to_csv(output_path)\n",
                "                \n",
                "                # Print statistics for this file\n",
                "                print(f\"  Saved to {output_path}\")\n",
                "                print(f\"  Shape: {result_df.shape}\")\n",
                "                print(f\"  Date range: {result_df.index.min()} to {result_df.index.max()}\")\n",
                "                print(f\"  Missing values: {result_df.isna().sum().sum()}/{result_df.size} \" \n",
                "                     f\"({100*result_df.isna().sum().sum()/result_df.size:.2f}%)\")\n",
                "                print(f\"  Sample columns: {list(result_df.columns)[:3]}\")\n",
                "                print(\"\")\n",
                "        else:\n",
                "            print(f\"File not found: {file_path}\")\n",
                "    \n",
                "    print(\"\\nAll files processed and saved to individual pivoted CSVs in:\")\n",
                "    print(output_folder_path)\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Clean up column names"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import os\n",
                "\n",
                "# Define the source and target directories\n",
                "source_dir = Path(\"pivoted_data\")\n",
                "target_dir = Path(\"pivoted_data_filtered\")\n",
                "\n",
                "# Dictionary for replacements\n",
                "replacements = {\n",
                "    \"Automatic Frequency Restoration Reserve (aFRR)\": \"aFRR\",\n",
                "    \"Manual Frequency Restoration Reserve (mFRR)\": \"mFRR\",\n",
                "    \"Frequency Containment Reserve (FCR)\": \"FCR\"\n",
                "}\n",
                "\n",
                "def rename_columns(df):\n",
                "    \"\"\"Rename columns if they contain specific strings\"\"\"\n",
                "    new_columns = {}\n",
                "    modified = False\n",
                "    \n",
                "    for col in df.columns:\n",
                "        new_col = col\n",
                "        for old, new in replacements.items():\n",
                "            if old in col:\n",
                "                new_col = col.replace(old, new)\n",
                "                modified = True\n",
                "        if new_col != col:\n",
                "            new_columns[col] = new_col\n",
                "    \n",
                "    if modified:\n",
                "        df = df.rename(columns=new_columns)\n",
                "    return df, modified\n",
                "\n",
                "# Process all CSV files in subdirectories\n",
                "for source_path in source_dir.rglob(\"*.csv\"):\n",
                "    # Read the CSV file with low_memory=False to avoid dtype warnings\n",
                "    df = pd.read_csv(source_path, low_memory=False)\n",
                "    \n",
                "    # Rename columns and check if any modifications were made\n",
                "    df, was_modified = rename_columns(df)\n",
                "    \n",
                "    # Only save if modifications were made\n",
                "    if was_modified:\n",
                "        # Create corresponding directory structure in target_dir\n",
                "        relative_path = source_path.relative_to(source_dir)\n",
                "        target_path = target_dir / relative_path\n",
                "        \n",
                "        # Create directory if it doesn't exist\n",
                "        target_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "        \n",
                "        # Save the modified CSV\n",
                "        df.to_csv(target_path, index=False)\n",
                "        print(f\"Processed and saved: {relative_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Sanity check column names"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import numpy as np\n",
                "\n",
                "# Define the source and target directories\n",
                "source_dir = Path(\"pivoted_data\")\n",
                "target_dir = Path(\"pivoted_data_filtered\")\n",
                "\n",
                "# Dictionary for replacements\n",
                "replacements = {\n",
                "    \"Automatic Frequency Restoration Reserve (aFRR)\": \"aFRR\",\n",
                "    \"Manual Frequency Restoration Reserve (mFRR)\": \"mFRR\",\n",
                "    \"Frequency Containment Reserve (FCR)\": \"FCR\"\n",
                "}\n",
                "\n",
                "def get_modified_column_name(original_col):\n",
                "    \"\"\"Convert original column name to its modified version\"\"\"\n",
                "    modified_col = original_col\n",
                "    for old, new in replacements.items():\n",
                "        if old in original_col:\n",
                "            modified_col = original_col.replace(old, new)\n",
                "    return modified_col\n",
                "\n",
                "def compare_dataframes(original_df, modified_df, file_path):\n",
                "    \"\"\"Compare two dataframes and report any differences\"\"\"\n",
                "    \n",
                "    # Check if the data shape is the same\n",
                "    if original_df.shape != modified_df.shape:\n",
                "        print(f\"\\n {file_path}: Shape mismatch!\")\n",
                "        print(f\"Original shape: {original_df.shape}\")\n",
                "        print(f\"Modified shape: {modified_df.shape}\")\n",
                "        return False\n",
                "    \n",
                "    # Get expected column names after modification\n",
                "    expected_cols = [get_modified_column_name(col) for col in original_df.columns]\n",
                "    actual_cols = modified_df.columns.tolist()\n",
                "    \n",
                "    # Check if columns match\n",
                "    if set(expected_cols) != set(actual_cols):\n",
                "        print(f\"\\n {file_path}: Column names don't match as expected!\")\n",
                "        print(\"Mismatched columns:\")\n",
                "        print(\"Expected but not found:\", set(expected_cols) - set(actual_cols))\n",
                "        print(\"Found but not expected:\", set(actual_cols) - set(expected_cols))\n",
                "        return False\n",
                "    \n",
                "    # Reorder modified_df columns to match the expected order\n",
                "    modified_df = modified_df[expected_cols]\n",
                "    \n",
                "    # Compare data values column by column\n",
                "    for col in original_df.columns:\n",
                "        modified_col = get_modified_column_name(col)\n",
                "        \n",
                "        # Convert to string to handle mixed types\n",
                "        orig_series = original_df[col].astype(str)\n",
                "        mod_series = modified_df[modified_col].astype(str)\n",
                "        \n",
                "        if not orig_series.equals(mod_series):\n",
                "            print(f\"\\n {file_path}: Data values are different in column {col}!\")\n",
                "            return False\n",
                "    \n",
                "    print(f\"\\n {file_path}: Data is identical (only column names changed)\")\n",
                "    return True\n",
                "\n",
                "# Find all modified files in target directory\n",
                "print(\"Starting comparison...\")\n",
                "for target_path in target_dir.rglob(\"*.csv\"):\n",
                "    # Get corresponding source file path\n",
                "    relative_path = target_path.relative_to(target_dir)\n",
                "    source_path = source_dir / relative_path\n",
                "    \n",
                "    if source_path.exists():\n",
                "        print(f\"\\nComparing {relative_path}\")\n",
                "        # Read both files\n",
                "        original_df = pd.read_csv(source_path, low_memory=False)\n",
                "        modified_df = pd.read_csv(target_path, low_memory=False)\n",
                "        \n",
                "        # Compare the files\n",
                "        compare_dataframes(original_df, modified_df, relative_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Clean up columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import shutil\n",
                "\n",
                "# Define source and target directories\n",
                "source_dir = Path('pivoted_data')\n",
                "\n",
                "target_dir = Path('pivoted_data_short')\n",
                "\n",
                "# Create target directory if it doesn't exist\n",
                "target_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Columns to check for\n",
                "columns_to_check = ['ResolutionCode', 'AreaTypeCode', 'OutAreaTypeCode', 'InAreaTypeCode', 'Currency', 'TimeHorizon', 'PriceType', 'ReserveSource', 'ContractType', 'SourceName', 'TypeOfProduct', 'Replacement Reserve (RR)', 'Resolution', 'Action']\n",
                "\n",
                "# Loop through all subdirectories in source directory\n",
                "for subdir in source_dir.iterdir():\n",
                "    if subdir.is_dir():\n",
                "        # Create corresponding subdirectory in target\n",
                "        new_subdir = target_dir / subdir.name\n",
                "        new_subdir.mkdir(parents=True, exist_ok=True)\n",
                "        \n",
                "        # Process all CSV files in the subdirectory\n",
                "        for csv_file in subdir.glob('*.csv'):\n",
                "            # Read the CSV\n",
                "            df = pd.read_csv(csv_file, low_memory=False)\n",
                "            # print(df.head())\n",
                "            \n",
                "            # Check if any column contains the specified strings\n",
                "            columns_to_drop = [col for col in df.columns \n",
                "                             if any(check in col for check in columns_to_check)]\n",
                "            \n",
                "            # Drop the columns if found\n",
                "            if columns_to_drop:\n",
                "                df = df.drop(columns=columns_to_drop)\n",
                "                \n",
                "            # Save to new location\n",
                "            new_file_path = new_subdir / csv_file.name\n",
                "            df.to_csv(new_file_path, index=False)\n",
                "            \n",
                "            print(f\"Processed {csv_file.name}: Dropped {len(columns_to_drop)} columns\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Encode data and remove deprecated data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Balancing folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "\n",
                "# Define directories\n",
                "input_dir = Path(\"pivoted_data_short/Balancing\")\n",
                "output_dir = Path(\"pivoted_data_encoded/Balancing\")\n",
                "\n",
                "# Ensure output directory exists\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "# Loop through all CSV files in the Balancing folder\n",
                "for file in os.listdir(input_dir):\n",
                "    if file.endswith(\".csv\"):  # Process only CSV files\n",
                "        file_path = os.path.join(input_dir, file)\n",
                "        \n",
                "        # Create new filename by keeping only part before first underscore\n",
                "        base_name = file.split('_')[0] + '.csv'\n",
                "        output_file_path = os.path.join(output_dir, base_name)\n",
                "\n",
                "        print(f\"Processing: {file}\")\n",
                "        \n",
                "        # Read CSV\n",
                "        df = pd.read_csv(file_path, low_memory=False)  # Added low_memory=False to avoid DtypeWarning\n",
                "        \n",
                "        # Identify first column as DateTime (if applicable)\n",
                "        df.rename(columns={df.columns[0]: \"DateTime\"}, inplace=True)\n",
                "        df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"], errors=\"coerce\")\n",
                "\n",
                "        # Identify categorical columns (excluding DateTime)\n",
                "        categorical_columns = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
                "        if \"DateTime\" in categorical_columns:\n",
                "            categorical_columns.remove(\"DateTime\")\n",
                "\n",
                "        # Identify numerical (continuous) columns\n",
                "        # numerical_columns = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
                "\n",
                "        # One-hot encoding categorical variables while avoiding encoding missing values\n",
                "        for col in categorical_columns:\n",
                "            dummies = pd.get_dummies(df[col], prefix=col, dummy_na=False, dtype=int)  # Avoid encoding NaN\n",
                "            df = pd.concat([df, dummies], axis=1)\n",
                "            df.drop(columns=[col], inplace=True)\n",
                "\n",
                "        # Standardize numerical (continuous) columns while preserving NaNs\n",
                "        # if numerical_columns:\n",
                "        #     scaler = StandardScaler()\n",
                "        #     df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
                "\n",
                "        # Save the processed file\n",
                "        df.to_csv(output_file_path, index=False)\n",
                "        print(f\"Processed and saved: {output_file_path}\")\n",
                "\n",
                "print(\"All Balancing files processed successfully!\") "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "folder_path = Path(\"pivoted_data_encoded/Balancing\")\n",
                "\n",
                "\n",
                "# Loop through all CSV files in the folder\n",
                "for filename in os.listdir(folder_path):\n",
                "    if filename.endswith('.csv'):\n",
                "        file_path = os.path.join(folder_path, filename)\n",
                "        \n",
                "        # Create variable name from filename\n",
                "        df_name = filename.replace('.csv', '')\n",
                "        \n",
                "        # Read the CSV file\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "        \n",
                "        # Convert DateTime column to datetime\n",
                "        df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
                "        \n",
                "        # Get list of columns excluding DateTime\n",
                "        columns_to_check = [col for col in df.columns if col != 'DateTime']\n",
                "        columns_to_keep = ['DateTime']  # Always keep DateTime column\n",
                "        \n",
                "        # Check each column\n",
                "        for col in columns_to_check:\n",
                "            # Check if column has any data after 2023\n",
                "            has_recent_data = df[df['DateTime'].dt.year >= 2023][col].notna().any()\n",
                "            \n",
                "            # Calculate missing percentage for pre-2023 data\n",
                "            pre_2023_data = df[df['DateTime'].dt.year < 2023][col]\n",
                "            missing_percentage = (pre_2023_data.isna().sum() / len(pre_2023_data)) * 100\n",
                "            \n",
                "            # Keep column if it has recent data OR if pre-2023 missing data is less than threshold\n",
                "            if has_recent_data or missing_percentage < 95.45:\n",
                "                columns_to_keep.append(col)\n",
                "        \n",
                "        # Filter DataFrame to keep only selected columns\n",
                "        df_filtered = df[columns_to_keep]\n",
                "        \n",
                "        # Print summary\n",
                "        print(f\"\\nFile: {filename}\")\n",
                "        print(f\"Original columns: {len(df.columns)}\")\n",
                "        print(f\"Columns after filtering: {len(df_filtered.columns)}\")\n",
                "        print(f\"Removed {len(df.columns) - len(df_filtered.columns)} columns\")\n",
                "        \n",
                "        # Save filtered DataFrame back to CSV\n",
                "        output_path = os.path.join(folder_path, filename)\n",
                "        df_filtered.to_csv(output_path, index=False)\n",
                "        print(f\"Saved filtered data to: {output_path}\")\n",
                "\n",
                "print(\"\\nAll files processed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Congestion Management"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "\n",
                "# Define directories\n",
                "input_dir = Path(\"pivoted_data_short/Congestion Management\")\n",
                "output_dir = Path(\"pivoted_data_encoded/Congestion Management\")\n",
                "\n",
                "# Ensure output directory exists\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "# Loop through all CSV files in the Congestion Management folder\n",
                "for file in os.listdir(input_dir):\n",
                "    if file.endswith(\".csv\"):  # Process only CSV files\n",
                "        file_path = os.path.join(input_dir, file)\n",
                "        \n",
                "        # Create new filename by keeping only part before first underscore\n",
                "        base_name = file.split('_')[0] + '.csv'\n",
                "        output_file_path = os.path.join(output_dir, base_name)\n",
                "\n",
                "        print(f\"Processing: {file}\")\n",
                "        \n",
                "        # Read CSV\n",
                "        df = pd.read_csv(file_path, low_memory=False)  # Added low_memory=False to avoid DtypeWarning\n",
                "        \n",
                "        # Identify first column as DateTime (if applicable)\n",
                "        df.rename(columns={df.columns[0]: \"DateTime\"}, inplace=True)\n",
                "        df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"], errors=\"coerce\")\n",
                "\n",
                "        # Identify categorical columns (excluding DateTime)\n",
                "        categorical_columns = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
                "        if \"DateTime\" in categorical_columns:\n",
                "            categorical_columns.remove(\"DateTime\")\n",
                "\n",
                "        # Identify numerical (continuous) columns\n",
                "        # numerical_columns = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
                "\n",
                "        # One-hot encoding categorical variables while avoiding encoding missing values\n",
                "        for col in categorical_columns:\n",
                "            dummies = pd.get_dummies(df[col], prefix=col, dummy_na=False, dtype=int)  # Avoid encoding NaN\n",
                "            df = pd.concat([df, dummies], axis=1)\n",
                "            df.drop(columns=[col], inplace=True)\n",
                "\n",
                "        # Standardize numerical (continuous) columns while preserving NaNs\n",
                "        # if numerical_columns:\n",
                "        #     scaler = StandardScaler()\n",
                "        #     df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
                "\n",
                "        # Save the processed file\n",
                "        df.to_csv(output_file_path, index=False)\n",
                "        print(f\"Processed and saved: {output_file_path}\")\n",
                "\n",
                "print(\"All Congestion Management files processed successfully!\") "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Using relative path from current working directory\n",
                "folder_path = Path(\"pivoted_data_encoded/Congestion Management\")\n",
                "\n",
                "# Loop through all CSV files in the folder\n",
                "for filename in os.listdir(folder_path):\n",
                "    if filename.endswith('.csv'):\n",
                "        file_path = os.path.join(folder_path, filename)\n",
                "        \n",
                "        # Create variable name from filename\n",
                "        df_name = filename.replace('.csv', '')\n",
                "        \n",
                "        # Read the CSV file\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "        \n",
                "        # Convert DateTime column to datetime\n",
                "        df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
                "        \n",
                "        # Get list of columns excluding DateTime\n",
                "        columns_to_check = [col for col in df.columns if col != 'DateTime']\n",
                "        columns_to_keep = ['DateTime']  # Always keep DateTime column\n",
                "        \n",
                "        # Check each column\n",
                "        for col in columns_to_check:\n",
                "            # Check if column has any data after 2023\n",
                "            has_recent_data = df[df['DateTime'].dt.year >= 2023][col].notna().any()\n",
                "            \n",
                "            # Calculate missing percentage for pre-2023 data\n",
                "            pre_2023_data = df[df['DateTime'].dt.year < 2023][col]\n",
                "            missing_percentage = (pre_2023_data.isna().sum() / len(pre_2023_data)) * 100\n",
                "            \n",
                "            # Keep column if it has recent data OR if pre-2023 missing data is less than threshold\n",
                "            if has_recent_data or missing_percentage < 95.45:\n",
                "                columns_to_keep.append(col)\n",
                "        \n",
                "        # Filter DataFrame to keep only selected columns\n",
                "        df_filtered = df[columns_to_keep]\n",
                "        \n",
                "        # Print summary\n",
                "        print(f\"\\nFile: {filename}\")\n",
                "        print(f\"Original columns: {len(df.columns)}\")\n",
                "        print(f\"Columns after filtering: {len(df_filtered.columns)}\")\n",
                "        print(f\"Removed {len(df.columns) - len(df_filtered.columns)} columns\")\n",
                "        \n",
                "        # Save filtered DataFrame back to CSV\n",
                "        output_path = os.path.join(folder_path, filename)\n",
                "        df_filtered.to_csv(output_path, index=False)\n",
                "        print(f\"Saved filtered data to: {output_path}\")\n",
                "\n",
                "print(\"\\nAll files processed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generation folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Define directories\n",
                "input_dir = Path(\"pivoted_data_short/Generation\")\n",
                "output_dir = Path(\"pivoted_data_encoded/Generation\")\n",
                "\n",
                "# Ensure output directory exists\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "# Loop through all CSV files in the Generation folder\n",
                "for file in os.listdir(input_dir):\n",
                "    if file.endswith(\".csv\"):  # Process only CSV files\n",
                "        file_path = os.path.join(input_dir, file)\n",
                "        \n",
                "        # Create new filename by keeping only part before first underscore\n",
                "        base_name = file.split('_')[0] + '.csv'\n",
                "        output_file_path = os.path.join(output_dir, base_name)\n",
                "\n",
                "        print(f\"Processing: {file}\")\n",
                "        \n",
                "        # Read CSV\n",
                "        df = pd.read_csv(file_path, low_memory=False)  # Added low_memory=False to avoid DtypeWarning\n",
                "        \n",
                "        # Identify first column as DateTime (if applicable)\n",
                "        df.rename(columns={df.columns[0]: \"DateTime\"}, inplace=True)\n",
                "        df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"], errors=\"coerce\")\n",
                "\n",
                "        # Identify categorical columns (excluding DateTime)\n",
                "        categorical_columns = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
                "        if \"DateTime\" in categorical_columns:\n",
                "            categorical_columns.remove(\"DateTime\")\n",
                "\n",
                "\n",
                "\n",
                "        # One-hot encoding categorical variables while avoiding encoding missing values\n",
                "        for col in categorical_columns:\n",
                "            dummies = pd.get_dummies(df[col], prefix=col, dummy_na=False, dtype=int)  # Avoid encoding NaN, dtype = 1 or 0\n",
                "            df = pd.concat([df, dummies], axis=1)\n",
                "            df.drop(columns=[col], inplace=True)\n",
                "\n",
                "\n",
                "        # Save the processed file\n",
                "        df.to_csv(output_file_path, index=False)\n",
                "        print(f\"Processed and saved: {output_file_path}\")\n",
                "\n",
                "print(\"All Generation files processed successfully!\") "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Using relative path from current working directory\n",
                "folder_path = Path(\"pivoted_data_encoded/Generation\")\n",
                "\n",
                "# Loop through all CSV files in the folder\n",
                "for filename in os.listdir(folder_path):\n",
                "    if filename.endswith('.csv'):\n",
                "        file_path = os.path.join(folder_path, filename)\n",
                "        \n",
                "        # Create variable name from filename\n",
                "        df_name = filename.replace('.csv', '')\n",
                "        \n",
                "        # Read the CSV file\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "        \n",
                "        # Convert DateTime column to datetime\n",
                "        df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
                "        \n",
                "        # Get list of columns excluding DateTime\n",
                "        columns_to_check = [col for col in df.columns if col != 'DateTime']\n",
                "        columns_to_keep = ['DateTime']  # Always keep DateTime column\n",
                "        \n",
                "        # Check each column\n",
                "        for col in columns_to_check:\n",
                "            # Check if column has any data after 2023\n",
                "            has_recent_data = df[df['DateTime'].dt.year >= 2023][col].notna().any()\n",
                "            \n",
                "            # Calculate missing percentage for pre-2023 data\n",
                "            pre_2023_data = df[df['DateTime'].dt.year < 2023][col]\n",
                "            missing_percentage = (pre_2023_data.isna().sum() / len(pre_2023_data)) * 100\n",
                "            \n",
                "            # Keep column if it has recent data OR if pre-2023 missing data is less than threshold\n",
                "            if has_recent_data or missing_percentage < 95.45:\n",
                "                columns_to_keep.append(col)\n",
                "        \n",
                "        # Filter DataFrame to keep only selected columns\n",
                "        df_filtered = df[columns_to_keep]\n",
                "        \n",
                "        # Print summary\n",
                "        print(f\"\\nFile: {filename}\")\n",
                "        print(f\"Original columns: {len(df.columns)}\")\n",
                "        print(f\"Columns after filtering: {len(df_filtered.columns)}\")\n",
                "        print(f\"Removed {len(df.columns) - len(df_filtered.columns)} columns\")\n",
                "        \n",
                "        # Save filtered DataFrame back to CSV\n",
                "        output_path = os.path.join(folder_path, filename)\n",
                "        df_filtered.to_csv(output_path, index=False)\n",
                "        print(f\"Saved filtered data to: {output_path}\")\n",
                "\n",
                "print(\"\\nAll files processed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Define directories\n",
                "input_dir = Path(\"pivoted_data_short/Load\")\n",
                "output_dir = Path(\"pivoted_data_encoded/Load\")\n",
                "\n",
                "# Ensure output directory exists\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "# Loop through all CSV files in the Load folder\n",
                "for file in os.listdir(input_dir):\n",
                "    if file.endswith(\".csv\"):  # Process only CSV files\n",
                "        file_path = os.path.join(input_dir, file)\n",
                "        \n",
                "        # Create new filename by keeping only part before first underscore\n",
                "        base_name = file.split('_')[0] + '.csv'\n",
                "        output_file_path = os.path.join(output_dir, base_name)\n",
                "\n",
                "        print(f\"Processing: {file}\")\n",
                "        \n",
                "        # Read CSV\n",
                "        df = pd.read_csv(file_path, low_memory=False)  # Added low_memory=False to avoid DtypeWarning\n",
                "        \n",
                "        # Identify first column as DateTime (if applicable)\n",
                "        df.rename(columns={df.columns[0]: \"DateTime\"}, inplace=True)\n",
                "        df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"], errors=\"coerce\")\n",
                "\n",
                "        # Identify categorical columns (excluding DateTime)\n",
                "        categorical_columns = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
                "        if \"DateTime\" in categorical_columns:\n",
                "            categorical_columns.remove(\"DateTime\")\n",
                "\n",
                "        # One-hot encoding categorical variables while avoiding encoding missing values\n",
                "        for col in categorical_columns:\n",
                "            dummies = pd.get_dummies(df[col], prefix=col, dummy_na=False, dtype=int)  # Avoid encoding NaN, dtype = 1 or 0\n",
                "            df = pd.concat([df, dummies], axis=1)\n",
                "            df.drop(columns=[col], inplace=True)\n",
                "\n",
                "        # Save the processed file\n",
                "        df.to_csv(output_file_path, index=False)\n",
                "        print(f\"Processed and saved: {output_file_path}\")\n",
                "\n",
                "print(\"All Load files processed successfully!\") "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Using relative path from current working directory\n",
                "folder_path = Path(\"pivoted_data_encoded/Load\")\n",
                "\n",
                "# Loop through all CSV files in the folder\n",
                "for filename in os.listdir(folder_path):\n",
                "    if filename.endswith('.csv'):\n",
                "        file_path = os.path.join(folder_path, filename)\n",
                "        \n",
                "        # Create variable name from filename\n",
                "        df_name = filename.replace('.csv', '')\n",
                "        \n",
                "        # Read the CSV file\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "        \n",
                "        # Convert DateTime column to datetime\n",
                "        df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
                "        \n",
                "        # Get list of columns excluding DateTime\n",
                "        columns_to_check = [col for col in df.columns if col != 'DateTime']\n",
                "        columns_to_keep = ['DateTime']  # Always keep DateTime column\n",
                "        \n",
                "        # Check each column\n",
                "        for col in columns_to_check:\n",
                "            # Check if column has any data after 2023\n",
                "            has_recent_data = df[df['DateTime'].dt.year >= 2023][col].notna().any()\n",
                "            \n",
                "            # Calculate missing percentage for pre-2023 data\n",
                "            pre_2023_data = df[df['DateTime'].dt.year < 2023][col]\n",
                "            missing_percentage = (pre_2023_data.isna().sum() / len(pre_2023_data)) * 100\n",
                "            \n",
                "            # Keep column if it has recent data OR if pre-2023 missing data is less than threshold\n",
                "            if has_recent_data or missing_percentage < 95.45:\n",
                "                columns_to_keep.append(col)\n",
                "        \n",
                "        # Filter DataFrame to keep only selected columns\n",
                "        df_filtered = df[columns_to_keep]\n",
                "        \n",
                "        # Print summary\n",
                "        print(f\"\\nFile: {filename}\")\n",
                "        print(f\"Original columns: {len(df.columns)}\")\n",
                "        print(f\"Columns after filtering: {len(df_filtered.columns)}\")\n",
                "        print(f\"Removed {len(df.columns) - len(df_filtered.columns)} columns\")\n",
                "        \n",
                "        # Save filtered DataFrame back to CSV\n",
                "        output_path = os.path.join(folder_path, filename)\n",
                "        df_filtered.to_csv(output_path, index=False)\n",
                "        print(f\"Saved filtered data to: {output_path}\")\n",
                "\n",
                "print(\"\\nAll files processed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Outages folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Define directories\n",
                "input_dir = Path(\"pivoted_data_short/Outages\")\n",
                "output_dir = Path(\"pivoted_data_encoded/Outages\")\n",
                "\n",
                "# Ensure output directory exists\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "# Loop through all CSV files in the Outages folder\n",
                "for file in os.listdir(input_dir):\n",
                "    if file.endswith(\".csv\"):  # Process only CSV files\n",
                "        file_path = os.path.join(input_dir, file)\n",
                "        \n",
                "        # Create new filename by keeping only part before first underscore\n",
                "        base_name = file.split('_')[0] + '.csv'\n",
                "        output_file_path = os.path.join(output_dir, base_name)\n",
                "\n",
                "        print(f\"Processing: {file}\")\n",
                "        \n",
                "        # Read CSV\n",
                "        df = pd.read_csv(file_path, low_memory=False)  # Added low_memory=False to avoid DtypeWarning\n",
                "        \n",
                "        # Remove columns containing \"_Type\"\n",
                "        type_columns = [col for col in df.columns if \"_Type\" in col]\n",
                "        if type_columns:\n",
                "            df = df.drop(columns=type_columns)\n",
                "            print(f\"Removed {len(type_columns)} Type columns: {type_columns}\")\n",
                "        \n",
                "        # Identify first column as DateTime (if applicable)\n",
                "        df.rename(columns={df.columns[0]: \"DateTime\"}, inplace=True)\n",
                "        df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"], errors=\"coerce\")\n",
                "\n",
                "        # Identify categorical columns (excluding DateTime)\n",
                "        categorical_columns = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
                "        if \"DateTime\" in categorical_columns:\n",
                "            categorical_columns.remove(\"DateTime\")\n",
                "\n",
                "        # One-hot encoding categorical variables while avoiding encoding missing values\n",
                "        for col in categorical_columns:\n",
                "            dummies = pd.get_dummies(df[col], prefix=col, dummy_na=False, dtype=int)  # Avoid encoding NaN, dtype = 1 or 0\n",
                "            df = pd.concat([df, dummies], axis=1)\n",
                "            df.drop(columns=[col], inplace=True)\n",
                "\n",
                "        # Save the processed file\n",
                "        df.to_csv(output_file_path, index=False)\n",
                "        print(f\"Processed and saved: {output_file_path}\")\n",
                "\n",
                "print(\"All Outages files processed successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Using relative path from current working directory\n",
                "folder_path = Path(\"pivoted_data_encoded/Outages\")\n",
                "\n",
                "# Loop through all CSV files in the folder\n",
                "for filename in os.listdir(folder_path):\n",
                "    if filename.endswith('.csv'):\n",
                "        file_path = os.path.join(folder_path, filename)\n",
                "        \n",
                "        # Create variable name from filename\n",
                "        df_name = filename.replace('.csv', '')\n",
                "        \n",
                "        # Read the CSV file\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "        \n",
                "        # Convert DateTime column to datetime\n",
                "        df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
                "        \n",
                "        # Get list of columns excluding DateTime\n",
                "        columns_to_check = [col for col in df.columns if col != 'DateTime']\n",
                "        columns_to_keep = ['DateTime']  # Always keep DateTime column\n",
                "        \n",
                "        # Check each column\n",
                "        for col in columns_to_check:\n",
                "            # Check if column has any data after 2023\n",
                "            has_recent_data = df[df['DateTime'].dt.year >= 2023][col].notna().any()\n",
                "            \n",
                "            # Calculate missing percentage for pre-2023 data\n",
                "            pre_2023_data = df[df['DateTime'].dt.year < 2023][col]\n",
                "            missing_percentage = (pre_2023_data.isna().sum() / len(pre_2023_data)) * 100\n",
                "            \n",
                "            # Keep column if it has recent data OR if pre-2023 missing data is less than threshold\n",
                "            if has_recent_data or missing_percentage < 95.45:\n",
                "                columns_to_keep.append(col)\n",
                "        \n",
                "        # Filter DataFrame to keep only selected columns\n",
                "        df_filtered = df[columns_to_keep]\n",
                "        \n",
                "        # Print summary\n",
                "        print(f\"\\nFile: {filename}\")\n",
                "        print(f\"Original columns: {len(df.columns)}\")\n",
                "        print(f\"Columns after filtering: {len(df_filtered.columns)}\")\n",
                "        print(f\"Removed {len(df.columns) - len(df_filtered.columns)} columns\")\n",
                "        \n",
                "        # Save filtered DataFrame back to CSV\n",
                "        output_path = os.path.join(folder_path, filename)\n",
                "        df_filtered.to_csv(output_path, index=False)\n",
                "        print(f\"Saved filtered data to: {output_path}\")\n",
                "\n",
                "print(\"\\nAll files processed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transmission folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Define directories\n",
                "input_dir = Path(\"pivoted_data_short/Transmission\")\n",
                "output_dir = Path(\"pivoted_data_encoded/Transmission\")\n",
                "\n",
                "# Ensure output directory exists\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "# Loop through all CSV files in the Transmission folder\n",
                "for file in os.listdir(input_dir):\n",
                "    if file.endswith(\".csv\"):  # Process only CSV files\n",
                "        file_path = os.path.join(input_dir, file)\n",
                "        \n",
                "        # Create new filename by keeping only part before first underscore\n",
                "        base_name = file.split('_')[0] + '.csv'\n",
                "        output_file_path = os.path.join(output_dir, base_name)\n",
                "\n",
                "        print(f\"Processing: {file}\")\n",
                "        \n",
                "        # Read CSV\n",
                "        df = pd.read_csv(file_path, low_memory=False)  # Added low_memory=False to avoid DtypeWarning\n",
                "        \n",
                "        # Identify first column as DateTime (if applicable)\n",
                "        df.rename(columns={df.columns[0]: \"DateTime\"}, inplace=True)\n",
                "        df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"], errors=\"coerce\")\n",
                "\n",
                "        # Identify categorical columns (excluding DateTime)\n",
                "        categorical_columns = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
                "        if \"DateTime\" in categorical_columns:\n",
                "            categorical_columns.remove(\"DateTime\")\n",
                "\n",
                "\n",
                "\n",
                "        # One-hot encoding categorical variables while avoiding encoding missing values\n",
                "        for col in categorical_columns:\n",
                "            dummies = pd.get_dummies(df[col], prefix=col, dummy_na=False, dtype=int)  # Avoid encoding NaN, dtype = 1 or 0\n",
                "            df = pd.concat([df, dummies], axis=1)\n",
                "            df.drop(columns=[col], inplace=True)\n",
                "\n",
                "\n",
                "        # Save the processed file\n",
                "        df.to_csv(output_file_path, index=False)\n",
                "        print(f\"Processed and saved: {output_file_path}\")\n",
                "\n",
                "print(\"All Transmission files processed successfully!\") "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Using relative path from current working directory\n",
                "current_dir = os.getcwd()\n",
                "folder_path = Path(\"pivoted_data_encoded/Transmission\")\n",
                "\n",
                "# Loop through all CSV files in the folder\n",
                "for filename in os.listdir(folder_path):\n",
                "    if filename.endswith('.csv'):\n",
                "        file_path = os.path.join(folder_path, filename)\n",
                "        \n",
                "        # Create variable name from filename\n",
                "        df_name = filename.replace('.csv', '')\n",
                "        \n",
                "        # Read the CSV file\n",
                "        df = pd.read_csv(file_path, low_memory=False)\n",
                "        \n",
                "        # Convert DateTime column to datetime\n",
                "        df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
                "        \n",
                "        # Get list of columns excluding DateTime\n",
                "        columns_to_check = [col for col in df.columns if col != 'DateTime']\n",
                "        columns_to_keep = ['DateTime']  # Always keep DateTime column\n",
                "        \n",
                "        # Check each column\n",
                "        for col in columns_to_check:\n",
                "            # Check if column has any data after 2023\n",
                "            has_recent_data = df[df['DateTime'].dt.year >= 2023][col].notna().any()\n",
                "            \n",
                "            # Calculate missing percentage for pre-2023 data\n",
                "            pre_2023_data = df[df['DateTime'].dt.year < 2023][col]\n",
                "            missing_percentage = (pre_2023_data.isna().sum() / len(pre_2023_data)) * 100\n",
                "            \n",
                "            # Keep column if it has recent data OR if pre-2023 missing data is less than threshold\n",
                "            if has_recent_data or missing_percentage < 95.45:\n",
                "                columns_to_keep.append(col)\n",
                "        \n",
                "        # Filter DataFrame to keep only selected columns\n",
                "        df_filtered = df[columns_to_keep]\n",
                "        \n",
                "        # Print summary\n",
                "        print(f\"\\nFile: {filename}\")\n",
                "        print(f\"Original columns: {len(df.columns)}\")\n",
                "        print(f\"Columns after filtering: {len(df_filtered.columns)}\")\n",
                "        print(f\"Removed {len(df.columns) - len(df_filtered.columns)} columns\")\n",
                "        \n",
                "        # Save filtered DataFrame back to CSV\n",
                "        output_path = os.path.join(folder_path, filename)\n",
                "        df_filtered.to_csv(output_path, index=False)\n",
                "        print(f\"Saved filtered data to: {output_path}\")\n",
                "\n",
                "print(\"\\nAll files processed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Combine into Master Data csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "\n",
                "def combine_csv_files():\n",
                "    # Define the base directory and output path\n",
                "    base_dir = Path(\"pivoted_data_encoded\")\n",
                "    output_path = Path(\"master_data/resultsset.csv\")\n",
                "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    print(\"Starting CSV combination process...\")\n",
                "    \n",
                "    # Initialize an empty DataFrame to store the combined data\n",
                "    master_df = None\n",
                "    \n",
                "    # Walk through all directories\n",
                "    for root, dirs, files in os.walk(base_dir):\n",
                "        # Skip the base directory itself\n",
                "        if root == base_dir:\n",
                "            continue\n",
                "            \n",
                "        category = os.path.basename(root)  # Get the category name (folder name)\n",
                "        print(f\"\\nProcessing {category} folder...\")\n",
                "        \n",
                "        # Process each CSV file in the current directory\n",
                "        for file in files:\n",
                "            if file.endswith('.csv'):\n",
                "                file_path = os.path.join(root, file)\n",
                "                print(f\"  Reading {file}...\")\n",
                "                \n",
                "                try:\n",
                "                    # Read the CSV file\n",
                "                    df = pd.read_csv(file_path, low_memory=False)\n",
                "                    \n",
                "                    # Convert DateTime to datetime type\n",
                "                    df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
                "                    \n",
                "                    # Filter data up to 2025-01-31\n",
                "                    # df = df[df['DateTime'] <= '2025-01-31']\n",
                "                    \n",
                "                    # Check if data has half-hourly frequency\n",
                "                    time_diffs = df['DateTime'].diff().dropna()\n",
                "                    if len(time_diffs) > 0 and time_diffs.min() < pd.Timedelta(hours=1):\n",
                "                        print(f\"    Detected sub-hourly data in {file}, aggregating to hourly...\")\n",
                "                        \n",
                "                        # Round DateTime to nearest hour\n",
                "                        df['HourlyDateTime'] = df['DateTime'].dt.floor('h')\n",
                "                        \n",
                "                        # Group by hour and aggregate\n",
                "                        # For numeric columns, use mean\n",
                "                        # For categorical/object columns, use first value\n",
                "                        numeric_cols = df.select_dtypes(include=['number']).columns\n",
                "                        object_cols = df.select_dtypes(include=['object']).columns.difference(['DateTime'])\n",
                "                        \n",
                "                        agg_dict = {}\n",
                "                        for col in df.columns:\n",
                "                            if col in numeric_cols:\n",
                "                                agg_dict[col] = 'sum'\n",
                "                            elif col in object_cols:\n",
                "                                agg_dict[col] = 'first'\n",
                "                        \n",
                "                        # Remove DateTime from aggregation\n",
                "                        if 'DateTime' in agg_dict:\n",
                "                            del agg_dict['DateTime']\n",
                "                        if 'HourlyDateTime' in agg_dict:\n",
                "                            del agg_dict['HourlyDateTime']\n",
                "                        \n",
                "                        # Perform aggregation\n",
                "                        df = df.groupby('HourlyDateTime').agg(agg_dict).reset_index()\n",
                "                        df.rename(columns={'HourlyDateTime': 'DateTime'}, inplace=True)\n",
                "                    \n",
                "                    # Set DateTime as index\n",
                "                    df.set_index('DateTime', inplace=True)\n",
                "                    \n",
                "                    # Add category prefix to column names (except DateTime)\n",
                "                    df.columns = [f\"{category}_{file.replace('.csv', '')}_{col}\" for col in df.columns]\n",
                "                    \n",
                "                    # Merge with master_df\n",
                "                    if master_df is None:\n",
                "                        master_df = df\n",
                "                    else:\n",
                "                        master_df = master_df.join(df, how='outer')\n",
                "                    \n",
                "                    print(f\"    Added {len(df.columns)} columns from {file}\")\n",
                "                    \n",
                "                except Exception as e:\n",
                "                    print(f\"    Error processing {file}: {str(e)}\")\n",
                "                    continue\n",
                "    \n",
                "    if master_df is not None:\n",
                "        # Sort index to ensure chronological order\n",
                "        master_df.sort_index(inplace=True)\n",
                "        \n",
                "        # Apply filtering to remove columns with no recent data or mostly missing historical data\n",
                "        print(\"\\nFiltering columns based on data availability...\")\n",
                "        \n",
                "        original_columns = len(master_df.columns)\n",
                "        columns_to_keep = []\n",
                "        \n",
                "        for col in master_df.columns:\n",
                "            # Check if column has any data after 2023\n",
                "            has_recent_data = master_df.loc[master_df.index >= '2023-01-01', col].notna().any()\n",
                "            \n",
                "            # Calculate missing percentage for pre-2023 data\n",
                "            pre_2023_data = master_df.loc[master_df.index < '2023-01-01', col]\n",
                "            missing_percentage = (pre_2023_data.isna().sum() / len(pre_2023_data)) * 100 if len(pre_2023_data) > 0 else 0\n",
                "            \n",
                "            # Keep column if it has recent data OR if pre-2023 missing data is less than threshold\n",
                "            if has_recent_data or missing_percentage < 95.45:\n",
                "                columns_to_keep.append(col)\n",
                "        \n",
                "        # Filter DataFrame to keep only selected columns\n",
                "        master_df = master_df[columns_to_keep]\n",
                "        \n",
                "        print(f\"Original columns: {original_columns}\")\n",
                "        print(f\"Columns after filtering: {len(master_df.columns)}\")\n",
                "        print(f\"Removed {original_columns - len(master_df.columns)} columns\")\n",
                "        \n",
                "        # Ensure all data is at hourly frequency\n",
                "        print(\"\\nEnsuring hourly frequency for all data...\")\n",
                "        master_df = master_df.asfreq('h')\n",
                "        \n",
                "        # Save the combined DataFrame\n",
                "        print(f\"\\nSaving master dataset...\")\n",
                "        print(f\"Total columns: {len(master_df.columns)}\")\n",
                "        print(f\"Date range: {master_df.index.min()} to {master_df.index.max()}\")\n",
                "        print(f\"Total rows: {len(master_df)}\")\n",
                "        \n",
                "        # Calculate memory usage\n",
                "        memory_usage = master_df.memory_usage(deep=True).sum() / 1024**2  # Convert to MB\n",
                "        print(f\"Memory usage: {memory_usage:.2f} MB\")\n",
                "        \n",
                "        # Save to CSV\n",
                "        master_df.to_csv(output_path)\n",
                "        print(f\"\\nMaster dataset saved to: {output_path}\")\n",
                "        \n",
                "        # Generate summary statistics\n",
                "        summary_path = Path(\"master_data/summary_master_data.csv\")\n",
                "        summary = pd.DataFrame({\n",
                "            'non_null_count': master_df.count(),\n",
                "            'null_count': master_df.isnull().sum(),\n",
                "            'null_percentage': (master_df.isnull().sum() / len(master_df) * 100).round(2),\n",
                "            'unique_values': master_df.nunique(),\n",
                "            'memory_usage_mb': master_df.memory_usage(deep=True) / 1024**2\n",
                "        })\n",
                "        summary.to_csv(summary_path)\n",
                "        print(f\"Summary statistics saved to: {summary_path}\")\n",
                "        \n",
                "    else:\n",
                "        print(\"No data was processed. Please check the input directory and files.\")\n",
                "        \n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    combine_csv_files()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Combine TO and FROM parameters into one, e.g. TO_SE3_FROM_X become TO_SE3 from all sources"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read the dataset\n",
                "csv_path = Path(\"master_data/resultsset.csv\")\n",
                "df = pd.read_csv(csv_path)\n",
                "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
                "df.set_index('DateTime', inplace=True)\n",
                "\n",
                "# List of regions and generation types\n",
                "regions = ['SE', 'SE1', 'SE2', 'SE3', 'SE4']\n",
                "generation_types = ['Hydro Water Reservoir', 'Wind Onshore', 'Fossil Gas', 'Fossil Oil', 'Nuclear']\n",
                "\n",
                "# 1. For Outages_UnavailabilityOfProductionUnits, keep only columns with space before underscore\n",
                "for region in regions:\n",
                "    for gen_type in generation_types:\n",
                "        # Find columns with space before underscore\n",
                "        space_cols = [col for col in df.columns if f'Outages_UnavailabilityOfProductionUnits_{region}_{gen_type} _' in col]\n",
                "        # Find columns without space before underscore\n",
                "        no_space_cols = [col for col in df.columns if f'Outages_UnavailabilityOfProductionUnits_{region}_{gen_type}_' in col \n",
                "                        and col not in space_cols]\n",
                "        \n",
                "        # Drop columns without space\n",
                "        if no_space_cols:\n",
                "            df = df.drop(columns=no_space_cols)\n",
                "\n",
                "# 2. Remove Outages_UnavailabilityOfGenerationUnits columns\n",
                "generation_cols = [col for col in df.columns if 'Outages_UnavailabilityOfGenerationUnits_' in col]\n",
                "df = df.drop(columns=generation_cols)\n",
                "\n",
                "# 3. Aggregate Outages_UnavailabilityInTransmissionGrid columns\n",
                "for region in regions:\n",
                "    # Find all FROM columns for this region (both where region is FROM and TO)\n",
                "    from_cols = [col for col in df.columns if f'Outages_UnavailabilityInTransmissionGrid_FROM_{region}_TO_' in col]\n",
                "    to_cols = [col for col in df.columns if f'Outages_UnavailabilityInTransmissionGrid_FROM_' in col and f'_TO_{region}_' in col]\n",
                "    \n",
                "    if from_cols:\n",
                "        df[f'Outages_UnavailabilityInTransmissionGrid_FROM_{region}_NewNTC'] = df[from_cols].sum(axis=1)\n",
                "        df = df.drop(columns=from_cols)\n",
                "    \n",
                "    if to_cols:\n",
                "        df[f'Outages_UnavailabilityInTransmissionGrid_TO_{region}_NewNTC'] = df[to_cols].sum(axis=1)\n",
                "        df = df.drop(columns=to_cols)\n",
                "\n",
                "# 4. Aggregate Congestion Management columns\n",
                "for region in regions:\n",
                "    # Find all FROM columns for this region (both where region is FROM and TO)\n",
                "    from_cols = [col for col in df.columns if f'Congestion Management_Countertrading_FROM_{region}_TO_' in col]\n",
                "    to_cols = [col for col in df.columns if f'Congestion Management_Countertrading_FROM_' in col and f'_TO_{region}_' in col]\n",
                "    \n",
                "    if from_cols:\n",
                "        df[f'Congestion Management_Countertrading_FROM_{region}_ChangeInCrosszonalExchange(MW)'] = df[from_cols].sum(axis=1)\n",
                "        df = df.drop(columns=from_cols)\n",
                "    \n",
                "    if to_cols:\n",
                "        df[f'Congestion Management_Countertrading_TO_{region}_ChangeInCrosszonalExchange(MW)'] = df[to_cols].sum(axis=1)\n",
                "        df = df.drop(columns=to_cols)\n",
                "\n",
                "# 5. Aggregate transmission capacity and flow columns\n",
                "transmission_patterns = [\n",
                "    ('Transmission_ForecastedYearAheadTransferCapacities_', 'ForecastTransferCapacity'),\n",
                "    ('Transmission_ForecastedMonthAheadTransferCapacities_', 'ForecastTransferCapacity'),\n",
                "    ('Transmission_ForecastedWeekAheadTransferCapacities_', 'ForecastTransferCapacity'),\n",
                "    ('Transmission_OfferedIntradayTransferCapacityImplicit_', 'Capacity'),\n",
                "    ('Transmission_PhysicalFlows_', 'FlowValue')\n",
                "]\n",
                "\n",
                "for pattern, suffix in transmission_patterns:\n",
                "    for region in regions:\n",
                "        # Find all FROM columns for this region (both where region is FROM and TO)\n",
                "        from_cols = [col for col in df.columns if f'{pattern}FROM_{region}_TO_' in col]\n",
                "        to_cols = [col for col in df.columns if f'{pattern}FROM_' in col and f'_TO_{region}_' in col]\n",
                "        \n",
                "        if from_cols:\n",
                "            df[f'{pattern}FROM_{region}_{suffix}'] = df[from_cols].sum(axis=1)\n",
                "            df = df.drop(columns=from_cols)\n",
                "        \n",
                "        if to_cols:\n",
                "            df[f'{pattern}TO_{region}_{suffix}'] = df[to_cols].sum(axis=1)\n",
                "            df = df.drop(columns=to_cols)\n",
                "\n",
                "# 6. Aggregate Load forecasts\n",
                "load_patterns = [\n",
                "    'Load_WeekAheadTotalLoadForecast_',\n",
                "    'Load_YearAheadTotalLoadForecast_',\n",
                "    'Load_MonthAheadTotalLoadForecast_'\n",
                "]\n",
                "\n",
                "for pattern in load_patterns:\n",
                "    for region in regions:\n",
                "        min_col = f\"{pattern}{region}_MinimumTotalLoadValue\"\n",
                "        max_col = f\"{pattern}{region}_MaximumTotalLoadValue\"\n",
                "        \n",
                "        if min_col in df.columns and max_col in df.columns:\n",
                "            new_col = f\"{pattern}{region}_AvgMinMax\"\n",
                "            df[new_col] = df[[min_col, max_col]].mean(axis=1)\n",
                "            df = df.drop(columns=[min_col, max_col])\n",
                "\n",
                "\n",
                "# Define the columns to keep\n",
                "columns_to_keep = []\n",
                "\n",
                "# Base patterns for each region\n",
                "for region in regions:\n",
                "    patterns = [\n",
                "        f\"Congestion Management_Countertrading_FROM_{region}_ChangeInCrosszonalExchange(MW)\",\n",
                "        f\"Congestion Management_Countertrading_TO_{region}_ChangeInCrosszonalExchange(MW)\",\n",
                "        f\"Outages_UnavailabilityInTransmissionGrid_FROM_{region}_NewNTC\",\n",
                "        f\"Outages_UnavailabilityInTransmissionGrid_TO_{region}_NewNTC\",\n",
                "        f\"Load_ActualTotalLoad_{region}_TotalLoadValue\",\n",
                "        f\"Load_WeekAheadTotalLoadForecast_{region}_AvgMinMax\",\n",
                "        f\"Load_YearAheadTotalLoadForecast_{region}_AvgMinMax\",\n",
                "        f\"Load_MonthAheadTotalLoadForecast_{region}_AvgMinMax\",\n",
                "        f\"Load_DayAheadTotalLoadForecast_{region}_TotalLoadValue\",\n",
                "        f\"Balancing_ImbalancePrices_{region}_PositiveImbalancePrice\",\n",
                "        f\"Balancing_ImbalancePrices_{region}_NegativeImbalancePrice\",\n",
                "        f\"Balancing_AggregatedBalancingEnergyBids_{region}_mFRR_OfferedUpBidVolume[MW]\",\n",
                "        f\"Balancing_AggregatedBalancingEnergyBids_{region}_mFRR_OfferedDownBidVolume[MW]\",\n",
                "        f\"Balancing_AggregatedBalancingEnergyBids_{region}_mFRR_ActivatedDownBidVolume[MW]\",\n",
                "        f\"Balancing_AggregatedBalancingEnergyBids_{region}_mFRR_ActivatedUpBidVolume[MW]\",\n",
                "        f\"Balancing_AmountAndPricesPaidOfBalancingReservesUnderContract_{region}_aFRR_Down_Volume(MW)\",\n",
                "        f\"Balancing_AmountAndPricesPaidOfBalancingReservesUnderContract_{region}_aFRR_Up_Volume(MW)\",\n",
                "        f\"Balancing_AmountAndPricesPaidOfBalancingReservesUnderContract_{region}_FCR_Symmetric_Volume(MW)\",\n",
                "        f\"Balancing_AmountAndPricesPaidOfBalancingReservesUnderContract_{region}_mFRR_Down_Volume(MW)\",\n",
                "        f\"Balancing_AmountAndPricesPaidOfBalancingReservesUnderContract_{region}_mFRR_Up_Volume(MW)\",\n",
                "        f\"Balancing_AmountAndPricesPaidOfBalancingReservesUnderContract_{region}_aFRR_Down_Price(MW/ISP)\",\n",
                "        f\"Balancing_AmountAndPricesPaidOfBalancingReservesUnderContract_{region}_aFRR_Up_Price(MW/ISP)\",\n",
                "        f\"Balancing_AmountAndPricesPaidOfBalancingReservesUnderContract_{region}_FCR_Symmetric_Price(MW/ISP)\",\n",
                "        f\"Balancing_AmountAndPricesPaidOfBalancingReservesUnderContract_{region}_mFRR_Down_Price(MW/ISP)\",\n",
                "        f\"Balancing_AmountAndPricesPaidOfBalancingReservesUnderContract_{region}_mFRR_Up_Price(MW/ISP)\",\n",
                "        f\"Balancing_PricesOfActivatedBalancingEnergy_{region}_aFRR_NotSpecifiedUpPrice\",\n",
                "        f\"Balancing_PricesOfActivatedBalancingEnergy_{region}_FCR_NotSpecifiedUpPrice\",\n",
                "        f\"Balancing_PricesOfActivatedBalancingEnergy_{region}_mFRR_NotSpecifiedUpPrice\",\n",
                "        f\"Balancing_PricesOfActivatedBalancingEnergy_{region}_aFRR_NotSpecifiedDownPrice\",\n",
                "        f\"Balancing_PricesOfActivatedBalancingEnergy_{region}_FCR_NotSpecifiedDownPrice\",\n",
                "        f\"Balancing_PricesOfActivatedBalancingEnergy_{region}_mFRR_NotSpecifiedDownPrice\",\n",
                "        f\"Balancing_ActivatedBalancingEnergy_{region}_aFRR_NotSpecifiedUpActivatedVolume\",\n",
                "        f\"Balancing_ActivatedBalancingEnergy_{region}_FCR_NotSpecifiedUpActivatedVolume\",\n",
                "        f\"Balancing_ActivatedBalancingEnergy_{region}_mFRR_NotSpecifiedUpActivatedVolume\",\n",
                "        f\"Balancing_ActivatedBalancingEnergy_{region}_aFRR_NotSpecifiedDownActivatedVolume\",\n",
                "        f\"Balancing_ActivatedBalancingEnergy_{region}_FCR_NotSpecifiedDownActivatedVolume\",\n",
                "        f\"Balancing_ActivatedBalancingEnergy_{region}_mFRR_NotSpecifiedDownActivatedVolume\",\n",
                "        f\"Balancing_AcceptedAggregatedOffers_{region}_aFRR_NotSpecifiedUpAcceptedVolume\",\n",
                "        f\"Balancing_AcceptedAggregatedOffers_{region}_mFRR_NotSpecifiedUpAcceptedVolume\",\n",
                "        f\"Balancing_AcceptedAggregatedOffers_{region}_aFRR_NotSpecifiedDownAcceptedVolume\",\n",
                "        f\"Balancing_AcceptedAggregatedOffers_{region}_mFRR_NotSpecifiedDownAcceptedVolume\",\n",
                "        f\"Transmission_ImplicitAllocationsNetPositions_{region}_Daily_Export_NetPosition[MW]\",\n",
                "        f\"Transmission_ImplicitAllocationsNetPositions_{region}_Daily_Import_NetPosition[MW]\",\n",
                "        f\"Transmission_ForecastedYearAheadTransferCapacities_TO_{region}_ForecastTransferCapacity\",\n",
                "        f\"Transmission_ForecastedYearAheadTransferCapacities_FROM_{region}_ForecastTransferCapacity\",\n",
                "        f\"Transmission_ForecastedMonthAheadTransferCapacities_TO_{region}_ForecastTransferCapacity\",\n",
                "        f\"Transmission_ForecastedMonthAheadTransferCapacities_FROM_{region}_ForecastTransferCapacity\",\n",
                "        f\"Transmission_ForecastedWeekAheadTransferCapacities_TO_{region}_ForecastTransferCapacity\",\n",
                "        f\"Transmission_ForecastedWeekAheadTransferCapacities_FROM_{region}_ForecastTransferCapacity\",\n",
                "        f\"Transmission_OfferedIntradayTransferCapacityImplicit_TO_{region}_Capacity\",\n",
                "        f\"Transmission_OfferedIntradayTransferCapacityImplicit_FROM_{region}_Capacity\",\n",
                "        f\"Transmission_PhysicalFlows_TO_{region}_FlowValue\",\n",
                "        f\"Transmission_PhysicalFlows_FROM_{region}_FlowValue\",\n",
                "        f\"Transmission_EnergyPrices_{region}_Price[Currency/MWh]\",\n",
                "        f\"Generation_CurrentGenerationForecastForWindAndSolar_{region}_AggregatedGenerationForecast\",\n",
                "        f\"Generation_AggregatedFillingRateOfWaterReservoirsAndHydroStoragePlants_{region}_StoredEnergy\",\n",
                "        f\"Generation_DayAheadAggregatedGeneration_{region}_ScheduledGeneration\",\n",
                "        f\"Generation_DayAheadGenerationForecastForWindAndSolar_{region}_AggregatedGenerationForecast\"\n",
                "    ]\n",
                "    columns_to_keep.extend(patterns)\n",
                "    \n",
                "    # Add generation type specific columns\n",
                "    for gen_type in generation_types:\n",
                "        columns_to_keep.append(f\"Outages_UnavailabilityOfProductionUnits_{region}_{gen_type} _UnavailableCapacity\")\n",
                "\n",
                "# Filter columns that actually exist in the DataFrame\n",
                "existing_columns = [col for col in columns_to_keep if col in df.columns]\n",
                "\n",
                "# Keep only the specified columns\n",
                "df = df[existing_columns]\n",
                "\n",
                "# Print some information about the filtering\n",
                "print(f\"Original number of columns requested: {len(columns_to_keep)}\")\n",
                "print(f\"Number of columns found in dataset: {len(existing_columns)}\")\n",
                "print(f\"Missing columns: {len(columns_to_keep) - len(existing_columns)}\")\n",
                "\n",
                "# Save the modified dataset\n",
                "save_path = Path(\"master_data/results_final.csv\")\n",
                "df.to_csv(save_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Clean up column names"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Path to the input CSV file\n",
                "input_file = Path(\"master_data/results_final.csv\")\n",
                "\n",
                "# Path to the output CSV file\n",
                "output_file = Path(\"master_data/results_final_fixed.csv\")\n",
                "\n",
                "# Read the CSV file\n",
                "print(f\"Reading CSV file: {input_file}\")\n",
                "df = pd.read_csv(input_file)\n",
                "\n",
                "# Get the original column names\n",
                "original_columns = df.columns.tolist()\n",
                "print(f\"Number of columns in CSV: {len(original_columns)}\")\n",
                "\n",
                "# Create new column names by removing spaces\n",
                "new_columns = [col.replace(' ', '') for col in original_columns]\n",
                "\n",
                "# Rename the columns\n",
                "df.columns = new_columns\n",
                "\n",
                "# Save the dataframe to a new CSV file\n",
                "print(f\"Saving to: {output_file}\")\n",
                "df.to_csv(output_file, index=False)\n",
                "\n",
                "print(\"Column renaming complete!\")\n",
                "print(\"Examples of renamed columns:\")\n",
                "for i, (old, new) in enumerate(zip(original_columns, new_columns)):\n",
                "    if old != new:\n",
                "        print(f\"  {old} -> {new}\")\n",
                "    if i >= 5:  # Just show a few examples\n",
                "        print(\"  ...\")\n",
                "        break\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "# Path to the input CSV file\n",
                "input_file = Path(\"master_data/results_final.csv\")\n",
                "\n",
                "# Path to the output CSV file\n",
                "output_file = Path(\"master_data/results_final_fixed.csv\")\n",
                "\n",
                "def verify_files_equal():\n",
                "    \"\"\"\n",
                "    Verify that the content of both files is identical except for the headers.\n",
                "    \"\"\"\n",
                "    print(\"\\nVerifying files have identical content...\")\n",
                "    \n",
                "    # Read both files\n",
                "    original_df = pd.read_csv(input_file)\n",
                "    new_df = pd.read_csv(output_file)\n",
                "    \n",
                "    # Check number of rows and columns\n",
                "    if original_df.shape != new_df.shape:\n",
                "        print(f\" Files have different shapes: {original_df.shape} vs {new_df.shape}\")\n",
                "        return False\n",
                "    \n",
                "    # Rename columns in original to match new format\n",
                "    renamed_original_df = original_df.copy()\n",
                "    renamed_original_df.columns = [col.replace(' ', '') for col in original_df.columns]\n",
                "    \n",
                "    # Verify all renamed columns match the new file's columns\n",
                "    col_match = all(a == b for a, b in zip(renamed_original_df.columns, new_df.columns))\n",
                "    if not col_match:\n",
                "        print(\" Renamed columns don't match exactly\")\n",
                "        # Print columns that don't match\n",
                "        for i, (a, b) in enumerate(zip(renamed_original_df.columns, new_df.columns)):\n",
                "            if a != b:\n",
                "                print(f\"  Column {i}: '{a}' vs '{b}'\")\n",
                "        return False\n",
                "    \n",
                "    # Compare data values column by column\n",
                "    all_equal = True\n",
                "    for col in renamed_original_df.columns:\n",
                "        if not renamed_original_df[col].equals(new_df[col]):\n",
                "            all_equal = False\n",
                "            print(f\" Column '{col}' has different values\")\n",
                "            \n",
                "            # Show a sample of differences\n",
                "            diff_indices = ~(renamed_original_df[col] == new_df[col])\n",
                "            if diff_indices.any():\n",
                "                diff_rows = diff_indices[diff_indices].index[:5]  # Get first 5 different rows\n",
                "                print(\"  Sample differences (first 5):\")\n",
                "                for idx in diff_rows:\n",
                "                    print(f\"  Row {idx}: '{renamed_original_df.loc[idx, col]}' vs '{new_df.loc[idx, col]}'\")\n",
                "    \n",
                "    # Check for any NA values that might differ\n",
                "    original_na_count = renamed_original_df.isna().sum().sum()\n",
                "    new_na_count = new_df.isna().sum().sum()\n",
                "    if original_na_count != new_na_count:\n",
                "        all_equal = False\n",
                "        print(f\" Number of NA values differs: {original_na_count} in original vs {new_na_count} in new file\")\n",
                "        \n",
                "        # Check which columns have different NA counts\n",
                "        for col in renamed_original_df.columns:\n",
                "            orig_na = renamed_original_df[col].isna().sum()\n",
                "            new_na = new_df[col].isna().sum()\n",
                "            if orig_na != new_na:\n",
                "                print(f\"  Column '{col}' has different NA counts: {orig_na} vs {new_na}\")\n",
                "    \n",
                "    if all_equal:\n",
                "        print(\"Verification successful! Files have identical data with only header differences\")\n",
                "        return True\n",
                "    else:\n",
                "        return False\n",
                "\n",
                "# Call the verification function after creating the new file\n",
                "verify_files_equal()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Merge with other master data to extend data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "\n",
                "# paths to your files\n",
                "old_path   = Path(\"processed_data/results/resultsset_v4.csv\")\n",
                "new_path   = Path(\"master_data/results_final_fixed.csv\")\n",
                "save_path  = Path(\"master_data/results_merged.csv\")\n",
                "\n",
                "# 1) Load both CSVs, parsing the datetime index\n",
                "old = pd.read_csv(old_path, parse_dates=True, index_col=0)\n",
                "new = pd.read_csv(new_path, parse_dates=True, index_col=0)\n",
                "old.index = pd.to_datetime(old.index)\n",
                "new.index = pd.to_datetime(new.index)\n",
                "\n",
                "# 2) Find which columns they share\n",
                "common_cols = old.columns.intersection(new.columns)\n",
                "\n",
                "# 3) Determine the cut‐off timestamp in the old data\n",
                "cutoff_old = old.index.max()\n",
                "\n",
                "# 4) Pull only those rows & columns from the new data beyond the old cutoff\n",
                "new_beyond = new.loc[new.index > cutoff_old, common_cols]\n",
                "\n",
                "# 5) Concatenate, sort, then truncate at 2025-05-01\n",
                "merged = pd.concat([old, new_beyond], axis=0).sort_index()\n",
                "\n",
                "# --- cut at ---\n",
                "merged = merged.loc[:'2025-02-20']\n",
                "\n",
                "# 6) Write out your merged file\n",
                "merged.to_csv(save_path)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "csv  = Path(\"master_data/results_merged.csv\")\n",
                "df = pd.read_csv(csv)\n",
                "\n",
                "df.head()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
